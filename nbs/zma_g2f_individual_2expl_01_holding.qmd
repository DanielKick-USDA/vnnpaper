---
title: Visible Neural Network
jupyter: python3
---

#---
#skip_exec: true
#---



```{python}
# Data ----
from dataG2F.core import get_data
from dataG2F.qol  import ensure_dir_path_exists

# Data Utilities ----
import numpy  as np
import pandas as pd

# Model Building  ----
## General ====
import torch
from   torch import nn
import torch.nn.functional as F
from   torch.utils.data import Dataset
from   torch.utils.data import DataLoader

from vnnpaper.zma import \
    BigDataset,    \
    plDNN_general, \
    mask_parents,  \
    vnn_factory_1, \
    vnn_factory_2, \
    vnn_factory_3

# Hyperparameter Tuning ----
import os # needed for checking history (saved by lightning) 

## Logging with Pytorch Lightning ====
import lightning.pytorch as pl
from   lightning.pytorch.loggers import CSVLogger # used to save the history of each trial (used by ax)

## Adaptive Experimentation Platform ====
from ax.service.ax_client import AxClient, ObjectiveProperties
# from ax.utils.notebook.plotting import init_notebook_plotting, render


import re
import yaml # to parse hparams.yaml into dictionaries
import plotly.express as px
```

```{python}
torch.set_float32_matmul_precision('medium')
```

```{python}
# init_notebook_plotting()
```

```{python}
# cache_path = '../nbs_artifacts/zma_g2f_individual_2expl/'
```

```{python}
# gapit_dir = '/home/kickd/Documents/gapit_singularity/5_Genotype_Data_All_Years_hzg060'

# sorted([e for e in os.listdir(gapit_dir) if e[-1] == 'v'])
```

```{python}
# import pandas as pd
# M = pd.read_csv(gapit_dir+'/GAPIT.Association.GWAS_Results.CMLM.Yield_Mg_ha.csv')
# M.head()
```

```{python}
# import plotly.express as px


# px.scatter(
#     M.loc[M.Chr == 1, ], 
#     'Pos', 
#     'P.value', 
#     log_y=True)
```






```{python}
# # JSON path and log dir
# def read_hyp_exp_results(exp_name, exp_yvar, **kwargs):
#     if 'log_path' in kwargs:
#         if kwargs['log_path'] != None:
#             log_path = kwargs['log_path']
#         else:
#             log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}__{exp_yvar}'
#     else:
#         log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}__{exp_yvar}'
#     json_path = log_path+'.json'

#     # get all versions, confirm that a metrics file exists
#     version_logs = os.listdir(log_path)
#     version_logs = [e for e in version_logs if os.path.exists(f'{log_path}/{e}/metrics.csv')]
#     # extract and sort ints
#     version_logs = sorted([int(e.split('_')[-1]) for e in version_logs])

#     # Produce a tidy df for each version
#     def _get_tidy_metrics(metrics_path, i):
#         df = pd.read_csv(metrics_path)
#         df = pd.melt(
#                     df, 
#                     id_vars= ['step', 'epoch'],
#                     var_name= 'split',
#                     value_vars= ['train_loss', 'val_loss'],
#                     value_name= 'loss'
#                 ).dropna(
#                 ).reset_index(drop=True
#                 ).assign(version = i)
#         return(df)
#     # all training histories
#     metrics = pd.concat([_get_tidy_metrics(metrics_path = f'{log_path}/version_{i}/metrics.csv', 
#                                            i = i) for i in version_logs])
    
#     if os.path.exists(json_path):
#         return (AxClient.load_from_json_file(filepath = json_path), metrics)
#     else:
#         return metrics
```

```{python}
# ax_client, metrics = read_hyp_exp_results(
#     exp_name = 'zma_g2f_individual_0hyps',
#     exp_yvar = [
#         'Yield_Mg_ha',
#         # 'Pollen_DAP_days',
#         # 'Silk_DAP_days',
#         # 'Plant_Height_cm',
#         # 'Ear_Height_cm',
#         # 'Grain_Moisture',
#         # 'Twt_kg_m3',
#         ][0]

# )

# ax_client, metrics = read_hyp_exp_results(
#     exp_name = None,
#     exp_yvar = None,
#     log_path = '../nbs_artifacts/zma_g2f_individual_0hyps/lightning/zma_g2f_individual_0hyps__Yield_Mg_ha'
#     )

# ax_client, metrics = read_hyp_exp_results(
#     exp_name = None,
#     exp_yvar = None,
#     log_path = '../nbs_artifacts/zma_g2f_individual_1mods/lightning/zma_g2f_individual_1mods'
#     )
```

```{python}
# os.listdir('../nbs_artifacts/zma_g2f_individual_1mods/lightning/zma_g2f_individual_1mods')
```


```{python}
# from ax.utils.notebook.plotting import init_notebook_plotting, render
# init_notebook_plotting()

# json_path = '../nbs_artifacts/zma_g2f_individual_0hyps/lightning/zma_g2f_individual_0hyps__Yield_Mg_ha.json'
```

```{python}
# # JSON path and log dir
# def read_hyp_exp_results(exp_name, exp_yvar, **kwargs):
#     if 'log_path' in kwargs:
#         log_path = kwargs['log_path']
#     else:
#         log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}__{exp_yvar}'
#     json_path = log_path+'.json'

#     # get all versions, confirm that a metrics file exists
#     version_logs = os.listdir(log_path)
#     version_logs = [e for e in version_logs if os.path.exists(f'{log_path}/{e}/metrics.csv')]
#     # extract and sort ints
#     version_logs = sorted([int(e.split('_')[-1]) for e in version_logs])

#     # Produce a tidy df for each version
#     def _get_tidy_metrics(metrics_path, i):
#         df = pd.read_csv(metrics_path)
#         df = pd.melt(
#                     df, 
#                     id_vars= ['step', 'epoch'],
#                     var_name= 'split',
#                     value_vars= ['train_loss', 'val_loss'],
#                     value_name= 'loss'
#                 ).dropna(
#                 ).reset_index(drop=True
#                 ).assign(version = i)
#         return(df)
#     # all training histories
#     metrics = pd.concat([_get_tidy_metrics(metrics_path = f'{log_path}/version_{i}/metrics.csv', 
#                                            i = i) for i in version_logs])
    
#     if os.path.exists(json_path):
#         return (AxClient.load_from_json_file(filepath = json_path), metrics)
#     else:
#         return metrics


# ax_client, metrics = read_hyp_exp_results(
#     exp_name = 'zma_g2f_individual_0hyps',
#     exp_yvar = [
#         'Yield_Mg_ha',
#         # 'Pollen_DAP_days',
#         # 'Silk_DAP_days',
#         # 'Plant_Height_cm',
#         # 'Ear_Height_cm',
#         # 'Grain_Moisture',
#         # 'Twt_kg_m3',
#         ][0]

# )
```

```{python}
# import plotly.express as px
# px.scatter(metrics.loc[(metrics.split == 'train_loss'), ], x = 'step', y = 'loss', color = 'version')
```

```{python}
# px.scatter(metrics.loc[(metrics.split == 'val_loss'), ], x = 'step', y = 'loss', color = 'version')
```

```{python}
# px.line(metrics.loc[(metrics.split == 'val_loss'), ], x = 'step', y = 'loss', color = 'version')
```

```{python}
# idx, params, loss = ax_client.get_best_trial()
# params
```










## Setup

```{python}
cache_path = '../nbs_artifacts/zma_g2f_individual_2expl/'
```

```{python}
## Settings ====
run_hyps = 64 
run_hyps_force = False # should we run more trials even if the target number has been reached?
max_hyps = 64

# Run settings: 
params_run = {
    'batch_size': 256,
    'max_epoch' : 1,    
}

# data settings
params_data = {
    # 'y_var': 'Yield_Mg_ha',
    'y_var': [
        # Description quoted from competition data readme
        # 'Yield_Mg_ha',     # Grain yield in Mg per ha at 15.5% grain moisture, using plot area without alley (Mg/ha).
        # 'Pollen_DAP_days', # Number of days after planting that 50% of plants in the plot began shedding pollen.
        # 'Silk_DAP_days',   # Number of days after planting that 50% of plants in the plot had visible silks.
        'Plant_Height_cm', # Measured as the distance between the base of a plant and the ligule of the flag leaf (centimeter).
        # 'Ear_Height_cm',   # Measured as the distance from the ground to the primary ear bearing node (centimeter).
        # 'Grain_Moisture',  # Water content in grain at harvest (percentage).
        # 'Twt_kg_m3'        # Shelled grain test weight (kg/m3), a measure of grain density.
    ],

    'y_resid': 'None', # None, Env, Geno
    'y_resid_strat': 'None', # None, naive_mean, filter_mean, ...
    'holdout_parents': [
        ## 2022 ====
        'LH244',
        ## 2021 ====
        'PHZ51',
        # 'PHP02',
        # 'PHK76',
        ## 2019 ====
        # 'PHT69',
        'LH195',
        ## 2017 ====
        # 'PHW52',
        # 'PHN82',
        ## 2016 ====
        # 'DK3IIH6',
        ## 2015 ====
        # 'PHB47',
        # 'LH82',
        ## 2014 ====
        # 'LH198',
        # 'LH185',
        # 'PB80',
        # 'CG102',
 ],    
}
```

```{python}
params = {
'default_out_nodes_inp'  : 4,
'default_out_nodes_edge' : 16,
'default_out_nodes_out'  : 1,

'default_drop_nodes_inp' : 0.0,
'default_drop_nodes_edge': 0.0,
'default_drop_nodes_out' : 0.0,

'default_reps_nodes_inp' : 1,
'default_reps_nodes_edge': 1,
'default_reps_nodes_out' : 1,

'default_decay_rate': 1

}
```

```{python}
lightning_log_dir = cache_path+"lightning"
exp_name = [e for e in cache_path.split('/') if e != ''][-1]
```

```{python}
# parameterization is needed for setup. These values will be overwritten by Ax if tuning is occuring. 
# in this file I define params later. I've included it here to gurantee that we can merge other params dicts into it.

default_out_nodes_inp  = params['default_out_nodes_inp' ]
default_out_nodes_edge = params['default_out_nodes_edge'] 
default_out_nodes_out  = params['default_out_nodes_out' ]

default_drop_nodes_inp = params['default_drop_nodes_inp' ] 
default_drop_nodes_edge= params['default_drop_nodes_edge'] 
default_drop_nodes_out = params['default_drop_nodes_out' ] 

default_reps_nodes_inp = params['default_reps_nodes_inp' ]
default_reps_nodes_edge= params['default_reps_nodes_edge']
default_reps_nodes_out = params['default_reps_nodes_out' ]

default_decay_rate = params['default_decay_rate' ]
```

```{python}
batch_size = params_run['batch_size']
max_epoch  = params_run['max_epoch']

y_var = params_data['y_var']
```

```{python}
save_prefix = [e for e in cache_path.split('/') if e != ''][-1]

if 'None' != params_data['y_resid_strat']:
    save_prefix = save_prefix+'_'+params_data['y_resid_strat']

ensure_dir_path_exists(dir_path = cache_path)
```

```{python}
use_gpu_num = 0

device = "cuda" if torch.cuda.is_available() else "cpu"
if use_gpu_num in [0, 1]: 
    torch.cuda.set_device(use_gpu_num)
print(f"Using {device} device")
```

## Standard setut for VNN

### Load Data

```{python}
# Data Prep ----
obs_geno_lookup          = get_data('obs_geno_lookup')
phno                     = get_data('phno')
ACGT_gene_slice_list     = get_data('KEGG_slices')       # 'KEGG_slices':  'ACGT_gene_slice_list.pkl',
ACGT_gene_slice_loci     = get_data('KEGG_slices_names') # 'KEGG_slices_names': 'ACGT_gene_site_name_list.pkl',
parsed_kegg_gene_entries = get_data('KEGG_entries')      # 'KEGG_entries': 'filtered_kegg_gene_entries.pkl',
```

```{python}
[len(e) for e in [ACGT_gene_slice_list, ACGT_gene_slice_loci]]
```

```{python}
ACGT_gene_slice_list[0].shape, len(ACGT_gene_slice_loci[0])
```


```{python}
# make sure that the given y variable is there
# single column version
# phno = phno.loc[(phno[y_var].notna()), ].copy()
# phno = phno.reset_index().drop(columns='index')

# multicolumn
# mask based on the y variables
na_array = phno[y_var].isna().to_numpy().sum(axis=1)
mask_no_na = list(0 == na_array)

phno = phno.loc[mask_no_na, ].copy()
phno = phno.reset_index().drop(columns='index')
```

```{python}
# update obs_geno_lookup

tmp = phno.reset_index().rename(columns={'index': 'Phno_Idx_new'}).loc[:, ['Phno_Idx_new', 'Geno_Idx']]
tmp = pd.merge(tmp,
          tmp.drop(columns='Phno_Idx_new').drop_duplicates().reset_index().rename(columns={'index': 'Phno_Idx_Orig_new'}))
tmp = tmp.sort_values('Phno_Idx_new').reset_index(drop=True)

obs_geno_lookup = tmp.to_numpy()
```

```{python}
# make holdout sets
holdout_parents = params_data['holdout_parents']

# create a mask for parent genotype
mask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)

train_mask = mask.sum(axis=1) == 0
test_mask  = mask.sum(axis=1) > 0

train_idx = train_mask.loc[train_mask].index
test_idx  = test_mask.loc[test_mask].index
```

```{python}
# convert y to residual if needed

if params_data['y_resid'] == 'None':
    pass
else:
    if params_data['y_resid_strat'] == 'naive_mean':
        # use only data in the training set (especially since testers will be more likely to be found across envs)
        # get enviromental means, subtract from observed value
        tmp = phno.loc[train_idx, ]
        env_mean = tmp.groupby(['Env_Idx']
                     ).agg(Env_Mean = (y_var, 'mean')
                     ).reset_index()
        tmp = phno.merge(env_mean)
        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']
        phno = tmp.drop(columns='Env_Mean')

    if params_data['y_resid_strat'] == 'filter_mean':
        # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations
        def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],
                                    year = 2014):
            # Within each year what hybrids are most common?
            tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')

            all_envs = set(tmp.Env)
            # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?
            # site counts for sets which contain all envs
            i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year >= i), 'Env'])) == len(all_envs)])

            before = len(set(tmp.loc[:, 'Hybrid']))
            after  = len(set(tmp.loc[(tmp.Year >= i), 'Hybrid']))
            print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')
            tmp = tmp.loc[(tmp.Year >= i), ['Env', 'Hybrid']].reset_index(drop=True)
            return tmp


        tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]
        filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) 
                          for i in list(set(phno.Year)) ]
        env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')

        env_mean = env_mean.groupby(['Env_Idx']
                          ).agg(Env_Mean = (y_var, 'mean')
                          ).reset_index()

        tmp = phno.merge(env_mean)
        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']
        phno = tmp.drop(columns='Env_Mean')
        
```

```{python}
# center and y value data
assert 0 == phno.loc[:, y_var].isna().sum().sum() # second sum is for multiple y_vars

y = phno.loc[:, y_var].to_numpy() # added to make multiple ys work
# use train index to prevent information leakage
y_c = y[train_idx].mean(axis=0)
y_s = y[train_idx].std(axis=0)

y = (y - y_c)/y_s
```

### Fit Using VNNHelper

```{python}
myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = params, ACGT_gene_slice_list = ACGT_gene_slice_list)
```

### Calculate nodes membership in each matrix and positions within each

```{python}
### Creating Structured Matrices for Layers
M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)
```

### Setup Dataloader using `M_list`

```{python}
lookup_dict = new_lookup_dict

vals = get_data('KEGG_slices')
vals = [torch.from_numpy(e).to(torch.float) for e in vals]
# restrict to the tensors that will be used
vals = torch.concat([vals[lookup_dict[i]].reshape(4926, -1) 
                     for i in M_list[0].row_inp
                    #  for i in dd[0]['inp'] # matches
                     ], axis = 1)

vals = vals.to('cuda')
```

```{python}
# build the site lookup for vals
vals_loci = [ACGT_gene_slice_loci[lookup_dict[i]] for i in M_list[0].row_inp]
vals_loci = sum(vals_loci, [])
# confirm there are equal loci and input snps
assert vals.shape[1]/4 == len(vals_loci)
```

```{python}
with open('./vals_loci_filter.txt', 'w') as f:
    for e in vals_loci:
        f.writelines('\t'.join(e[1:].split('_'))+'\n')
```

```{python}
training_dataloader = DataLoader(BigDataset(
    lookups_are_filtered = False,
    lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),
    lookup_geno = torch.from_numpy(obs_geno_lookup),
    y =           torch.from_numpy(y).to(torch.float32)[:, None].squeeze(),
    G =           vals,
    G_type = 'raw',
    send_batch_to_gpu = 'cuda:0'
    ),
    batch_size = batch_size,
    shuffle = True 
)

validation_dataloader = DataLoader(BigDataset(
    lookups_are_filtered = False,
    lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),
    lookup_geno = torch.from_numpy(obs_geno_lookup),
    y =           torch.from_numpy(y).to(torch.float32)[:, None].squeeze(),
    G =           vals,
    G_type = 'raw',
    send_batch_to_gpu = 'cuda:0'
    ),
    batch_size = batch_size,
    shuffle = False 
)
```

### Structured Layer

```{python}
class NeuralNetwork(nn.Module):
    def __init__(self, layer_list):
        super(NeuralNetwork, self).__init__()
        self.layer_list = nn.ModuleList(layer_list)
 
    def forward(self, x):
        for l in self.layer_list:
            x = l(x)
        return x
```

## Load results and examine

### Within Model:

#### Hyperparmeter performance variability:

```{python}
# JSON path and log dir
def read_hyp_exp_results(exp_name, exp_yvar, **kwargs):
    if 'log_path' in kwargs:
        if kwargs['log_path'] != None:
            log_path = kwargs['log_path']
        else:
            log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}__{exp_yvar}'
    else:
        log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}__{exp_yvar}'
    json_path = log_path+'.json'

    # get all versions, confirm that a metrics file exists
    version_logs = os.listdir(log_path)
    version_logs = [e for e in version_logs if os.path.exists(f'{log_path}/{e}/metrics.csv')]
    # extract and sort ints
    version_logs = sorted([int(e.split('_')[-1]) for e in version_logs])

    # Produce a tidy df for each version
    def _get_tidy_metrics(metrics_path, i):
        df = pd.read_csv(metrics_path)
        df = pd.melt(
                    df, 
                    id_vars= ['step', 'epoch'],
                    var_name= 'split',
                    value_vars= ['train_loss', 'val_loss'],
                    value_name= 'loss'
                ).dropna(
                ).reset_index(drop=True
                ).assign(version = i)
        return(df)
    # all training histories
    metrics = pd.concat([_get_tidy_metrics(metrics_path = f'{log_path}/version_{i}/metrics.csv', 
                                           i = i) for i in version_logs])
    
    if os.path.exists(json_path):
        return (AxClient.load_from_json_file(filepath = json_path), metrics)
    else:
        return metrics
```

```{python}
import pyarrow as pa
import pyarrow.parquet as pq

#TODO
# write to parquet files so I can make the figured I want in ggplot+patchwork

# table = pa.Table.from_pandas(table)
# pq.write_table(table, 'table.parquet')

# x1 = pq.read_table('table.parquet').to_pandas()
```

```{python}
os.listdir('../nbs_artifacts/zma_g2f_individual_0hyps/lightning')
```

```{python}
# os.listdir('../nbs_artifacts/')
# zma_g2f_individual_eres_1mods
```

```{python}
ax_client, metrics = read_hyp_exp_results(
    exp_name = 'zma_g2f_individual_0hyps',
    exp_yvar = [
        'Yield_Mg_ha',
        # 'Pollen_DAP_days',
        # 'Silk_DAP_days',
        # 'Plant_Height_cm',
        # 'Ear_Height_cm',
        # 'Grain_Moisture',
        # 'Twt_kg_m3',
        ][0]
)



# ax_client, metrics = read_hyp_exp_results(
#     exp_name = None,
#     exp_yvar = None,
#     log_path = '../nbs_artifacts/zma_g2f_individual_0hyps/lightning/zma_g2f_individual_0hyps__Yield_Mg_ha'
#     )

# metrics = read_hyp_exp_results(
#     exp_name = None,
#     exp_yvar = None,
#     log_path = '../nbs_artifacts/zma_g2f_individual_1mods/lightning/zma_g2f_individual_1mods'
#     )
```

```{python}
df = ax_client.get_trials_data_frame()
df = df.loc[:, [
    'train_loss',
    'default_out_nodes_inp',
    'default_out_nodes_edge',
    'default_drop_nodes_inp',
    'default_drop_nodes_edge',
    'default_drop_nodes_out',
    'default_reps_nodes_inp',
    'default_reps_nodes_edge',
    'default_reps_nodes_out',
    'default_decay_rate',
    # 'default_out_nodes_out'
    ]]

df_scale = df.copy()
```

```{python}
df_scale = (df_scale - df_scale.min()) / (df_scale.max() - df_scale.min())

px.imshow(df_scale)
```

```{python}
px.line(df, x = df.index, y = 'train_loss', title= 'Minimum Loss')
```

```{python}
tmp = metrics.copy()
# tmp.version = tmp.version.astype(float)

tmp = tmp.merge(
    pd.DataFrame(
        [[i,j] for i in range(4) for j in range(8)], columns=['row', 'col']
    ).reset_index().rename(columns={'index':'version'})
)
```

```{python}
px.line(
    tmp.loc[
        (tmp.split == 'train_loss'), 
        ].groupby(['version', 'epoch', 'row', 'col']).agg(loss = ('loss', 'mean')).reset_index(),
    x = 'epoch', y = 'loss', 
    line_group='version', 
    # color='version', 
    # facet_col='version'
    facet_col= 'col',
    facet_row='row'
    )
```

```{python}
px.line(
    tmp.loc[
        (tmp.split == 'val_loss'), 
        ].groupby(['version', 'epoch', 'row', 'col']).agg(loss = ('loss', 'mean')).reset_index(),
    x = 'epoch', y = 'loss', 
    line_group='version', 
    # color='version', 
    # facet_col='version'
    facet_col= 'col',
    facet_row='row',
    # log_y=True
    )
```

```{python}
from plotnine import ggplot, \
    geom_point, \
    geom_line, \
    geom_area, \
    aes, \
    stat_smooth, \
    facet_wrap

(
    ggplot(metrics, aes("epoch", "loss", color="version"))
    + geom_point()
    # + stat_smooth(method="lm")
    + facet_wrap("split")
)
```

```{python}
tmp
```

```{python}
(
    ggplot(metrics.loc[(metrics.version > 28), ], aes("epoch", "loss", color="version"))
    + geom_line()
    # + stat_smooth(method="lm")
    + facet_wrap("split")
)
```

#### Performance over CV:

```{python}
os.listdir('../nbs_artifacts/zma_g2f_individual_1mods/lightning')
```

```{python}
# get model results table

def _get_1mods(exp_name = 'zma_g2f_individual_1mods'):
    metrics = read_hyp_exp_results(
        exp_name = None,
        exp_yvar = None,
        log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}'
        )

    # find all the version entries to parse
    def _get_version_holdouts(exp_name, version):
        with open(f'../nbs_artifacts/{exp_name}/lightning/{exp_name}/{version}/hparams.yaml') as f:
            hparams = yaml.safe_load(f)
        params_data = hparams['params_data']
        params_data['holdout_parents'] = '_'.join(params_data['holdout_parents'])
        params_data['y_var'] = '_'.join(params_data['y_var'])
        vnum = int(version.split('_')[-1])
        params_data = pd.DataFrame(
            params_data, 
            index=[vnum]
            ).assign(version = vnum)
        return params_data

    versions = [e for e in os.listdir(f'../nbs_artifacts/{exp_name}/lightning/{exp_name}') if re.match('version_\d+', e)]
    hparams = pd.concat([_get_version_holdouts(exp_name, version) for version in versions])

    return metrics.merge(hparams)

metrics = _get_1mods(exp_name = 'zma_g2f_individual_1mods')
```

```{python}
metrics = metrics.groupby(['holdout_parents', 'split', 'epoch', 'version']).agg(loss = ('loss', 'mean')).reset_index()
```

```{python}
(
    ggplot(metrics.loc[metrics.split == 'train_loss', ], aes("epoch", "loss", color="holdout_parents", group='version'))
    + geom_line()
    + facet_wrap("holdout_parents")
)
```

```{python}
(
    ggplot(metrics.loc[metrics.split == 'val_loss', ], aes("epoch", "loss", color="holdout_parents", group='version'))
    + geom_line()
    + facet_wrap("holdout_parents")
)
```

#### Salience over CV:

```{python}
# get model results table

def _identify_1mods_models(
        exp_name = 'zma_g2f_individual_1mods',
        y_var = 'Yield_Mg_ha'):

    model_names = [e for e in os.listdir(f'../nbs_artifacts/{exp_name}/models/') if re.findall(y_var, e)]

    out = []
    for model_name in model_names:
        holdout_parents, replicate = model_name.split('__')[-1][:-3].split('_rep')
        out.append({
            'model_name':model_name,
            'y_var':y_var,
            'holdout_parents':holdout_parents, 
            'replicate':int(replicate),
            'path':f'../nbs_artifacts/{exp_name}/models/{model_name}',
        })

    return out   


model_info = _identify_1mods_models(
        exp_name = 'zma_g2f_individual_1mods',
        y_var = 'Yield_Mg_ha')
model_info
```

```{python}
model = torch.load(
    model_info[0]['path']
    )
```

```{python}
# get saliency for all obs given model, dataloader
def _get_saliency(y_i, x_i, model):
    # y_i, x_i = (y_i.to('cpu'), x_i.to('cpu'))
    x_i.requires_grad_()
    model.eval()

    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)
    loss_fn = nn.MSELoss()

    loss = loss_fn(model(x_i), y_i)
    optimizer.zero_grad()
    loss.backward()
    out = x_i.grad
    return out
```

```{python}
# get saliencey for a single model over all minibatches
# ~2min
# all_grads = [
#     _get_saliency(
#         y_i = y_i, 
#         x_i = x_i,
#         model = model)
#         for idx, (y_i, x_i) in 
#             enumerate(iter(training_dataloader))
            
#         ]

# # [torch.Size([256, 144468

# all_grads = torch.concat(all_grads)
# # Saliency across all obs _not_ all genotypes
# saliency, _ = all_grads.reshape(all_grads.shape[0], 4, -1).abs().max(dim=1)

# px.histogram(saliency[0, :].numpy(), log_y=True)
```

```{python}
from tqdm import tqdm

# add in salience to all entries
for e in tqdm(model_info):
    # this is blowing up the memory usage.
    all_grads = [
        _get_saliency(
            y_i = y_i.to('cuda'), 
            x_i = x_i.to('cuda'),
            model = 
            torch.load(e['path']).to('cuda')
            # detach and transfer to cpu to prevent this from blowing up       
            ).detach().cpu()

            for idx, (y_i, x_i) in 
                enumerate(iter(training_dataloader))
                
            ]

    all_grads = torch.concat(all_grads)
    # Saliency across all obs _not_ all genotypes
    saliency, _ = all_grads.reshape(all_grads.shape[0], 4, -1).abs().max(dim=1)
    e['saliency'] = saliency
```




```{python}
# setup 
saliency_loci = pd.DataFrame(vals_loci, columns=['x'])['x'].str.split('_', expand=True).rename(columns={0:'chrom', 1:'pos'})
saliency_loci.chrom = saliency_loci.chrom.str.replace('S', '')
for e in list(saliency_loci): saliency_loci.loc[:, e] = saliency_loci.loc[:, e].astype(int)

saliency_loci = saliency_loci.reset_index().rename(columns={'index':'saliency_idx'})
saliency_loci['chrom'] = saliency_loci['chrom'].astype(int)
saliency_loci['pos'] = saliency_loci['pos'].astype(int)


saliency_loci
```

```{python}
for e in model_info:
    # saliency = model_info[0]['saliency']
    saliency = e['saliency']
    saliency_loci['saliency'] = saliency.numpy().max(axis = 0)
    tmp = saliency_loci.copy() # copy because we insert the values directly into a col of the df
    tmp = tmp.sort_values(['chrom', 'pos']).reset_index(drop = True)
    fig = px.scatter(tmp, x = tmp.index, y = 'saliency', color='chrom')
    fig.show()
```


```{python}
x = np.concatenate([e['saliency'].numpy().max(axis = 0)[:, None] for e in model_info], axis=1)
```

```{python}
x.shape
```

```{python}
import scipy

_ = [print(f"""{i},{j}, {
    scipy.stats.spearmanr(
        x[:, i].squeeze(), 
        x[:, j].squeeze()
        )}
        """) for i in range(5) for j in range(5) if i <j]
```

```{python}
px.line(y = x.var(axis=1))
```


```{python}
# what's high across all the replicates?
mu_div_var = x.mean(axis=1)/x.var(axis=1)

px.histogram(x = mu_div_var)
```

```{python}
saliency_loci['saliency'] = x.var(axis=1)
tmp = saliency_loci.copy() # copy because we insert the values directly into a col of the df
tmp = tmp.sort_values(['chrom', 'pos']).reset_index(drop = True)
fig = px.scatter(tmp, x = tmp.index, y = 'saliency', color='chrom')
fig.show()
```




```{python}

sals = []
model = model.to('cuda')
```

```{python}
y_i, x_i = next(iter(training_dataloader))
y_i.device
```

```{python}

sal = _get_saliency(
    y_i = y_i, 
    x_i = x_i,
    model = model)

sals.append([sal.detach().cpu()])
```

```{python}
# this is blowing up the memory usage.
all_grads = [
    _get_saliency(
        y_i = y_i.to('cuda'), 
        x_i = x_i.to('cuda'),
        model = 
        torch.load(
            model_info[0]['path']
            ).to('cuda')
            # detach and transfer to cpu to prevent this from blowing up       
        ).detach().cpu()

        for idx, (y_i, x_i) in 
            enumerate(iter(training_dataloader))
            
        ]
```









```{python}
y_i, x_i = next(iter(training_dataloader))


_get_saliency(
    y_i = y_i, 
    x_i = x_i,
    model = model).detach()

# # y_i, x_i = (y_i.to('cpu'), x_i.to('cpu'))

# model.to('cuda')(x_i)
```



```{python}
# model = torch.load('./vnnCG102_PHW52_PHB47_rep00.pt')
model = torch.load(
    '../nbs_artifacts/zma_g2f_individual_1mods/models/'+[
    'vnn__Plant_Height_cm__CG102_PHW52_PHB47_rep00.pt',
    'vnn__Plant_Height_cm__LH185_PHN82_PB80_rep00.pt',
    'vnn__Plant_Height_cm__LH244_PHZ51_LH195_rep00.pt',
    'vnn__Plant_Height_cm__LH82_PHT69_DK3IIH6_rep00.pt',
    'vnn__Plant_Height_cm__PHK76_LH198_PHP02_rep00.pt'][0]
    )
```

```{python}
# y_i.shape
```

```{python}
y_i, x_i = next(iter(training_dataloader))
y_i, x_i = (y_i.to('cpu'), x_i.to('cpu'))
```

```{python}
# x_i.requires_grad_()
# model.eval()

# optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)
# loss_fn = nn.MSELoss()

# loss = loss_fn(model(x_i), y_i)
# optimizer.zero_grad()
# loss.backward()
```

```{python}
# x_i.grad.shape
```

```{python}
# get saliency for all obs given model, dataloader
def _get_saliency(y_i, x_i, model):
    y_i, x_i = (y_i.to('cpu'), x_i.to('cpu'))
    x_i.requires_grad_()
    model.eval()

    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)
    loss_fn = nn.MSELoss()

    loss = loss_fn(model(x_i), y_i)
    optimizer.zero_grad()
    loss.backward()
    out = x_i.grad
    return out


# ~2min
all_grads = [
    _get_saliency(
        y_i = y_i, 
        x_i = x_i,
        model = model)
        for idx, (y_i, x_i) in 
            enumerate(iter(training_dataloader))
            
        ]

# [torch.Size([256, 144468

all_grads = torch.concat(all_grads)
# Saliency across all obs _not_ all genotypes
saliency, _ = all_grads.reshape(all_grads.shape[0], 4, -1).abs().max(dim=1)

px.histogram(saliency[0, :].numpy(), log_y=True)
```

```{python}
# saliency, _ = x_i.grad.reshape(256, 4, -1).abs().max(dim=1)
# px.histogram(saliency[0, :].numpy(), log_y=True)
```

```{python}
saliency_loci = pd.DataFrame(vals_loci, columns=['x'])['x'].str.split('_', expand=True).rename(columns={0:'chrom', 1:'pos'})
saliency_loci.chrom = saliency_loci.chrom.str.replace('S', '')
for e in list(saliency_loci): saliency_loci.loc[:, e] = saliency_loci.loc[:, e].astype(int)

saliency_loci = saliency_loci.reset_index().rename(columns={'index':'saliency_idx'})
saliency_loci
```

```{python}
saliency_loci['saliency'] = saliency.numpy().max(axis = 0)

saliency_loci['chrom'] = saliency_loci['chrom'].astype(int)
saliency_loci['pos'] = saliency_loci['pos'].astype(int)

saliency_loci.info()
```

```{python}
saliency_loci = saliency_loci.sort_values(['chrom', 'pos']).reset_index(drop = True)
saliency_loci.head()
```

```{python}
px.scatter(saliency_loci, x = saliency_loci.index, y = 'saliency', color='chrom')
```





```{python}
gapit_res.loc[:, 'negLogP'] = -np.log(gapit_res.loc[:, 'pvalue'].to_numpy())

px.scatter(gapit_res, x = gapit_res.index, y = 'negLogP', color= 'chrom')
```

```{python}
#FIXME This shouldn't happen but it appears that the sites in gapit_res are not a subset of those used in the vnn but instead partially overlap.
# There should be 30k obs and they should _all_ be present.

# this makes me wonder if they are accurate or not. 
gapit_vnn = gapit_res.merge(saliency_loci)
gapit_vnn.shape
```

```{python}
px.scatter(
    gapit_vnn, 
    x = 'saliency', y = 'negLogP', color= 'chrom')
```

```{python}
import scipy


scipy.stats.spearmanr(
    gapit_vnn.loc[:, ['negLogP']].to_numpy().squeeze(), 
    gapit_vnn.loc[:, ['saliency']].to_numpy().squeeze()
)
```

```{python}
chrom = 1
df1 = saliency_loci
df1_y = 'saliency'
df2 = gapit_res.rename(columns={'Chr':'chrom', 'Pos':'pos'})
df2_y = 'P.value'

df1 = df1.loc[(df1.chrom == chrom), ['pos', df1_y]].to_numpy()
df2 = df2.loc[(df2.chrom == chrom), ['pos', df2_y]].to_numpy()

# find the closest df2_y entry based on position
# get indexes of the smallest absolute distance based on position
idxs = [np.argmin( np.abs(df1[i, 0] - df2[:, 0]) ) for i in range(df1.shape[0])]

# bind the indexed closest values to the saliences
out = np.concatenate([df1, df2[idxs, 1][:, None]], axis = 1)

pd.DataFrame(out, columns=['pos'])



```

```{python}
# match up by distance:

gapit_res# = gapit_res.rename(columns={'Chr':'chrom', 'Pos':'pos'})

for i in saliency_loci.index:
    break


mask = (saliency_loci.index == 1)
chrom, pos = saliency_loci.loc[mask, ['chrom', 'pos']].values[0].tolist()

mask2 = (
    (gapit_res.chrom == chrom) &
    (gapit_res.pos == pos)) 

gapit_res.loc[mask2, ]
```

```{python}
# np.correlate(
#     gapit_vnn.loc[:, ['negLogP']].to_numpy().squeeze(), 
#     gapit_vnn.loc[:, ['saliency']].to_numpy().squeeze(), 
# )
```

```{python}
M_list[0].row_info['100285210']
parsed_kegg_gene_entries
```

```{python}
import re
from tqdm import tqdm

gene_df = list() 

for e in tqdm(parsed_kegg_gene_entries):
    chrm, pos = e['POSITION'].split(':')
    pos = sorted(re.findall('\d+\.\.\d+', pos)[0].split('..'))

    xs = range(int(pos[0]), int(pos[1]))
    gene_df.append(pd.DataFrame({
        'gene':[e['ENTRY'].split(' ')[0] for i in xs],
        'chrm':[int(chrm) for i in xs],
        'pos':xs,
        })
        )

gene_df = pd.concat(gene_df).reset_index(drop=True)
```

```{python}
gapit_res = gapit_res.merge(gene_df.rename(columns={'chrm':'Chr', 'pos':'Pos'}))
gapit_res.head()
```

```{python}
tmp = gapit_res.merge(meta.loc[:, ['gene', 'sal']]).drop_duplicates()
tmp.head()
```

```{python}
tmp = tmp.groupby('gene').agg(sal = ('sal', 'max'),
                        P = ('P.value', 'min')
                        ).reset_index()
tmp
```

```{python}
import plotnine as p9

(p9.ggplot(tmp, p9.aes(x = 'P', y = 'sal'))+
 p9.geom_point()
 )
```

```{python}
np.correlate(
np.asarray(tmp.loc[:, ['P']]).squeeze(),    
np.asarray(tmp.loc[:, ['sal']]).squeeze(),    
)
```


### Between Models: One vs Multiple Targets

#### Performance:

#### Salience?:

### Between Models:

#### Performance vs Linear Model:

#### Salience vs GWAS:



### GWAS

```{python}
def _get_gwas_results(
    gwas_output_dir = '../nbs_artifacts/zma_g2f_individual_gapit/output/',
    gwas_phno = 'Ear_Height_cm',
    gwas_geno = '5_Genotype_Data_All_Years_acgt_30000'
    ):
    gwas_result_path = gwas_output_dir+f'phno_{gwas_phno}__{gwas_geno}'
    gwas_result_name = [e for e in os.listdir(gwas_result_path) if re.match('GAPIT.Association.GWAS_Results', e)][0]
    out = pd.read_csv(gwas_result_path+'/'+gwas_result_name)
    return out
```

```{python}
gapit_res = _get_gwas_results(
    gwas_output_dir = '../nbs_artifacts/zma_g2f_individual_gapit/output/',
    gwas_phno = 'Ear_Height_cm',
    gwas_geno = '5_Genotype_Data_All_Years_acgt_30000'
    )
```

```{python}
def _standardize_names(df, name_dict = {'Pos':'pos', 'Chr':'chrom'}):
    "Replace specified names and then remove any '.' in any name and set all to lowercase"
    df = df.rename(columns = name_dict)
    df = df.rename(columns = {e:e.replace('.', '').lower() for e in list(df)})
    return df
```

```{python}
gapit_res = _standardize_names(gapit_res)
gapit_res
```

```{python}
# are the snps == S+chrom+_+pos ?



# are they a subset of vals_loci? (vals_loci is a subset of ACGT_gene_slice_loci )

vl = pd.DataFrame({'snp':vals_loci})
gapit_res.loc[:, ['snp']].merge(vl).shape[0], vl.merge(gapit_res.loc[:, ['snp']]).shape[0]
```

```{python}
tmp = gapit_res.assign(g = 1).merge(vl.assign(v = 1), how= 'outer')
```

```{python}
tmp.loc[(tmp.g != tmp.v), ]
```

```{python}
len(vals_loci), len(list(set(vals_loci)))
```

```{python}
# What about relative to the superset of vals_loci?

al = pd.DataFrame({'snp': sum(ACGT_gene_slice_loci, [])})
(
    gapit_res.loc[:, ['snp']].merge(al).shape[0], 
    al.merge(gapit_res.loc[:, ['snp']]).shape[0]
)
```



```{python}
#TODO confirm we can match up the inputs. The gwas input should be a subset of the vnn input.
len(vals_loci)
```

```{python}
with open('../nbs_artifacts/zma_g2f_individual_gapit/genotypes_holding/zma_filter_pos_30000.txt', 'r') as f:
    dat = f.readlines()
dat = ['S'+e.replace('\n', '').replace('\t', '_') for e in dat]
```

```{python}
x = [pd.DataFrame(e, columns=['site']) for e in [vals_loci, dat]]
for i in range(2): x[i].loc[:, ['vals_loci', 'dat'][i]] = 1

[print(e.shape) for e in x]

x[0].merge(x[1], how= 'outer')
```





```{python}
# can I just use this and modify holdout parents?
```


```{python}
def _prep_dls(
        y_var,
        train_idx,
        obs_geno_lookup,
        vals,
        batch_size,
        params_data):

    phno                     = get_data('phno')

    # multicolumn
    # mask based on the y variables
    # na_array = phno[y_var].isna().to_numpy().sum(axis=1)
    na_array = phno[ [y_var] if type(y_var) == list else y_var ].isna().to_numpy()
    if len(na_array.shape) == 2:
        na_array = na_array.sum(axis=1)
    mask_no_na = list(0 == na_array)

    phno = phno.loc[mask_no_na, ].copy()
    phno = phno.reset_index().drop(columns='index')


    # update obs_geno_lookup
    tmp = phno.reset_index().rename(columns={'index': 'Phno_Idx_new'}).loc[:, ['Phno_Idx_new', 'Geno_Idx']]
    tmp = pd.merge(tmp,
                tmp.drop(columns='Phno_Idx_new').drop_duplicates().reset_index().rename(columns={'index': 'Phno_Idx_Orig_new'}))
    tmp = tmp.sort_values('Phno_Idx_new').reset_index(drop=True)
    obs_geno_lookup = tmp.to_numpy()


    # make holdout sets
    holdout_parents = params_data['holdout_parents']

    # create a mask for parent genotype
    mask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)

    train_mask = mask.sum(axis=1) == 0
    test_mask  = mask.sum(axis=1) > 0

    train_idx = train_mask.loc[train_mask].index
    test_idx  = test_mask.loc[test_mask].index




    # center and y value data
    assert 0 == phno.loc[:, y_var].isna().sum().sum() # second sum is for multiple y_vars

    y = phno.loc[:, y_var].to_numpy() # added to make multiple ys work
    # use train index to prevent information leakage
    y_c = y[train_idx].mean(axis=0)
    y_s = y[train_idx].std(axis=0)

    y = (y - y_c)/y_s



    training_dataloader = DataLoader(BigDataset(
        lookups_are_filtered = False,
        lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),
        lookup_geno = torch.from_numpy(obs_geno_lookup),
        y =           torch.from_numpy(y).to(torch.float32)[:, None],
        G =           vals,
        G_type = 'raw',
        send_batch_to_gpu = 'cuda:0'
        ),
        batch_size = batch_size,
        shuffle = True 
    )

    validation_dataloader = DataLoader(BigDataset(
        lookups_are_filtered = False,
        lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),
        lookup_geno = torch.from_numpy(obs_geno_lookup),
        y =           torch.from_numpy(y).to(torch.float32)[:, None],
        G =           vals,
        G_type = 'raw',
        send_batch_to_gpu = 'cuda:0'
        ),
        batch_size = batch_size,
        shuffle = False 
    )

    return training_dataloader, validation_dataloader
```

```{python}
training_dataloader, validation_dataloader = _prep_dls(
            y_var = y_var[0],
            train_idx = train_idx,
            obs_geno_lookup = obs_geno_lookup,
            vals = vals,
            batch_size = batch_size,
            params_data = params_data)
```

```{python}
# Extracted from evaluate
parameterization = params

myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = parameterization, ACGT_gene_slice_list = ACGT_gene_slice_list)
M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)
layer_list =  vnn_factory_3(M_list = M_list)
model = NeuralNetwork(layer_list = layer_list)

VNN = plDNN_general(model)  
optimizer = VNN.configure_optimizers()
logger = CSVLogger(lightning_log_dir, name=exp_name)
logger.log_hyperparams(params={
    'params': parameterization
})

trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)
trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=
validation_dataloader)


# ################################################################################
# {'default_out_nodes_inp': 7, 'default_out_nodes_edge': 21, 'default_drop_nodes_inp': 0.15662193443756378, 'default_drop_nodes_edge': 0.01, 'default_drop_nodes_out': 0.01, 'default_reps_nodes_inp': 1, 'default_reps_nodes_edge': 1, 'default_reps_nodes_out': 1, 'default_decay_rate': 2.0, 'default_out_nodes_out': 1}
# ################################################################################
# Retaining 43.53%, 6067/13939 Entries

# GPU available: True (cuda), used: True
# TPU available: False, using: 0 TPU cores
# IPU available: False, using: 0 IPUs
# HPU available: False, using: 0 HPUs
# LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

#   | Name | Type          | Params
# ---------------------------------------
# 0 | mod  | NeuralNetwork | 915 M 
# ---------------------------------------
# 227 K     Trainable params
# 915 M     Non-trainable params
# 915 M     Total params
# 3,661.026 Total estimated model params size (MB)
```

```{python}
# Manual train loop to look at fitting within one epoch
parameterization = params
myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = parameterization, ACGT_gene_slice_list = ACGT_gene_slice_list)
M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)
layer_list =  vnn_factory_3(M_list = M_list)
model = NeuralNetwork(layer_list = layer_list)
```

```{python}
model.to('cuda')
```

```{python}
loss_fn = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), )

max_i = len(training_dataloader.dataset)


def _eval_model(dataloader = training_dataloader):
    model.eval()
    out = list()
    for batch, (yi, Xi) in enumerate(dataloader):
        yhat = model(Xi)
        loss = loss_fn(yhat, yi)
        out.append( (batch, loss.item(), len(Xi)) )
    return out

results = list()
results.append(
    pd.DataFrame(
        _eval_model(dataloader = validation_dataloader), 
        columns=['batch', 'loss', 'n']
        ).assign(split = 'validate', minibatch = -1)
)

for batch, (yi, Xi) in enumerate(training_dataloader):
    if not model.training:
        model.train()

    yhat = model(Xi)
    loss = loss_fn(yhat, yi)

    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    results.append(
    pd.DataFrame(
        _eval_model(dataloader = validation_dataloader), 
        columns=['batch', 'loss', 'n']
        ).assign(split = 'validate', minibatch = batch)
    )
    # takes about 7 min
```

```{python}
res = pd.concat(results)

# px.line(res, x = 'minibatch', y = 'loss', line_group='batch')
# px.scatter(res, x = 'minibatch', y = 'loss', trendline='lowess')

# px.box(res, x = 'minibatch', y = 'loss')
```

```{python}
from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap, geom_smooth, geom_ribbon

def q10(x): return x.quantile(0.10)
def q25(x): return x.quantile(0.25)
def q50(x): return x.quantile(0.50)
def q75(x): return x.quantile(0.75)
def q90(x): return x.quantile(0.90)

res_sum = res.loc[:, ['minibatch', 'split', 'loss']].groupby(['minibatch', 'split']).agg(
    mean=('loss', 'mean'),
    std = ('loss', 'std'),
    q25 =('loss', q25),
    q50 =('loss', q50),
    q75 =('loss', q75),    
    q10 =('loss', q10),
    q90 =('loss', q90),    
    ).reset_index()

res_sum = res_sum.assign(low = res_sum['mean']-res_sum['std']
                ).assign(high= res_sum['mean']+res_sum['std'])

(
    ggplot()
    + geom_point(data=res, mapping= aes('minibatch', 'loss'),                          color = '#d1d1d1')
    + geom_ribbon(data=res_sum, mapping= aes('minibatch', ymax = 'high', ymin = 'low'), fill = '#9d9d9d', alpha = 0.3)
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'mean'),                      color = 'gray')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q50'), color = 'cornflowerblue', linetype = '-')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q25'), color = 'cornflowerblue', linetype = '-.')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q75'), color = 'cornflowerblue', linetype = '-.')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q10'), color = 'cornflowerblue', linetype = ':')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q90'), color = 'cornflowerblue', linetype = ':')
)
```

```{python}
res = res.loc[(res.minibatch < 10), ]


res_sum = res.loc[:, ['minibatch', 'split', 'loss']].groupby(['minibatch', 'split']).agg(
    mean=('loss', 'mean'),
    std = ('loss', 'std'),
    q25 =('loss', q25),
    q50 =('loss', q50),
    q75 =('loss', q75),    
    q10 =('loss', q10),
    q90 =('loss', q90),    
    ).reset_index()

res_sum = res_sum.assign(low = res_sum['mean']-res_sum['std']
                ).assign(high= res_sum['mean']+res_sum['std'])

(
    ggplot()
    + geom_point(data=res, mapping= aes('minibatch', 'loss'),                          color = '#d1d1d1')
    + geom_ribbon(data=res_sum, mapping= aes('minibatch', ymax = 'high', ymin = 'low'), fill = '#9d9d9d', alpha = 0.3)
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'mean'),                      color = 'gray')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q50'), color = 'cornflowerblue', linetype = '-')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q25'), color = 'cornflowerblue', linetype = '-.')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q75'), color = 'cornflowerblue', linetype = '-.')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q10'), color = 'cornflowerblue', linetype = ':')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q90'), color = 'cornflowerblue', linetype = ':')
)
```







```{python}
# y_vars = y_var.copy()
```

```{python}
# for y_var in y_vars:
#     # y_var = ith_y_var
#     # update exp_name
#     exp_name = [e for e in cache_path.split('/') if e != ''][-1]
#     exp_name += '__'+y_var

#     print(''.join(['-' for i in range(80)]))
#     print(f'experiment: {exp_name}')
#     print(''.join(['-' for i in range(80)]))
#     print('\n')

#     training_dataloader, validation_dataloader = _prep_dls(
#             y_var = y_var,
#             train_idx = train_idx,
#             obs_geno_lookup = obs_geno_lookup,
#             vals = vals,
#             batch_size = batch_size,
#             params_data = params_data)

#     ## Generated variables ====
#     json_path = f"./{lightning_log_dir}/{exp_name}.json"

#     loaded_json = False
#     if os.path.exists(json_path): 
#         ax_client = (AxClient.load_from_json_file(filepath = json_path))
#         loaded_json = True

#     else:
#         ax_client = AxClient()
#         ax_client.create_experiment(
#             name=exp_name,
#             parameters=params_list,
#             objectives={"train_loss": ObjectiveProperties(minimize=True)}
#         )

#     run_trials_bool = True
#     if run_hyps_force == False:
#         if loaded_json: 
#             # check if we've reached the max number of hyperparamters combinations to test
#             if max_hyps <= (ax_client.generation_strategy.trials_as_df.index.max()+1):
#                 run_trials_bool = False

#     if run_trials_bool:
#         # run the trials
#         for i in range(run_hyps):
#             parameterization, trial_index = ax_client.get_next_trial()
#             # Local evaluation here can be replaced with deployment to external system.
#             ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameterization))

#         ax_client.save_to_json_file(filepath = json_path)
```



```{python}
# if there is not hyperparameter tuning then this function returns only the training history
metrics = read_hyp_exp_results(exp_name = exp_name, exp_yvar = y_var[0], log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}')
```

```{python}
px.scatter(metrics, x = 'epoch', y = 'loss', color = 'split', trendline='lowess')
```

```{python}
# mod = trainer.model.mod.to('cuda')
mod = trainer.model.mod.to('cpu')
```

```{python}
yi, xi = next(iter(training_dataloader))

yi, xi = yi.to('cpu'), xi.to('cpu')
```

```{python}
opt = trainer.optimizers[0]

yhat = mod(xi)
loss = nn.MSELoss()(yhat, yi)
opt.zero_grad()
loss.backward()
```

```{python}
px.histogram(mod.layer_list[-1].weights.grad)
```

```{python}
px.histogram(mod.layer_list[0].weights.grad)
```

```{python}
mod.layer_list[0].weights.grad
xi.shape
```

```{python}
mod.layer_list[0].weight
```

```{python}



# mod.layer_list[0].weight.values()


# mod.layer_list[0].weight
```

```{python}
# this is a suitable way to get the relevant weights for a node. 

#TODO separate node weights by 
ith_layer = 0
M = M_list[ith_layer]

idxs = mod.layer_list[0].weight.indices()
vals = mod.layer_list[0].weight.values()

out = M.col_out[0] # list of nodes

c1, c2 = (M.col_info[out][e] for e in ['start', 'stop']) # col_info appears to be the 0th axis

mask = ((idxs[0] >= c1) & (idxs[0] < c2))


vals[mask]


# r1, r2 = (M.row_info[out][e] for e in ['start', 'stop']) # so this 
```

```{python}
max([M.col_info[e]['stop'] for e in M.col_out])
```


```{python}
# [e for e in dir(mod.layer_list[0]) if e[0] != '_']
```


```{python}
# for the below to work I need to either reproduce edge dict creation OR extract it from the existing information. I'll do the latter.
i = -1
l = M_list[i]
# l.col_info
# l.row_info


l.col_out # these are keys

l.row_inp # these are values to assign to keys

# after the fact I'll go through and clean the graph so that references pointing to itself are removed

```

```{python}
# Copy code I wrote in aim_2a_B_VNN.ipynb
## Interpretability
```

```{python}
model = trainer.model.mod
```

```{python}
# set up gradients for testing

yi, xi = next(iter(training_dataloader))
yi = yi.to('cpu')
xi = xi.to('cpu')


optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)
loss_fn = nn.MSELoss()

loss = loss_fn(model(xi), yi)
optimizer.zero_grad()
loss.backward()
```

```{python}
# find indices in a structured matrix that correspond to the in/out of a given node. 

def find_node_in_layer_info(
    layer_info_list:list, # List where all elements are `sparsevnn.core.structured_layer_info`. 
    edge_dict:dict, # dictionary of graph edges. Needed to find the correct input dim. indices of the `sparsevnn.core.SparseLinearCustom`. 
    query:str, # name of the node to be identified
):
    # find the right index in the list:
    list_idx = [i for i in range(len(layer_info_list)) if query in layer_info_list[i].col_out]
    assert len(list_idx) == 1
    list_idx = list_idx[0]

    # if an edge or output
    if edge_dict[query] != []:
        rows_info = {e : layer_info_list[list_idx].row_info[e] for  e in edge_dict[query]}
    else: # if input
        rows_info = {e : layer_info_list[list_idx].row_info[e] for  e in [query]}

    return {
        'list_idx':  list_idx,
        'col_info':  layer_info_list[list_idx].col_info[query],
        'rows_info': rows_info
        }
```

```{python}
find_node_in_layer_info(
    layer_info_list = M_list,
    edge_dict=edge_dict,
    query = 'ProteinPhosphatasesAndAssociatedProteins[Br-Zma01009]'        
)
```






```{python}

```







