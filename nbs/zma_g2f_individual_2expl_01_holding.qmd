---
title: Visible Neural Network
jupyter: python3
---

#---
#skip_exec: true
#---



```{python}
# Data ----
from dataG2F.core import get_data
from dataG2F.qol  import ensure_dir_path_exists

# Data Utilities ----
import numpy  as np
import pandas as pd

# Model Building  ----
## General ====
import torch
from   torch import nn
import torch.nn.functional as F
from   torch.utils.data import Dataset
from   torch.utils.data import DataLoader

from vnnpaper.zma import \
    BigDataset,    \
    plDNN_general, \
    mask_parents,  \
    vnn_factory_1, \
    vnn_factory_2, \
    vnn_factory_3

# Hyperparameter Tuning ----
import os # needed for checking history (saved by lightning) 

## Logging with Pytorch Lightning ====
import lightning.pytorch as pl
from   lightning.pytorch.loggers import CSVLogger # used to save the history of each trial (used by ax)

## Adaptive Experimentation Platform ====
from ax.service.ax_client import AxClient, ObjectiveProperties
# from ax.utils.notebook.plotting import init_notebook_plotting, render


import re
import yaml # to parse hparams.yaml into dictionaries
import plotly.express as px


import pyarrow as pa
import pyarrow.parquet as pq


from plotnine import ggplot, \
    geom_point, \
    geom_line, \
    geom_area, \
    aes, \
    stat_smooth, \
    facet_wrap

from tqdm import tqdm
```

```{python}
torch.set_float32_matmul_precision('medium')
```

```{python}
# init_notebook_plotting()
```

```{python}
# cache_path = '../nbs_artifacts/zma_g2f_individual_2expl/'
```

```{python}
# gapit_dir = '/home/kickd/Documents/gapit_singularity/5_Genotype_Data_All_Years_hzg060'

# sorted([e for e in os.listdir(gapit_dir) if e[-1] == 'v'])
```

```{python}
# import pandas as pd
# M = pd.read_csv(gapit_dir+'/GAPIT.Association.GWAS_Results.CMLM.Yield_Mg_ha.csv')
# M.head()
```

```{python}
# import plotly.express as px


# px.scatter(
#     M.loc[M.Chr == 1, ], 
#     'Pos', 
#     'P.value', 
#     log_y=True)
```






```{python}
# # JSON path and log dir
# def read_hyp_exp_results(exp_name, exp_yvar, **kwargs):
#     if 'log_path' in kwargs:
#         if kwargs['log_path'] != None:
#             log_path = kwargs['log_path']
#         else:
#             log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}__{exp_yvar}'
#     else:
#         log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}__{exp_yvar}'
#     json_path = log_path+'.json'

#     # get all versions, confirm that a metrics file exists
#     version_logs = os.listdir(log_path)
#     version_logs = [e for e in version_logs if os.path.exists(f'{log_path}/{e}/metrics.csv')]
#     # extract and sort ints
#     version_logs = sorted([int(e.split('_')[-1]) for e in version_logs])

#     # Produce a tidy df for each version
#     def _get_tidy_metrics(metrics_path, i):
#         df = pd.read_csv(metrics_path)
#         df = pd.melt(
#                     df, 
#                     id_vars= ['step', 'epoch'],
#                     var_name= 'split',
#                     value_vars= ['train_loss', 'val_loss'],
#                     value_name= 'loss'
#                 ).dropna(
#                 ).reset_index(drop=True
#                 ).assign(version = i)
#         return(df)
#     # all training histories
#     metrics = pd.concat([_get_tidy_metrics(metrics_path = f'{log_path}/version_{i}/metrics.csv', 
#                                            i = i) for i in version_logs])
    
#     if os.path.exists(json_path):
#         return (AxClient.load_from_json_file(filepath = json_path), metrics)
#     else:
#         return metrics
```

```{python}
# ax_client, metrics = read_hyp_exp_results(
#     exp_name = 'zma_g2f_individual_0hyps',
#     exp_yvar = [
#         'Yield_Mg_ha',
#         # 'Pollen_DAP_days',
#         # 'Silk_DAP_days',
#         # 'Plant_Height_cm',
#         # 'Ear_Height_cm',
#         # 'Grain_Moisture',
#         # 'Twt_kg_m3',
#         ][0]

# )

# ax_client, metrics = read_hyp_exp_results(
#     exp_name = None,
#     exp_yvar = None,
#     log_path = '../nbs_artifacts/zma_g2f_individual_0hyps/lightning/zma_g2f_individual_0hyps__Yield_Mg_ha'
#     )

# ax_client, metrics = read_hyp_exp_results(
#     exp_name = None,
#     exp_yvar = None,
#     log_path = '../nbs_artifacts/zma_g2f_individual_1mods/lightning/zma_g2f_individual_1mods'
#     )
```

```{python}
# os.listdir('../nbs_artifacts/zma_g2f_individual_1mods/lightning/zma_g2f_individual_1mods')
```


```{python}
# from ax.utils.notebook.plotting import init_notebook_plotting, render
# init_notebook_plotting()

# json_path = '../nbs_artifacts/zma_g2f_individual_0hyps/lightning/zma_g2f_individual_0hyps__Yield_Mg_ha.json'
```

```{python}
# # JSON path and log dir
# def read_hyp_exp_results(exp_name, exp_yvar, **kwargs):
#     if 'log_path' in kwargs:
#         log_path = kwargs['log_path']
#     else:
#         log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}__{exp_yvar}'
#     json_path = log_path+'.json'

#     # get all versions, confirm that a metrics file exists
#     version_logs = os.listdir(log_path)
#     version_logs = [e for e in version_logs if os.path.exists(f'{log_path}/{e}/metrics.csv')]
#     # extract and sort ints
#     version_logs = sorted([int(e.split('_')[-1]) for e in version_logs])

#     # Produce a tidy df for each version
#     def _get_tidy_metrics(metrics_path, i):
#         df = pd.read_csv(metrics_path)
#         df = pd.melt(
#                     df, 
#                     id_vars= ['step', 'epoch'],
#                     var_name= 'split',
#                     value_vars= ['train_loss', 'val_loss'],
#                     value_name= 'loss'
#                 ).dropna(
#                 ).reset_index(drop=True
#                 ).assign(version = i)
#         return(df)
#     # all training histories
#     metrics = pd.concat([_get_tidy_metrics(metrics_path = f'{log_path}/version_{i}/metrics.csv', 
#                                            i = i) for i in version_logs])
    
#     if os.path.exists(json_path):
#         return (AxClient.load_from_json_file(filepath = json_path), metrics)
#     else:
#         return metrics


# ax_client, metrics = read_hyp_exp_results(
#     exp_name = 'zma_g2f_individual_0hyps',
#     exp_yvar = [
#         'Yield_Mg_ha',
#         # 'Pollen_DAP_days',
#         # 'Silk_DAP_days',
#         # 'Plant_Height_cm',
#         # 'Ear_Height_cm',
#         # 'Grain_Moisture',
#         # 'Twt_kg_m3',
#         ][0]

# )
```

```{python}
# import plotly.express as px
# px.scatter(metrics.loc[(metrics.split == 'train_loss'), ], x = 'step', y = 'loss', color = 'version')
```

```{python}
# px.scatter(metrics.loc[(metrics.split == 'val_loss'), ], x = 'step', y = 'loss', color = 'version')
```

```{python}
# px.line(metrics.loc[(metrics.split == 'val_loss'), ], x = 'step', y = 'loss', color = 'version')
```

```{python}
# idx, params, loss = ax_client.get_best_trial()
# params
```










## Setup

```{python}
cache_path = '../nbs_artifacts/zma_g2f_individual_2expl/'
```

```{python}
## Settings ====
run_hyps = 64 
run_hyps_force = False # should we run more trials even if the target number has been reached?
max_hyps = 64

# Run settings: 
params_run = {
    'batch_size': 256,
    'max_epoch' : 1,    
}

# data settings
params_data = {
    # 'y_var': 'Yield_Mg_ha',
    'y_var': [
        # Description quoted from competition data readme
        # 'Yield_Mg_ha',     # Grain yield in Mg per ha at 15.5% grain moisture, using plot area without alley (Mg/ha).
        # 'Pollen_DAP_days', # Number of days after planting that 50% of plants in the plot began shedding pollen.
        # 'Silk_DAP_days',   # Number of days after planting that 50% of plants in the plot had visible silks.
        'Plant_Height_cm', # Measured as the distance between the base of a plant and the ligule of the flag leaf (centimeter).
        # 'Ear_Height_cm',   # Measured as the distance from the ground to the primary ear bearing node (centimeter).
        # 'Grain_Moisture',  # Water content in grain at harvest (percentage).
        # 'Twt_kg_m3'        # Shelled grain test weight (kg/m3), a measure of grain density.
    ],

    'y_resid': 'None', # None, Env, Geno
    'y_resid_strat': 'None', # None, naive_mean, filter_mean, ...
    'holdout_parents': [
        ## 2022 ====
        'LH244',
        ## 2021 ====
        'PHZ51',
        # 'PHP02',
        # 'PHK76',
        ## 2019 ====
        # 'PHT69',
        'LH195',
        ## 2017 ====
        # 'PHW52',
        # 'PHN82',
        ## 2016 ====
        # 'DK3IIH6',
        ## 2015 ====
        # 'PHB47',
        # 'LH82',
        ## 2014 ====
        # 'LH198',
        # 'LH185',
        # 'PB80',
        # 'CG102',
 ],    
}
```

```{python}
params = {
'default_out_nodes_inp'  : 4,
'default_out_nodes_edge' : 16,
'default_out_nodes_out'  : 1,

'default_drop_nodes_inp' : 0.0,
'default_drop_nodes_edge': 0.0,
'default_drop_nodes_out' : 0.0,

'default_reps_nodes_inp' : 1,
'default_reps_nodes_edge': 1,
'default_reps_nodes_out' : 1,

'default_decay_rate': 1

}
```

```{python}
lightning_log_dir = cache_path+"lightning"
exp_name = [e for e in cache_path.split('/') if e != ''][-1]
```

```{python}
# parameterization is needed for setup. These values will be overwritten by Ax if tuning is occuring. 
# in this file I define params later. I've included it here to gurantee that we can merge other params dicts into it.

default_out_nodes_inp  = params['default_out_nodes_inp' ]
default_out_nodes_edge = params['default_out_nodes_edge'] 
default_out_nodes_out  = params['default_out_nodes_out' ]

default_drop_nodes_inp = params['default_drop_nodes_inp' ] 
default_drop_nodes_edge= params['default_drop_nodes_edge'] 
default_drop_nodes_out = params['default_drop_nodes_out' ] 

default_reps_nodes_inp = params['default_reps_nodes_inp' ]
default_reps_nodes_edge= params['default_reps_nodes_edge']
default_reps_nodes_out = params['default_reps_nodes_out' ]

default_decay_rate = params['default_decay_rate' ]
```

```{python}
batch_size = params_run['batch_size']
max_epoch  = params_run['max_epoch']

y_var = params_data['y_var']
```

```{python}
save_prefix = [e for e in cache_path.split('/') if e != ''][-1]

if 'None' != params_data['y_resid_strat']:
    save_prefix = save_prefix+'_'+params_data['y_resid_strat']

ensure_dir_path_exists(dir_path = cache_path)
```

```{python}
use_gpu_num = 0

device = "cuda" if torch.cuda.is_available() else "cpu"
if use_gpu_num in [0, 1]: 
    torch.cuda.set_device(use_gpu_num)
print(f"Using {device} device")
```

## Standard setut for VNN

### Load Data

```{python}
# Data Prep ----
obs_geno_lookup          = get_data('obs_geno_lookup')
phno                     = get_data('phno')
ACGT_gene_slice_list     = get_data('KEGG_slices')       # 'KEGG_slices':  'ACGT_gene_slice_list.pkl',
ACGT_gene_slice_loci     = get_data('KEGG_slices_names') # 'KEGG_slices_names': 'ACGT_gene_site_name_list.pkl',
parsed_kegg_gene_entries = get_data('KEGG_entries')      # 'KEGG_entries': 'filtered_kegg_gene_entries.pkl',
```

```{python}
[len(e) for e in [ACGT_gene_slice_list, ACGT_gene_slice_loci]]
```

```{python}
ACGT_gene_slice_list[0].shape, len(ACGT_gene_slice_loci[0])
```

```{python}
# make sure that the given y variable is there
# single column version
# phno = phno.loc[(phno[y_var].notna()), ].copy()
# phno = phno.reset_index().drop(columns='index')

# multicolumn
# mask based on the y variables
na_array = phno[y_var].isna().to_numpy().sum(axis=1)
mask_no_na = list(0 == na_array)

phno = phno.loc[mask_no_na, ].copy()
phno = phno.reset_index().drop(columns='index')
```

```{python}
# update obs_geno_lookup

tmp = phno.reset_index().rename(columns={'index': 'Phno_Idx_new'}).loc[:, ['Phno_Idx_new', 'Geno_Idx']]
tmp = pd.merge(tmp,
          tmp.drop(columns='Phno_Idx_new').drop_duplicates().reset_index().rename(columns={'index': 'Phno_Idx_Orig_new'}))
tmp = tmp.sort_values('Phno_Idx_new').reset_index(drop=True)

obs_geno_lookup = tmp.to_numpy()
```

```{python}
# make holdout sets
holdout_parents = params_data['holdout_parents']

# create a mask for parent genotype
mask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)

train_mask = mask.sum(axis=1) == 0
test_mask  = mask.sum(axis=1) > 0

train_idx = train_mask.loc[train_mask].index
test_idx  = test_mask.loc[test_mask].index
```

```{python}
# convert y to residual if needed

if params_data['y_resid'] == 'None':
    pass
else:
    if params_data['y_resid_strat'] == 'naive_mean':
        # use only data in the training set (especially since testers will be more likely to be found across envs)
        # get enviromental means, subtract from observed value
        tmp = phno.loc[train_idx, ]
        env_mean = tmp.groupby(['Env_Idx']
                     ).agg(Env_Mean = (y_var, 'mean')
                     ).reset_index()
        tmp = phno.merge(env_mean)
        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']
        phno = tmp.drop(columns='Env_Mean')

    if params_data['y_resid_strat'] == 'filter_mean':
        # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations
        def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],
                                    year = 2014):
            # Within each year what hybrids are most common?
            tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')

            all_envs = set(tmp.Env)
            # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?
            # site counts for sets which contain all envs
            i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year >= i), 'Env'])) == len(all_envs)])

            before = len(set(tmp.loc[:, 'Hybrid']))
            after  = len(set(tmp.loc[(tmp.Year >= i), 'Hybrid']))
            print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')
            tmp = tmp.loc[(tmp.Year >= i), ['Env', 'Hybrid']].reset_index(drop=True)
            return tmp


        tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]
        filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) 
                          for i in list(set(phno.Year)) ]
        env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')

        env_mean = env_mean.groupby(['Env_Idx']
                          ).agg(Env_Mean = (y_var, 'mean')
                          ).reset_index()

        tmp = phno.merge(env_mean)
        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']
        phno = tmp.drop(columns='Env_Mean')
        
```

```{python}
# center and y value data
assert 0 == phno.loc[:, y_var].isna().sum().sum() # second sum is for multiple y_vars

y = phno.loc[:, y_var].to_numpy() # added to make multiple ys work
# use train index to prevent information leakage
y_c = y[train_idx].mean(axis=0)
y_s = y[train_idx].std(axis=0)

y = (y - y_c)/y_s
```

### Fit Using VNNHelper

```{python}
myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = params, ACGT_gene_slice_list = ACGT_gene_slice_list)
```

### Calculate nodes membership in each matrix and positions within each

```{python}
### Creating Structured Matrices for Layers
M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)
```

### Setup Dataloader using `M_list`


```{python}
lookup_dict = new_lookup_dict

vals = get_data('KEGG_slices')
vals = [torch.from_numpy(e).to(torch.float) for e in vals]
# restrict to the tensors that will be used
vals = torch.concat([vals[lookup_dict[i]].reshape(4926, -1) 
                     for i in M_list[0].row_inp
                    #  for i in dd[0]['inp'] # matches
                     ], axis = 1)

vals = vals.to('cuda')
```

```{python}
## build the site lookup for vals ====
vals_loci = [ACGT_gene_slice_loci[lookup_dict[i]] for i in M_list[0].row_inp]
vals_loci = sum(vals_loci, [])
# confirm there are equal loci and input snps
assert vals.shape[1]/4 == len(vals_loci)

## build up a vals_loci reference, a column on entrys for each loci ====
vals_entry = [ 
    [parsed_kegg_gene_entries[lookup_dict[i]]['ENTRY'] 
     for j in range(len(ACGT_gene_slice_loci[lookup_dict[i]]))]
     for i in M_list[0].row_inp
 ]

vals_entry = sum(vals_entry, [])
# confirm there are equal loci and input snps
assert vals.shape[1]/4 == len(vals_entry)


# package into a df 
vals_lookup_df = pd.DataFrame(zip(vals_loci.copy(), vals_entry.copy()), columns= ['snp', 'entry'])
vals_lookup_df
```

```{python}
# quick addition to filter hmp for gwas
with open('./vals_loci_filter.txt', 'w') as f:
    for e in vals_loci:
        f.writelines('\t'.join(e[1:].split('_'))+'\n')
```

```{python}
training_dataloader = DataLoader(BigDataset(
    lookups_are_filtered = False,
    lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),
    lookup_geno = torch.from_numpy(obs_geno_lookup),
    y =           torch.from_numpy(y).to(torch.float32)[:, None].squeeze(),
    G =           vals,
    G_type = 'raw',
    send_batch_to_gpu = 'cuda:0'
    ),
    batch_size = batch_size,
    shuffle = True 
)

validation_dataloader = DataLoader(BigDataset(
    lookups_are_filtered = False,
    lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),
    lookup_geno = torch.from_numpy(obs_geno_lookup),
    y =           torch.from_numpy(y).to(torch.float32)[:, None].squeeze(),
    G =           vals,
    G_type = 'raw',
    send_batch_to_gpu = 'cuda:0'
    ),
    batch_size = batch_size,
    shuffle = False 
)
```

### Structured Layer

```{python}
class NeuralNetwork(nn.Module):
    def __init__(self, layer_list):
        super(NeuralNetwork, self).__init__()
        self.layer_list = nn.ModuleList(layer_list)
 
    def forward(self, x):
        for l in self.layer_list:
            x = l(x)
        return x
```

## Load results and examine



## Get Benchmark Performance

### bWGR BLUP

```{python}
def _get_bWGR_res(
        exp_yvar = 'eres_Yield_Mg_ha', 
        result_name = 'hat' # must be one of 'hmp_info', 'hat', or b
        ):
    result_path = f'../nbs_artifacts/zma_g2f_individual_bWGR_all/output/phno_all_{exp_yvar}__5_Genotype_Data_All_Years_vals_loci'
    if result_name != 'hmp_info':
        x = pd.read_table(result_path+f'/fm_{result_name}.txt', sep=' ').reset_index()

    match result_name:
        case 'hmp_info':
             # special case that loads in the info for the snps
            with open('../nbs_artifacts/zma_g2f_individual_bWGR_all/genotypes/5_Genotype_Data_All_Years_vals_loci.hmp.txt', 'r') as f:
                x = f.readlines()
            x = [e.split('\t')[0:4] for e in x]
            x = pd.DataFrame(x[1:], columns= x[0])

        case 'hat':
             obs = pd.read_table(f'../nbs_artifacts/zma_g2f_individual_bWGR_all/phenotypes/phno_all_{exp_yvar}.txt', sep='\t', skiprows=2)
             x = x.rename(columns={'index':'Taxa', 'V1':'YHat'}).drop(columns='Taxa')
             x = pd.concat([obs, x], axis=1)
        
        case 'b':
             x = x.rename(columns={'index':'SNP_int', 'x':'b'})
        
        case _:
            print(f'{result_name} is not implmented, returning none.')
            x = None
    
    return x
```

```{python}

# Here's how to get snp specific effects
x = pd.concat([_get_bWGR_res(exp_yvar = 'eres_Yield_Mg_ha', result_name='hmp_info'), 
               _get_bWGR_res(exp_yvar = 'eres_Yield_Mg_ha', result_name='b')], axis=1)
x['abs_b'] = x.b.abs()
x
```

```{python}
px.scatter(x.reset_index(), 'index', 'abs_b', color='chrom')
```

```{python}
x = _get_bWGR_res(exp_yvar = 'eres_Yield_Mg_ha', result_name='hat')
x
```

```{python}
print(f'MSE: {(x.Yield_Mg_ha - x.YHat).pow(2).mean()}')
px.scatter(x, 'Yield_Mg_ha', 'YHat')
```

```{python}
# For all exp_yvar below
# get b -> pseudo manhattan
# get yhats -> performance benchmark

exp_yvars = [
# TODO run on all yvars
 'Ear_Height_cm',
 'Grain_Moisture',
 'Plant_Height_cm',
 'Pollen_DAP_days',
 'Silk_DAP_days',
 'Twt_kg_m3',
 'Yield_Mg_ha',
 'eres_Ear_Height_cm',
 'eres_Grain_Moisture',
 'eres_Plant_Height_cm',
 'eres_Pollen_DAP_days',
 'eres_Silk_DAP_days',
 'eres_Twt_kg_m3',
 'eres_Yield_Mg_ha']

exp_yvar = exp_yvars[-1]

# cache_path
# table = pa.Table.from_pandas(table)
# pq.write_table(table, 'table.parquet')
# x1 = pq.read_table('table.parquet').to_pandas()
```

```{python}
## Accumulate and save out the model b terms with snp annotations ====
x = [pd.concat([
    _get_bWGR_res(exp_yvar = exp_yvar, result_name='hmp_info'), 
    _get_bWGR_res(exp_yvar = exp_yvar, result_name='b')], axis=1).rename(columns={'b': f'b_{exp_yvar}'}) 
    for exp_yvar in exp_yvars
    ]
# merge summary dataframe into a large table
out = x[0]
for xx in x[1:]:
    out = out.merge(xx)

pq.write_table(pa.Table.from_pandas(out), f'{cache_path}benchmark_bWGR_b_{exp_yvar}.parquet')
out.head()
```

```{python}
## Save out the model predictions ====
# because there are a different number of observations for each yvar we can't cbind them. If I deduped the predictions then the observed would need be cut. 
# For that reason I'll just save them from the list comp.
_ = [pq.write_table(
    pa.Table.from_pandas(
        _get_bWGR_res(exp_yvar = exp_yvar, result_name='hat').rename(columns={'YHat':f'{exp_yvar}_YHat'})
    ), 
    f'{cache_path}benchmark_bWGR_hat_{exp_yvar}.parquet')
    for exp_yvar in exp_yvars
    ]
```

### GWAS on BLUEs

```{python}
def _get_gwas_results(
    gwas_output_dir = '../nbs_artifacts/zma_g2f_individual_gapit/output/',
    gwas_phno = 'Ear_Height_cm',
    gwas_geno = '5_Genotype_Data_All_Years_acgt_30000'
    ):
    gwas_result_path = gwas_output_dir+f'phno_{gwas_phno}__{gwas_geno}'
    gwas_result_name = [e for e in os.listdir(gwas_result_path) if re.match('GAPIT.Association.GWAS_Results', e)][0]
    out = pd.read_csv(gwas_result_path+'/'+gwas_result_name)
    return out
```

```{python}
def _standardize_names(df, name_dict = {'Pos':'pos', 'Chr':'chrom'}):
    "Replace specified names and then remove any '.' in any name and set all to lowercase"
    df = df.rename(columns = name_dict)
    df = df.rename(columns = {e:e.replace('.', '').lower() for e in list(df)})
    return df
```

```{python}
_ = [pq.write_table(
    pa.Table.from_pandas(
        _standardize_names(
            _get_gwas_results(
                gwas_output_dir = '../nbs_artifacts/zma_g2f_individual_bWGR_all_gapit_blink/output/',
                gwas_phno = exp_yvar,
                gwas_geno = '5_Genotype_Data_All_Years_vals_loci'
                )
            )
        ), 
    f'{cache_path}benchmark_bWGR_blink_{exp_yvar}.parquet')
    for exp_yvar in exp_yvars
    ]
```

### Just in case GWAS on non-blues?

```{python}
_ = [pq.write_table(
    pa.Table.from_pandas(
        _standardize_names(
            _get_gwas_results(
                gwas_output_dir = '../nbs_artifacts/zma_g2f_individual_gapit_blink/output/',
                gwas_phno = exp_yvar,
                gwas_geno = '5_Genotype_Data_All_Years_vals_loci'
                )
            )
        ), 
    f'{cache_path}benchmark_mean_blink_{exp_yvar}.parquet')
    for exp_yvar in exp_yvars

#TODO I will need to rerun these. Likely an issue with 02_next.sh
    if exp_yvar not in [
    'Grain_Moisture',
    'Twt_kg_m3',
    'eres_Grain_Moisture',
    'eres_Twt_kg_m3',
    ]
    ]
```

## Get Alternate Model Performance

### Hyperparameter placeholder

```{python}
# JSON path and log dir
def read_hyp_exp_results(exp_name, exp_yvar, **kwargs):
    if 'log_path' in kwargs:
        if kwargs['log_path'] != None:
            log_path = kwargs['log_path']
        else:
            log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}__{exp_yvar}'
    else:
        log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}__{exp_yvar}'
    json_path = log_path+'.json'

    # get all versions, confirm that a metrics file exists
    version_logs = os.listdir(log_path)
    version_logs = [e for e in version_logs if os.path.exists(f'{log_path}/{e}/metrics.csv')]
    # extract and sort ints
    version_logs = sorted([int(e.split('_')[-1]) for e in version_logs])

    # Produce a tidy df for each version
    def _get_tidy_metrics(metrics_path, i):
        df = pd.read_csv(metrics_path)
        df = pd.melt(
                    df, 
                    id_vars= ['step', 'epoch'],
                    var_name= 'split',
                    value_vars= ['train_loss', 'val_loss'],
                    value_name= 'loss'
                ).dropna(
                ).reset_index(drop=True
                ).assign(version = i)
        return(df)
    # all training histories
    metrics = pd.concat([_get_tidy_metrics(metrics_path = f'{log_path}/version_{i}/metrics.csv', 
                                           i = i) for i in version_logs])
    
    if os.path.exists(json_path):
        return (AxClient.load_from_json_file(filepath = json_path), metrics)
    else:
        return metrics
```

```{python}
os.listdir('../nbs_artifacts/zma_g2f_individual_0hyps/lightning')
```

```{python}
# os.listdir('../nbs_artifacts/')
# zma_g2f_individual_eres_1mods
```

```{python}
ax_client, metrics = read_hyp_exp_results(
    exp_name = 'zma_g2f_individual_0hyps',
    exp_yvar = [
        'Yield_Mg_ha',
        # 'Pollen_DAP_days',
        # 'Silk_DAP_days',
        # 'Plant_Height_cm',
        # 'Ear_Height_cm',
        # 'Grain_Moisture',
        # 'Twt_kg_m3',
        ][0]
)



# ax_client, metrics = read_hyp_exp_results(
#     exp_name = None,
#     exp_yvar = None,
#     log_path = '../nbs_artifacts/zma_g2f_individual_0hyps/lightning/zma_g2f_individual_0hyps__Yield_Mg_ha'
#     )

# metrics = read_hyp_exp_results(
#     exp_name = None,
#     exp_yvar = None,
#     log_path = '../nbs_artifacts/zma_g2f_individual_1mods/lightning/zma_g2f_individual_1mods'
#     )
```

```{python}
df = ax_client.get_trials_data_frame()
df = df.loc[:, [
    'train_loss',
    'default_out_nodes_inp',
    'default_out_nodes_edge',
    'default_drop_nodes_inp',
    'default_drop_nodes_edge',
    'default_drop_nodes_out',
    'default_reps_nodes_inp',
    'default_reps_nodes_edge',
    'default_reps_nodes_out',
    'default_decay_rate',
    # 'default_out_nodes_out'
    ]]

df_scale = df.copy()
```

```{python}
df_scale = (df_scale - df_scale.min()) / (df_scale.max() - df_scale.min())

# px.imshow(df_scale)
```

```{python}
# px.line(df, x = df.index, y = 'train_loss', title= 'Minimum Loss')
```

```{python}
tmp = metrics.copy()
# tmp.version = tmp.version.astype(float)

tmp = tmp.merge(
    pd.DataFrame(
        [[i,j] for i in range(4) for j in range(8)], columns=['row', 'col']
    ).reset_index().rename(columns={'index':'version'})
)

tmp.head()
```

```{python}
# px.line(
#     tmp.loc[
#         (tmp.split == 'train_loss'), 
#         ].groupby(['version', 'epoch', 'row', 'col']).agg(loss = ('loss', 'mean')).reset_index(),
#     x = 'epoch', y = 'loss', 
#     line_group='version', 
#     # color='version', 
#     # facet_col='version'
#     facet_col= 'col',
#     facet_row='row'
#     )
```

```{python}
# px.line(
#     tmp.loc[
#         (tmp.split == 'val_loss'), 
#         ].groupby(['version', 'epoch', 'row', 'col']).agg(loss = ('loss', 'mean')).reset_index(),
#     x = 'epoch', y = 'loss', 
#     line_group='version', 
#     # color='version', 
#     # facet_col='version'
#     facet_col= 'col',
#     facet_row='row',
#     # log_y=True
#     )
```

```{python}

# (
#     ggplot(metrics, aes("epoch", "loss", color="version"))
#     + geom_point()
#     # + stat_smooth(method="lm")
#     + facet_wrap("split")
# )
```

```{python}
# (
#     ggplot(metrics.loc[(metrics.version > 28), ], aes("epoch", "loss", color="version"))
#     + geom_line()
#     # + stat_smooth(method="lm")
#     + facet_wrap("split")
# )
```


```{python}
# exp_yvar = [
#         'Yield_Mg_ha',
#         # 'Pollen_DAP_days',
#         # 'Silk_DAP_days',
#         # 'Plant_Height_cm',
#         # 'Ear_Height_cm',
#         # 'Grain_Moisture',
#         # 'Twt_kg_m3',
#         ][0]

# exp_name = ['zma_g2f_individual_0hyps', 'zma_g2f_individual_eres_0hyps'][0]


metrics_list = []
df_list = []


for exp_name, exp_yvar in [
    (i,j) for i in 
    ['zma_g2f_individual_0hyps', 'zma_g2f_individual_eres_0hyps']
    for j in ['Yield_Mg_ha', 'Pollen_DAP_days', 'Silk_DAP_days', 
              'Plant_Height_cm', 'Ear_Height_cm', 'Grain_Moisture', 'Twt_kg_m3',]]:
    
    # zma_g2f_individual_eres_0hyps Grain_Moisture
    print(exp_name, exp_yvar)


    #FIXME the json file for 
    # exp_name = 'zma_g2f_individual_eres_0hyps',
    # exp_yvar = 'Grain_Moisture'
    # doesn't exist. Unclear why but that may need to be rerun 
    # For now I'm using this workaround.
    res = read_hyp_exp_results(
        exp_name = exp_name,
        exp_yvar = exp_yvar
    )

    # This is the case for Grain_Moisture
    if type(res) == pd.core.frame.DataFrame:
        metrics = res
        metrics['exp_name'] = exp_name
        metrics['exp_yvar'] = exp_yvar
        metrics_list.append(metrics)

    if type(res) == tuple:
        ax_client, metrics = res

        df = ax_client.get_trials_data_frame()
        df = df.loc[:, [
            'train_loss',
            'default_out_nodes_inp',
            'default_out_nodes_edge',
            'default_drop_nodes_inp',
            'default_drop_nodes_edge',
            'default_drop_nodes_out',
            'default_reps_nodes_inp',
            'default_reps_nodes_edge',
            'default_reps_nodes_out',
            'default_decay_rate',
            # 'default_out_nodes_out'
            ]]

        metrics['exp_name'] = exp_name
        df['exp_name']      = exp_name

        metrics['exp_yvar'] = exp_yvar
        df['exp_yvar']      = exp_yvar

        metrics_list.append(metrics)
        df_list.append(df)
```

```{python}
pq.write_table(
    pa.Table.from_pandas(pd.concat(df_list, axis=0)), 
    f'{cache_path}alternate_0hyps_tuner.parquet'
    )


pq.write_table(
    pa.Table.from_pandas(pd.concat(metrics_list, axis=0)), 
    f'{cache_path}alternate_0hyps_history.parquet'
    )
```


### Model Performance:

#### History

```{python}
# get model results table

def _get_1mods(exp_name = 'zma_g2f_individual_1mods'):
    metrics = read_hyp_exp_results(
        exp_name = None,
        exp_yvar = None,
        log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}'
        )

    # find all the version entries to parse
    def _get_version_holdouts(exp_name, version):
        with open(f'../nbs_artifacts/{exp_name}/lightning/{exp_name}/{version}/hparams.yaml') as f:
            hparams = yaml.safe_load(f)
        params_data = hparams['params_data']
        params_data['holdout_parents'] = '_'.join(params_data['holdout_parents'])
        params_data['y_var'] = '_'.join(params_data['y_var'])
        vnum = int(version.split('_')[-1])
        params_data = pd.DataFrame(
            params_data, 
            index=[vnum]
            ).assign(version = vnum)
        return params_data

    versions = [e for e in os.listdir(f'../nbs_artifacts/{exp_name}/lightning/{exp_name}') if re.match('version_\d+', e)]
    hparams = pd.concat([_get_version_holdouts(exp_name, version) for version in versions])

    return metrics.merge(hparams)
```

```{python}
os.listdir('../nbs_artifacts/zma_g2f_individual_1mods/lightning')
```

```{python}
## History for all models (across cvs) ====
metrics = _get_1mods(exp_name = 'zma_g2f_individual_1mods')


pq.write_table(
    pa.Table.from_pandas(metrics), 
    f'{cache_path}alternate_1mods_history.parquet'
    )


metrics.head()
```

```{python}
metrics = metrics.loc[(metrics.y_var == 'Yield_Mg_ha'), ].groupby(['holdout_parents', 'split', 'epoch', 'version']).agg(loss = ('loss', 'mean')).reset_index()
metrics
```

```{python}
# (
#     ggplot(metrics.loc[metrics.split == 'train_loss', ], aes("epoch", "loss", color="holdout_parents", group='version'))
#     + geom_line()
#     + facet_wrap("holdout_parents")
# )
```

```{python}
# (
#     ggplot(metrics.loc[metrics.split == 'val_loss', ], aes("epoch", "loss", color="holdout_parents", group='version'))
#     + geom_line()
#     + facet_wrap("holdout_parents")
# )
```

#### Model Predictions

```{python}
# get model results table

def _identify_1mods_models(
        exp_name = 'zma_g2f_individual_1mods',
        y_var = 'Yield_Mg_ha'):

    model_names = [e for e in os.listdir(f'../nbs_artifacts/{exp_name}/models/') if re.findall(y_var, e)]

    out = []
    for model_name in model_names:
        holdout_parents, replicate = model_name.split('__')[-1][:-3].split('_rep')
        out.append({
            'model_name':model_name,
            'y_var':y_var,
            'holdout_parents':holdout_parents, 
            'replicate':int(replicate),
            'path':f'../nbs_artifacts/{exp_name}/models/{model_name}',
        })

    return out   
```

```{python}
# model_info = _identify_1mods_models(
#         exp_name = 'zma_g2f_individual_1mods',
#         y_var = 'Yield_Mg_ha')
# model_info[0]

# _identify_1mods_models(
#         exp_name = 'zma_g2f_individual_eres_1mods',
#         y_var = 'Yield_Mg_ha')
```

```{python}
def _prep_data_for_prediction(
        params_data
        ):
    y_var = params_data['y_var']

    # Data Prep ----
    phno                     = get_data('phno')
    # make sure that the given y variable is there
    # single column version
    # phno = phno.loc[(phno[y_var].notna()), ].copy()
    # phno = phno.reset_index().drop(columns='index')

    # multicolumn
    # mask based on the y variables
    na_array = phno[y_var].isna().to_numpy().sum(axis=1)
    mask_no_na = list(0 == na_array)

    phno = phno.loc[mask_no_na, ].copy()
    phno = phno.reset_index().drop(columns='index')


    # update obs_geno_lookup
    tmp = phno.reset_index().rename(columns={'index': 'Phno_Idx_new'}).loc[:, ['Phno_Idx_new', 'Geno_Idx']]
    tmp = pd.merge(tmp,
            tmp.drop(columns='Phno_Idx_new').drop_duplicates().reset_index().rename(columns={'index': 'Phno_Idx_Orig_new'}))
    tmp = tmp.sort_values('Phno_Idx_new').reset_index(drop=True)

    obs_geno_lookup = tmp.to_numpy()
    
    
    # make holdout sets
    holdout_parents = params_data['holdout_parents']

    
    # create a mask for parent genotype
    mask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)

    train_mask = mask.sum(axis=1) == 0
    test_mask  = mask.sum(axis=1) > 0

    train_idx = train_mask.loc[train_mask].index
    test_idx  = test_mask.loc[test_mask].index


    # convert y to residual if needed
    if params_data['y_resid'] == 'None':
        pass
    else:
        if params_data['y_resid_strat'] == 'naive_mean':
            # use only data in the training set (especially since testers will be more likely to be found across envs)
            # get enviromental means, subtract from observed value
            for i in range(len(y_var)):
                tmp = phno.loc[train_idx, ]
                env_mean = tmp.groupby(['Env_Idx']
                            ).agg(Env_Mean = (y_var[i], 'mean')
                            ).reset_index()
                tmp = phno.merge(env_mean)
                tmp.loc[:, y_var[i]] = tmp.loc[:, y_var[i]] - tmp.loc[:, 'Env_Mean']
                phno = tmp.drop(columns='Env_Mean')

        if params_data['y_resid_strat'] == 'filter_mean':
            # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations
            def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],
                                        year = 2014):
                # Within each year what hybrids are most common?
                tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')

                all_envs = set(tmp.Env)
                # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?
                # site counts for sets which contain all envs
                i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year >= i), 'Env'])) == len(all_envs)])

                before = len(set(tmp.loc[:, 'Hybrid']))
                after  = len(set(tmp.loc[(tmp.Year >= i), 'Hybrid']))
                print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')
                tmp = tmp.loc[(tmp.Year >= i), ['Env', 'Hybrid']].reset_index(drop=True)
                return tmp

            for i in range(len(y_var)):
                tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]
                filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) 
                                for i in list(set(phno.Year)) ]
                env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')

                env_mean = env_mean.groupby(['Env_Idx']
                                ).agg(Env_Mean = (y_var[i], 'mean')
                                ).reset_index()

                tmp = phno.merge(env_mean)
                tmp.loc[:, y_var[i]] = tmp.loc[:, y_var[i]] - tmp.loc[:, 'Env_Mean']
                phno = tmp.drop(columns='Env_Mean')


    # center and y value data
    assert 0 == phno.loc[:, y_var].isna().sum().sum() # second sum is for multiple y_vars

    y = phno.loc[:, y_var].to_numpy() # added to make multiple ys work
    # use train index to prevent information leakage
    y_c = y[train_idx].mean(axis=0)
    y_s = y[train_idx].std(axis=0)

    y = (y - y_c)/y_s

    # y, y_c, y_s

    ## Ref df of targets ==== 
    phno_out = phno.copy()
    phno_out = phno_out.loc[:, ['Hybrid']+y_var]
    phno_out.loc[:, 'Train'] = False
    phno_out.loc[train_idx, 'Train'] = True
    phno_out['center'] = float(y_c[0])
    phno_out['scale'] = float(y_s[0])


    ## df to cbind in yhats ====
    # # here are the values for each hybrid
    # idx_of_genos = pd.DataFrame(obs_geno_lookup
    #                 ).loc[:, [1, 2]
    #                 ].drop_duplicates(
    #                 ).sort_values(1, ascending=True
    #                 ).loc[:, 2].values.tolist()
    # Hybrids = phno.loc[idx_of_genos, 'Hybrid'].values.tolist()
    # out = pd.DataFrame({'Hybrid':Hybrids, 'geno_idx':idx_of_genos})
    # out = pd.DataFrame(yhat, columns=[f'hat_{"_".join(y_var)}'])
    # out['Hybrids'] = Hybrids

    # Create a deduplicated df of the hybrids with the index info for vals (ig)
    idx_of_genos = pd.DataFrame(obs_geno_lookup, columns = ['iy', 'ig', 'iy1'])
    tmp = idx_of_genos.loc[:, ['iy1']].drop_duplicates()
    tmp.loc[:, 'Hybrid'] = phno.loc[(tmp['iy1'].values.tolist()), 'Hybrid'].values.tolist()
    idx_of_genos = idx_of_genos.loc[:, ['ig', 'iy1']
                              ].merge(tmp
                              ).drop_duplicates(
                              ).reset_index(drop=True
                              ).drop(columns = 'iy1')

    return phno_out, idx_of_genos
```


```{python}
exp_names = ['zma_g2f_individual_1mods', 'zma_g2f_individual_eres_1mods']


# exp_name  = exp_names[0]
# exp_yvar = 'Yield_Mg_ha' #exp_yvars[0]

for exp_name, exp_yvar in [(i,j) for i in exp_names for j in exp_yvars]:

    models_info = _identify_1mods_models(exp_name = exp_name, y_var = exp_yvar)
    if models_info == []: # confirm there are models to load (e.g. exp_name is eres in non eres exp.)
        pass
    else:
        # model_info = models_info[0]
        for model_info in tqdm(models_info):

            if type(exp_yvar) != list:
                y_var = [exp_yvar]

            y_ref, yhat_hybrids =_prep_data_for_prediction(
                    params_data = {
                        'y_var': y_var,
                        # set residual prediction strategy correctly
                        'y_resid':       ['None' if re.findall('_eres_', e) == [] else 'naive_mean' for e in [exp_name]][0],
                        'y_resid_strat': ['None' if re.findall('_eres_', e) == [] else 'naive_mean' for e in [exp_name]][0], # ignored if y_resid == "None"
                        'holdout_parents': model_info['holdout_parents'].split('_')
                        }
                    )

            ## Load model and run inference 
            model = torch.load(model_info['path']).to('cuda')
            yhat = model(vals)
            # yhat = yhat.to('cpu').detach().numpy()
            yhat = yhat.detach().cpu().numpy()

            # index into yhat to make everything match, then overwrite and cbind
            yhat = pd.DataFrame(yhat[yhat_hybrids.loc[:, 'ig'].values.tolist()], columns=[f'hat_{e}' for e in y_var])
            yhat_hybrids = pd.concat([yhat_hybrids, yhat], axis=1).drop(columns='ig')

            out = y_ref.merge(yhat_hybrids)


            if_eres = ["" if re.findall("_eres_", e) == [] else "eres_" for e in [exp_name]][0]

            pq.write_table(
                pa.Table.from_pandas(out), 
                cache_path+model_info['model_name'].replace('vnn__', f'alternate_1mods_hat__{if_eres}').replace('.pt', '.parquet')
                )
            
            # explictly free up gpu space (not being released when expected)
            del model
            torch.cuda.empty_cache()   
```

#### Salience over CV:

```{python}
def _prep_data_to_dls(
        # obs_geno_lookup,
        vals,
        batch_size,
        params_data
        ):
    y_var = params_data['y_var']

    # Data Prep ----
    phno                     = get_data('phno')
    # make sure that the given y variable is there
    # single column version
    # phno = phno.loc[(phno[y_var].notna()), ].copy()
    # phno = phno.reset_index().drop(columns='index')

    # multicolumn
    # mask based on the y variables
    na_array = phno[y_var].isna().to_numpy().sum(axis=1)
    mask_no_na = list(0 == na_array)

    phno = phno.loc[mask_no_na, ].copy()
    phno = phno.reset_index().drop(columns='index')


    # update obs_geno_lookup
    tmp = phno.reset_index().rename(columns={'index': 'Phno_Idx_new'}).loc[:, ['Phno_Idx_new', 'Geno_Idx']]
    tmp = pd.merge(tmp,
            tmp.drop(columns='Phno_Idx_new').drop_duplicates().reset_index().rename(columns={'index': 'Phno_Idx_Orig_new'}))
    tmp = tmp.sort_values('Phno_Idx_new').reset_index(drop=True)

    obs_geno_lookup = tmp.to_numpy()
    
    
    # make holdout sets
    holdout_parents = params_data['holdout_parents']

    
    # create a mask for parent genotype
    mask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)

    train_mask = mask.sum(axis=1) == 0
    test_mask  = mask.sum(axis=1) > 0

    train_idx = train_mask.loc[train_mask].index
    test_idx  = test_mask.loc[test_mask].index


    # convert y to residual if needed
    if params_data['y_resid'] == 'None':
        pass
    else:
        if params_data['y_resid_strat'] == 'naive_mean':
            # use only data in the training set (especially since testers will be more likely to be found across envs)
            # get enviromental means, subtract from observed value
            for i in range(len(y_var)):
                tmp = phno.loc[train_idx, ]
                env_mean = tmp.groupby(['Env_Idx']
                            ).agg(Env_Mean = (y_var[i], 'mean')
                            ).reset_index()
                tmp = phno.merge(env_mean)
                tmp.loc[:, y_var[i]] = tmp.loc[:, y_var[i]] - tmp.loc[:, 'Env_Mean']
                phno = tmp.drop(columns='Env_Mean')

        if params_data['y_resid_strat'] == 'filter_mean':
            # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations
            def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],
                                        year = 2014):
                # Within each year what hybrids are most common?
                tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')

                all_envs = set(tmp.Env)
                # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?
                # site counts for sets which contain all envs
                i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year >= i), 'Env'])) == len(all_envs)])

                before = len(set(tmp.loc[:, 'Hybrid']))
                after  = len(set(tmp.loc[(tmp.Year >= i), 'Hybrid']))
                print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')
                tmp = tmp.loc[(tmp.Year >= i), ['Env', 'Hybrid']].reset_index(drop=True)
                return tmp

            for i in range(len(y_var)):
                tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]
                filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) 
                                for i in list(set(phno.Year)) ]
                env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')

                env_mean = env_mean.groupby(['Env_Idx']
                                ).agg(Env_Mean = (y_var[i], 'mean')
                                ).reset_index()

                tmp = phno.merge(env_mean)
                tmp.loc[:, y_var[i]] = tmp.loc[:, y_var[i]] - tmp.loc[:, 'Env_Mean']
                phno = tmp.drop(columns='Env_Mean')


    # center and y value data
    assert 0 == phno.loc[:, y_var].isna().sum().sum() # second sum is for multiple y_vars

    y = phno.loc[:, y_var].to_numpy() # added to make multiple ys work
    # use train index to prevent information leakage
    y_c = y[train_idx].mean(axis=0)
    y_s = y[train_idx].std(axis=0)

    y = (y - y_c)/y_s



    training_dataloader = DataLoader(BigDataset(
        lookups_are_filtered = False,
        lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),
        lookup_geno = torch.from_numpy(obs_geno_lookup),
        y =           torch.from_numpy(y).to(torch.float32)[:, None],
        G =           vals,
        G_type = 'raw',
        send_batch_to_gpu = 'cuda:0'
        ),
        batch_size = batch_size,
        shuffle = True 
    )

    validation_dataloader = DataLoader(BigDataset(
        lookups_are_filtered = False,
        lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),
        lookup_geno = torch.from_numpy(obs_geno_lookup),
        y =           torch.from_numpy(y).to(torch.float32)[:, None],
        G =           vals,
        G_type = 'raw',
        send_batch_to_gpu = 'cuda:0'
        ),
        batch_size = batch_size,
        shuffle = False 
    )

    return training_dataloader, validation_dataloader
```

```{python}
# get saliency for all obs given model, dataloader
def _get_saliency(y_i, x_i, model):
    # y_i, x_i = (y_i.to('cpu'), x_i.to('cpu'))
    x_i.requires_grad_()
    model.eval()

    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)
    loss_fn = nn.MSELoss()

    loss = loss_fn(model(x_i), y_i)
    optimizer.zero_grad()
    loss.backward()
    out = x_i.grad
    return out
```

```{python}
for exp_name, exp_yvar in [(i,j) for i in exp_names for j in exp_yvars]:

    models_info = _identify_1mods_models(exp_name = exp_name, y_var = exp_yvar)
    if models_info == []: # confirm there are models to load (e.g. exp_name is eres in non eres exp.)
        pass
    else:
        for model_info in models_info:
            print(exp_name, exp_yvar, model_info)

            
            break
        break
```

```{python}
# setup 
saliency_loci = pd.concat([vals_lookup_df, vals_lookup_df['snp'].str.split('_', expand=True).rename(columns={0:'chrom', 1:'pos'})], axis=1)
saliency_loci.chrom = saliency_loci.chrom.str.replace('S', '')

for e in ['chrom', 'pos']: saliency_loci.loc[:, e] = saliency_loci.loc[:, e].astype(int)

saliency_loci = saliency_loci.reset_index().rename(columns={'index':'saliency_idx'})
saliency_loci['chrom'] = saliency_loci['chrom'].astype(int)
saliency_loci['pos'] = saliency_loci['pos'].astype(int)

pq.write_table(
    pa.Table.from_pandas(
    saliency_loci),
    cache_path+"alternate_1mods_salience_loci.parquet"
    )

saliency_loci.head()
```

```{python}
for exp_name, exp_yvar in [(i,j) for i in exp_names for j in exp_yvars]:

    models_info = _identify_1mods_models(exp_name = exp_name, y_var = exp_yvar)
    if models_info == []: # confirm there are models to load (e.g. exp_name is eres in non eres exp.)
        pass
    else:
        model_names = []
        trn_sal = []
        val_sal = []

        for model_info in tqdm(models_info[:2]):
        # for model_info in tqdm(models_info): #TODO
            print(exp_name, exp_yvar, model_info)
            def _get_saliency_for_dataloader(model_info, dl):
                # add in salience to all entries
                all_grads = [
                    _get_saliency(
                        y_i = y_i.to('cuda'), 
                        x_i = x_i.to('cuda'),
                        model = 
                        torch.load(model_info['path']).to('cuda')
                        # detach and transfer to cpu to prevent this from blowing up       
                        ).detach().cpu()

                        for idx, (y_i, x_i) in 
                            enumerate(iter(dl))
                            
                        ]

                all_grads = torch.concat(all_grads)
                # Saliency across all obs _not_ all genotypes
                saliency, _ = all_grads.reshape(all_grads.shape[0], 4, -1).abs().max(dim=1) # returns values, indices for max. Thats why the later is discarded
                # then repeat to get the maximum at the per-snp level
                saliency = saliency.max(axis=0)[0]
                return saliency
                

            training_dataloader, validation_dataloader = _prep_data_to_dls(
                    # obs_geno_lookup,
                    vals = vals,
                    batch_size = batch_size,
                    params_data = {
                        'y_var': y_var,
                        # set residual prediction strategy correctly
                        'y_resid':       ['None' if re.findall('_eres_', e) == [] else 'naive_mean' for e in [exp_name]][0],
                        'y_resid_strat': ['None' if re.findall('_eres_', e) == [] else 'naive_mean' for e in [exp_name]][0], # ignored if y_resid == "None"
                        'holdout_parents': model_info['holdout_parents'].split('_')
                        }
                    )
        

            model_names.append(model_info['model_name'])

            trn_sal.append( _get_saliency_for_dataloader(model_info = model_info, dl = training_dataloader)   )
            val_sal.append( _get_saliency_for_dataloader(model_info = model_info, dl = validation_dataloader) )
            
            # break

        # concat across replicates and write out a reference salience dataframe
        if_eres = ["" if re.findall("_eres_", e) == [] else "eres_" for e in [exp_name]][0]
        model_cols = [e.replace('vnn__', f'{if_eres}').replace('.pt', '') for e in model_names]

        for split, dat_sal in (('trn', trn_sal),('val', val_sal)):
            pq.write_table(pa.Table.from_pandas(pd.DataFrame(
                torch.concatenate([e[:, None] for e in dat_sal], axis=1).numpy(), 
                columns=model_cols)),
                cache_path+f"alternate_1mods_salience_{split}_{if_eres}{model_info['model_name'].split('__')[1]}.parquet"
                )
```

```{python}
Stop here!
```





























```{python}
# # get model results table

# def _identify_1mods_models(
#         exp_name = 'zma_g2f_individual_1mods',
#         y_var = 'Yield_Mg_ha'):

#     model_names = [e for e in os.listdir(f'../nbs_artifacts/{exp_name}/models/') if re.findall(y_var, e)]

#     out = []
#     for model_name in model_names:
#         holdout_parents, replicate = model_name.split('__')[-1][:-3].split('_rep')
#         out.append({
#             'model_name':model_name,
#             'y_var':y_var,
#             'holdout_parents':holdout_parents, 
#             'replicate':int(replicate),
#             'path':f'../nbs_artifacts/{exp_name}/models/{model_name}',
#         })

#     return out   


# model_info = _identify_1mods_models(
#         exp_name = 'zma_g2f_individual_1mods',
#         y_var = 'Yield_Mg_ha')
# model_info
```

```{python}
# model = torch.load(
#     model_info[0]['path']
#     )
# model_info[0]
```

```{python}
# get saliency for all obs given model, dataloader
def _get_saliency(y_i, x_i, model):
    # y_i, x_i = (y_i.to('cpu'), x_i.to('cpu'))
    x_i.requires_grad_()
    model.eval()

    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)
    loss_fn = nn.MSELoss()

    loss = loss_fn(model(x_i), y_i)
    optimizer.zero_grad()
    loss.backward()
    out = x_i.grad
    return out
```

```{python}
# get saliencey for a single model over all minibatches
# ~2min
# all_grads = [
#     _get_saliency(
#         y_i = y_i, 
#         x_i = x_i,
#         model = model)
#         for idx, (y_i, x_i) in 
#             enumerate(iter(training_dataloader))
            
#         ]

# # [torch.Size([256, 144468

# all_grads = torch.concat(all_grads)
# # Saliency across all obs _not_ all genotypes
# saliency, _ = all_grads.reshape(all_grads.shape[0], 4, -1).abs().max(dim=1)

# px.histogram(saliency[0, :].numpy(), log_y=True)
```

```{python}

# add in salience to all entries
for e in tqdm(model_info):
    # this is blowing up the memory usage.
    all_grads = [
        _get_saliency(
            y_i = y_i.to('cuda'), 
            x_i = x_i.to('cuda'),
            model = 
            torch.load(e['path']).to('cuda')
            # detach and transfer to cpu to prevent this from blowing up       
            ).detach().cpu()

            for idx, (y_i, x_i) in 
                enumerate(iter(training_dataloader))
                
            ]

    all_grads = torch.concat(all_grads)
    # Saliency across all obs _not_ all genotypes
    saliency, _ = all_grads.reshape(all_grads.shape[0], 4, -1).abs().max(dim=1)
    e['saliency'] = saliency
```

```{python}
# setup 
saliency_loci = pd.concat([vals_lookup_df, vals_lookup_df['snp'].str.split('_', expand=True).rename(columns={0:'chrom', 1:'pos'})], axis=1)
saliency_loci.chrom = saliency_loci.chrom.str.replace('S', '')

for e in ['chrom', 'pos']: saliency_loci.loc[:, e] = saliency_loci.loc[:, e].astype(int)

saliency_loci = saliency_loci.reset_index().rename(columns={'index':'saliency_idx'})
saliency_loci['chrom'] = saliency_loci['chrom'].astype(int)
saliency_loci['pos'] = saliency_loci['pos'].astype(int)

saliency_loci
```

```{python}
for e in model_info:
    # saliency = model_info[0]['saliency']
    saliency = e['saliency']
    saliency_loci['saliency'] = saliency.numpy().max(axis = 0)
    tmp = saliency_loci.copy() # copy because we insert the values directly into a col of the df
    tmp = tmp.sort_values(['chrom', 'pos']).reset_index(drop = True)
    fig = px.scatter(tmp, x = tmp.index, y = 'saliency', color='chrom')
    fig.show()
```

```{python}
## group by gene 

for e in model_info:
    # saliency = model_info[0]['saliency']
    saliency = e['saliency']
    saliency_loci['saliency'] = saliency.numpy().max(axis = 0)   
    tmp = saliency_loci.copy() # copy because we insert the values directly into a col of the df
    tmp = tmp.groupby(['chrom', 'entry']).agg(
        saliency = ('saliency', 'max'),
        pos = ('pos', 'max')        
        ).reset_index()

    tmp = tmp.sort_values(['chrom', 'pos']).reset_index(drop = True)
    fig = px.scatter(tmp, x = tmp.index, y = 'saliency', color='chrom')
    fig.show()
```

```{python}
x = np.concatenate([e['saliency'].numpy().max(axis = 0)[:, None] for e in model_info], axis=1)
```

```{python}
x.shape
```

```{python}
import scipy

_ = [print(f"""{i},{j}, {
    scipy.stats.spearmanr(
        x[:, i].squeeze(), 
        x[:, j].squeeze()
        )}
        """) for i in range(5) for j in range(5) if i <j]
```

```{python}
px.line(y = x.var(axis=1))
px.line(y = x.mean(axis=1))
```


```{python}
# what's high across all the replicates?
mu_div_var = x.mean(axis=1)/x.var(axis=1)

px.histogram(x = mu_div_var)
```

```{python}
saliency_loci['saliency'] = x.var(axis=1)
tmp = saliency_loci.copy() # copy because we insert the values directly into a col of the df
tmp = tmp.sort_values(['chrom', 'pos']).reset_index(drop = True)
fig = px.scatter(tmp, x = tmp.index, y = 'saliency', color='chrom')
fig.show()
```

```{python}
tmp
```


### Between Models: One vs Multiple Targets

#### Performance:

#### Salience?:

### Between Models:

#### Performance vs Linear Model:

#### Salience vs GWAS:



### GWAS

```{python}
# def _get_gwas_results(
#     gwas_output_dir = '../nbs_artifacts/zma_g2f_individual_gapit/output/',
#     gwas_phno = 'Ear_Height_cm',
#     gwas_geno = '5_Genotype_Data_All_Years_acgt_30000'
#     ):
#     gwas_result_path = gwas_output_dir+f'phno_{gwas_phno}__{gwas_geno}'
#     gwas_result_name = [e for e in os.listdir(gwas_result_path) if re.match('GAPIT.Association.GWAS_Results', e)][0]
#     out = pd.read_csv(gwas_result_path+'/'+gwas_result_name)
#     return out
```

```{python}
# # os.listdir('../nbs_artifacts/zma_g2f_individual_gapit/output')

# # [e.split('__')[0].replace('phno_', '') for e in os.listdir('../nbs_artifacts/zma_g2f_individual_gapit_blink/output')]


# # candidate ys
# sum([[e, 'eres_'+e] for e in 
# ['Yield_Mg_ha',
#  'Silk_DAP_days',
#  'Plant_Height_cm',
#  'Pollen_DAP_days',
#  'Ear_Height_cm']], [])
```

```{python}
# gapit_res = _get_gwas_results(
#     gwas_output_dir = '../nbs_artifacts/zma_g2f_individual_gapit_blink/output/',
#     gwas_phno = 'Yield_Mg_ha',
#     # gwas_phno = 'Ear_Height_cm',
#     gwas_geno = '5_Genotype_Data_All_Years_vals_loci'
#     )
```

```{python}
# def _standardize_names(df, name_dict = {'Pos':'pos', 'Chr':'chrom'}):
#     "Replace specified names and then remove any '.' in any name and set all to lowercase"
#     df = df.rename(columns = name_dict)
#     df = df.rename(columns = {e:e.replace('.', '').lower() for e in list(df)})
#     return df
```

```{python}
# gapit_res = _standardize_names(gapit_res)

# gapit_res['nlogp'] = -np.log(gapit_res['pvalue'])

# gapit_res
```

```{python}
# px.scatter(gapit_res, x = gapit_res.index, y = 'nlogp', color='chrom')
```

```{python}

# tmp = saliency_loci.merge(gapit_res)
# tmp['flag'] = ''
# tmp.loc[(tmp['h&bpvalue'] < 0.05), 'flag'] = 'sig'


# mask = (tmp['saliency'] > np.quantile(
#     tmp['saliency'],
#     q = .95
# ))

# tmp.loc[mask, 'flag'] = tmp.loc[mask, 'flag']+'sal'

# px.scatter(
#     tmp, 
#     x = 'saliency', 
#     y = 'nlogp', 
#     color='flag')
```

```{python}
# scipy.stats.spearmanr(tmp.nlogp, tmp.saliency)
```


```{python}
# x = torch.zeros((2,3))
# x[0, 0] = 1
# x[1, 1] = 1
# x # design matrix
```

```{python}
# # marker effects
# N_s  = torch.ones((1)) # shared variance
# N_mu = torch.rand((3)) # indpendent means


```

```{python}
# # N_mu * x
# torch.distributions.normal.Normal(
#     N_mu,
#     N_s
#     ).rsample() @ x[0, ]
```

```{python}
# torch.distributions.normal.Normal(
#     torch.ones((3)),
#     1
#     ).rsample() * x[0, ]
```


```{python}
# can I just use this and modify holdout parents?
```


```{python}
def _prep_dls(
        y_var,
        train_idx,
        obs_geno_lookup,
        vals,
        batch_size,
        params_data):

    phno                     = get_data('phno')

    # multicolumn
    # mask based on the y variables
    # na_array = phno[y_var].isna().to_numpy().sum(axis=1)
    na_array = phno[ [y_var] if type(y_var) == list else y_var ].isna().to_numpy()
    if len(na_array.shape) == 2:
        na_array = na_array.sum(axis=1)
    mask_no_na = list(0 == na_array)

    phno = phno.loc[mask_no_na, ].copy()
    phno = phno.reset_index().drop(columns='index')


    # update obs_geno_lookup
    tmp = phno.reset_index().rename(columns={'index': 'Phno_Idx_new'}).loc[:, ['Phno_Idx_new', 'Geno_Idx']]
    tmp = pd.merge(tmp,
                tmp.drop(columns='Phno_Idx_new').drop_duplicates().reset_index().rename(columns={'index': 'Phno_Idx_Orig_new'}))
    tmp = tmp.sort_values('Phno_Idx_new').reset_index(drop=True)
    obs_geno_lookup = tmp.to_numpy()


    # make holdout sets
    holdout_parents = params_data['holdout_parents']

    # create a mask for parent genotype
    mask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)

    train_mask = mask.sum(axis=1) == 0
    test_mask  = mask.sum(axis=1) > 0

    train_idx = train_mask.loc[train_mask].index
    test_idx  = test_mask.loc[test_mask].index




    # center and y value data
    assert 0 == phno.loc[:, y_var].isna().sum().sum() # second sum is for multiple y_vars

    y = phno.loc[:, y_var].to_numpy() # added to make multiple ys work
    # use train index to prevent information leakage
    y_c = y[train_idx].mean(axis=0)
    y_s = y[train_idx].std(axis=0)

    y = (y - y_c)/y_s



    training_dataloader = DataLoader(BigDataset(
        lookups_are_filtered = False,
        lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),
        lookup_geno = torch.from_numpy(obs_geno_lookup),
        y =           torch.from_numpy(y).to(torch.float32)[:, None],
        G =           vals,
        G_type = 'raw',
        send_batch_to_gpu = 'cuda:0'
        ),
        batch_size = batch_size,
        shuffle = True 
    )

    validation_dataloader = DataLoader(BigDataset(
        lookups_are_filtered = False,
        lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),
        lookup_geno = torch.from_numpy(obs_geno_lookup),
        y =           torch.from_numpy(y).to(torch.float32)[:, None],
        G =           vals,
        G_type = 'raw',
        send_batch_to_gpu = 'cuda:0'
        ),
        batch_size = batch_size,
        shuffle = False 
    )

    return training_dataloader, validation_dataloader
```

```{python}
training_dataloader, validation_dataloader = _prep_dls(
            y_var = y_var[0],
            train_idx = train_idx,
            obs_geno_lookup = obs_geno_lookup,
            vals = vals,
            batch_size = batch_size,
            params_data = params_data)
```

```{python}
# Extracted from evaluate
parameterization = params

myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = parameterization, ACGT_gene_slice_list = ACGT_gene_slice_list)
M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)
layer_list =  vnn_factory_3(M_list = M_list)
model = NeuralNetwork(layer_list = layer_list)

VNN = plDNN_general(model)  
optimizer = VNN.configure_optimizers()
logger = CSVLogger(lightning_log_dir, name=exp_name)
logger.log_hyperparams(params={
    'params': parameterization
})

trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)
trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=
validation_dataloader)


# ################################################################################
# {'default_out_nodes_inp': 7, 'default_out_nodes_edge': 21, 'default_drop_nodes_inp': 0.15662193443756378, 'default_drop_nodes_edge': 0.01, 'default_drop_nodes_out': 0.01, 'default_reps_nodes_inp': 1, 'default_reps_nodes_edge': 1, 'default_reps_nodes_out': 1, 'default_decay_rate': 2.0, 'default_out_nodes_out': 1}
# ################################################################################
# Retaining 43.53%, 6067/13939 Entries

# GPU available: True (cuda), used: True
# TPU available: False, using: 0 TPU cores
# IPU available: False, using: 0 IPUs
# HPU available: False, using: 0 HPUs
# LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

#   | Name | Type          | Params
# ---------------------------------------
# 0 | mod  | NeuralNetwork | 915 M 
# ---------------------------------------
# 227 K     Trainable params
# 915 M     Non-trainable params
# 915 M     Total params
# 3,661.026 Total estimated model params size (MB)
```

```{python}
# Manual train loop to look at fitting within one epoch
parameterization = params
myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = parameterization, ACGT_gene_slice_list = ACGT_gene_slice_list)
M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)
layer_list =  vnn_factory_3(M_list = M_list)
model = NeuralNetwork(layer_list = layer_list)
```

```{python}
model.to('cuda')
```

```{python}
loss_fn = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), )

max_i = len(training_dataloader.dataset)


def _eval_model(dataloader = training_dataloader):
    model.eval()
    out = list()
    for batch, (yi, Xi) in enumerate(dataloader):
        yhat = model(Xi)
        loss = loss_fn(yhat, yi)
        out.append( (batch, loss.item(), len(Xi)) )
    return out

results = list()
results.append(
    pd.DataFrame(
        _eval_model(dataloader = validation_dataloader), 
        columns=['batch', 'loss', 'n']
        ).assign(split = 'validate', minibatch = -1)
)

for batch, (yi, Xi) in enumerate(training_dataloader):
    if not model.training:
        model.train()

    yhat = model(Xi)
    loss = loss_fn(yhat, yi)

    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    results.append(
    pd.DataFrame(
        _eval_model(dataloader = validation_dataloader), 
        columns=['batch', 'loss', 'n']
        ).assign(split = 'validate', minibatch = batch)
    )
    # takes about 7 min
```

```{python}
res = pd.concat(results)

# px.line(res, x = 'minibatch', y = 'loss', line_group='batch')
# px.scatter(res, x = 'minibatch', y = 'loss', trendline='lowess')

# px.box(res, x = 'minibatch', y = 'loss')
```

```{python}
from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap, geom_smooth, geom_ribbon

def q10(x): return x.quantile(0.10)
def q25(x): return x.quantile(0.25)
def q50(x): return x.quantile(0.50)
def q75(x): return x.quantile(0.75)
def q90(x): return x.quantile(0.90)

res_sum = res.loc[:, ['minibatch', 'split', 'loss']].groupby(['minibatch', 'split']).agg(
    mean=('loss', 'mean'),
    std = ('loss', 'std'),
    q25 =('loss', q25),
    q50 =('loss', q50),
    q75 =('loss', q75),    
    q10 =('loss', q10),
    q90 =('loss', q90),    
    ).reset_index()

res_sum = res_sum.assign(low = res_sum['mean']-res_sum['std']
                ).assign(high= res_sum['mean']+res_sum['std'])

(
    ggplot()
    + geom_point(data=res, mapping= aes('minibatch', 'loss'),                          color = '#d1d1d1')
    + geom_ribbon(data=res_sum, mapping= aes('minibatch', ymax = 'high', ymin = 'low'), fill = '#9d9d9d', alpha = 0.3)
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'mean'),                      color = 'gray')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q50'), color = 'cornflowerblue', linetype = '-')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q25'), color = 'cornflowerblue', linetype = '-.')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q75'), color = 'cornflowerblue', linetype = '-.')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q10'), color = 'cornflowerblue', linetype = ':')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q90'), color = 'cornflowerblue', linetype = ':')
)
```

```{python}
res = res.loc[(res.minibatch < 10), ]


res_sum = res.loc[:, ['minibatch', 'split', 'loss']].groupby(['minibatch', 'split']).agg(
    mean=('loss', 'mean'),
    std = ('loss', 'std'),
    q25 =('loss', q25),
    q50 =('loss', q50),
    q75 =('loss', q75),    
    q10 =('loss', q10),
    q90 =('loss', q90),    
    ).reset_index()

res_sum = res_sum.assign(low = res_sum['mean']-res_sum['std']
                ).assign(high= res_sum['mean']+res_sum['std'])

(
    ggplot()
    + geom_point(data=res, mapping= aes('minibatch', 'loss'),                          color = '#d1d1d1')
    + geom_ribbon(data=res_sum, mapping= aes('minibatch', ymax = 'high', ymin = 'low'), fill = '#9d9d9d', alpha = 0.3)
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'mean'),                      color = 'gray')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q50'), color = 'cornflowerblue', linetype = '-')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q25'), color = 'cornflowerblue', linetype = '-.')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q75'), color = 'cornflowerblue', linetype = '-.')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q10'), color = 'cornflowerblue', linetype = ':')
    + geom_smooth(data=res_sum, mapping= aes('minibatch', 'q90'), color = 'cornflowerblue', linetype = ':')
)
```







```{python}
# y_vars = y_var.copy()
```

```{python}
# for y_var in y_vars:
#     # y_var = ith_y_var
#     # update exp_name
#     exp_name = [e for e in cache_path.split('/') if e != ''][-1]
#     exp_name += '__'+y_var

#     print(''.join(['-' for i in range(80)]))
#     print(f'experiment: {exp_name}')
#     print(''.join(['-' for i in range(80)]))
#     print('\n')

#     training_dataloader, validation_dataloader = _prep_dls(
#             y_var = y_var,
#             train_idx = train_idx,
#             obs_geno_lookup = obs_geno_lookup,
#             vals = vals,
#             batch_size = batch_size,
#             params_data = params_data)

#     ## Generated variables ====
#     json_path = f"./{lightning_log_dir}/{exp_name}.json"

#     loaded_json = False
#     if os.path.exists(json_path): 
#         ax_client = (AxClient.load_from_json_file(filepath = json_path))
#         loaded_json = True

#     else:
#         ax_client = AxClient()
#         ax_client.create_experiment(
#             name=exp_name,
#             parameters=params_list,
#             objectives={"train_loss": ObjectiveProperties(minimize=True)}
#         )

#     run_trials_bool = True
#     if run_hyps_force == False:
#         if loaded_json: 
#             # check if we've reached the max number of hyperparamters combinations to test
#             if max_hyps <= (ax_client.generation_strategy.trials_as_df.index.max()+1):
#                 run_trials_bool = False

#     if run_trials_bool:
#         # run the trials
#         for i in range(run_hyps):
#             parameterization, trial_index = ax_client.get_next_trial()
#             # Local evaluation here can be replaced with deployment to external system.
#             ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameterization))

#         ax_client.save_to_json_file(filepath = json_path)
```



```{python}
# if there is not hyperparameter tuning then this function returns only the training history
metrics = read_hyp_exp_results(exp_name = exp_name, exp_yvar = y_var[0], log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}')
```

```{python}
px.scatter(metrics, x = 'epoch', y = 'loss', color = 'split', trendline='lowess')
```

```{python}
# mod = trainer.model.mod.to('cuda')
mod = trainer.model.mod.to('cpu')
```

```{python}
yi, xi = next(iter(training_dataloader))

yi, xi = yi.to('cpu'), xi.to('cpu')
```

```{python}
opt = trainer.optimizers[0]

yhat = mod(xi)
loss = nn.MSELoss()(yhat, yi)
opt.zero_grad()
loss.backward()
```

```{python}
px.histogram(mod.layer_list[-1].weights.grad)
```

```{python}
px.histogram(mod.layer_list[0].weights.grad)
```

```{python}
mod.layer_list[0].weights.grad
xi.shape
```

```{python}
mod.layer_list[0].weight
```

```{python}



# mod.layer_list[0].weight.values()


# mod.layer_list[0].weight
```

```{python}
# this is a suitable way to get the relevant weights for a node. 

#TODO separate node weights by 
ith_layer = 0
M = M_list[ith_layer]

idxs = mod.layer_list[0].weight.indices()
vals = mod.layer_list[0].weight.values()

out = M.col_out[0] # list of nodes

c1, c2 = (M.col_info[out][e] for e in ['start', 'stop']) # col_info appears to be the 0th axis

mask = ((idxs[0] >= c1) & (idxs[0] < c2))


vals[mask]


# r1, r2 = (M.row_info[out][e] for e in ['start', 'stop']) # so this 
```

```{python}
max([M.col_info[e]['stop'] for e in M.col_out])
```


```{python}
# [e for e in dir(mod.layer_list[0]) if e[0] != '_']
```


```{python}
# for the below to work I need to either reproduce edge dict creation OR extract it from the existing information. I'll do the latter.
i = -1
l = M_list[i]
# l.col_info
# l.row_info


l.col_out # these are keys

l.row_inp # these are values to assign to keys

# after the fact I'll go through and clean the graph so that references pointing to itself are removed

```

```{python}
# Copy code I wrote in aim_2a_B_VNN.ipynb
## Interpretability
```

```{python}
model = trainer.model.mod
```

```{python}
# set up gradients for testing

yi, xi = next(iter(training_dataloader))
yi = yi.to('cpu')
xi = xi.to('cpu')


optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)
loss_fn = nn.MSELoss()

loss = loss_fn(model(xi), yi)
optimizer.zero_grad()
loss.backward()
```

```{python}
# find indices in a structured matrix that correspond to the in/out of a given node. 

def find_node_in_layer_info(
    layer_info_list:list, # List where all elements are `sparsevnn.core.structured_layer_info`. 
    edge_dict:dict, # dictionary of graph edges. Needed to find the correct input dim. indices of the `sparsevnn.core.SparseLinearCustom`. 
    query:str, # name of the node to be identified
):
    # find the right index in the list:
    list_idx = [i for i in range(len(layer_info_list)) if query in layer_info_list[i].col_out]
    assert len(list_idx) == 1
    list_idx = list_idx[0]

    # if an edge or output
    if edge_dict[query] != []:
        rows_info = {e : layer_info_list[list_idx].row_info[e] for  e in edge_dict[query]}
    else: # if input
        rows_info = {e : layer_info_list[list_idx].row_info[e] for  e in [query]}

    return {
        'list_idx':  list_idx,
        'col_info':  layer_info_list[list_idx].col_info[query],
        'rows_info': rows_info
        }
```

```{python}
find_node_in_layer_info(
    layer_info_list = M_list,
    edge_dict=edge_dict,
    query = 'ProteinPhosphatasesAndAssociatedProteins[Br-Zma01009]'        
)
```






```{python}

```







