{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visible Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ----\n",
    "from dataG2F.core import get_data\n",
    "from dataG2F.qol  import ensure_dir_path_exists\n",
    "\n",
    "# Data Utilities ----\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "from EnvDL.dlfn import BigDataset, plDNN_general\n",
    "from EnvDL.sets import mask_parents\n",
    "\n",
    "# Model Building  ----\n",
    "## General ====\n",
    "import torch\n",
    "from   torch import nn\n",
    "import torch.nn.functional as F\n",
    "from   torch.utils.data import Dataset\n",
    "from   torch.utils.data import DataLoader\n",
    "\n",
    "## VNN ====\n",
    "import sparsevnn\n",
    "from   sparsevnn.core import\\\n",
    "    VNNHelper, \\\n",
    "    structured_layer_info, \\\n",
    "    SparseLinearCustom\n",
    "from   sparsevnn.kegg import \\\n",
    "    kegg_connections_build, \\\n",
    "    kegg_connections_clean, \\\n",
    "    kegg_connections_append_y_hat, \\\n",
    "    kegg_connections_sanitize_names\n",
    "\n",
    "# Hyperparameter Tuning ----\n",
    "import os # needed for checking history (saved by lightning) \n",
    "\n",
    "## Logging with Pytorch Lightning ====\n",
    "import lightning.pytorch as pl\n",
    "from   lightning.pytorch.loggers import CSVLogger # used to save the history of each trial (used by ax)\n",
    "\n",
    "## Adaptive Experimentation Platform ====\n",
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "from ax.utils.notebook.plotting import init_notebook_plotting, render\n",
    "\n",
    "# For logging experiment results in sql database\n",
    "from ax.storage.sqa_store.db import init_engine_and_session_factory\n",
    "from ax.storage.sqa_store.db import get_engine, create_all_tables\n",
    "from ax.storage.sqa_store.save import save_experiment # saving\n",
    "from ax.storage.sqa_store.structs import DBSettings # loading\n",
    "# from ax.storage.sqa_store.load import load_experiment # loading alternate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "init_notebook_plotting()\n",
    "\n",
    "def fmt(text = '', h = 'h1', l = 80):\n",
    "    text = ''.join(['#' for i in range(int(h.replace('h', '')))])+' '+text+' '\n",
    "    diff = 80 - len(text)\n",
    "    if h == 'h1': char = '-'\n",
    "    if h == 'h2': char = '='\n",
    "    if h == 'h3': char = '.'\n",
    "    text = text+''.join([char for i in range(diff)])\n",
    "    return text\n",
    "\n",
    "fmt('Set node defaults', 'h1')\n",
    "# fmt('', 'h3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/aim_2a_G_Gene_VNN/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run settings: \n",
    "params_run = {\n",
    "    # 'batch_size': 256,\n",
    "    # 'max_epoch' : 64,  \n",
    "    'batch_size': 256,\n",
    "    'max_epoch' : 3,   \n",
    "}\n",
    "\n",
    "# data settings\n",
    "params_data = {\n",
    "    'y_var': 'Yield_Mg_ha',\n",
    "    'y_resid': 'None', # None, Env, Geno\n",
    "    'y_resid_strat': 'None', # None, naive_mean, filter_mean, ...\n",
    "    'holdout_parents': [\n",
    "        ## 2022 ====\n",
    "        'LH244',\n",
    "        ## 2021 ====\n",
    "        'PHZ51',\n",
    "        # 'PHP02',\n",
    "        # 'PHK76',\n",
    "        ## 2019 ====\n",
    "        # 'PHT69',\n",
    "        'LH195',\n",
    "        ## 2017 ====\n",
    "        # 'PHW52',\n",
    "        # 'PHN82',\n",
    "        ## 2016 ====\n",
    "        # 'DK3IIH6',\n",
    "        ## 2015 ====\n",
    "        # 'PHB47',\n",
    "        # 'LH82',\n",
    "        ## 2014 ====\n",
    "        # 'LH198',\n",
    "        # 'LH185',\n",
    "        # 'PB80',\n",
    "        # 'CG102',\n",
    " ],    \n",
    "}\n",
    "\n",
    "# in this file I define params later. I've included it here to gurantee that we can merge other params dicts into it.\n",
    "params = {\n",
    "'default_out_nodes_inp'  : 1,\n",
    "'default_out_nodes_edge' : 1,\n",
    "'default_out_nodes_out'  : 1,\n",
    "\n",
    "'default_drop_nodes_inp' : 0.0,\n",
    "'default_drop_nodes_edge': 0.0,\n",
    "'default_drop_nodes_out' : 0.0,\n",
    "\n",
    "'default_reps_nodes_inp' : 1,\n",
    "'default_reps_nodes_edge': 1,\n",
    "'default_reps_nodes_out' : 1,\n",
    "\n",
    "'default_decay_rate'     : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_out_nodes_inp  = params['default_out_nodes_inp' ]\n",
    "default_out_nodes_edge = params['default_out_nodes_edge'] \n",
    "default_out_nodes_out  = params['default_out_nodes_out' ]\n",
    "\n",
    "default_drop_nodes_inp = params['default_drop_nodes_inp' ] \n",
    "default_drop_nodes_edge= params['default_drop_nodes_edge'] \n",
    "default_drop_nodes_out = params['default_drop_nodes_out' ] \n",
    "\n",
    "default_reps_nodes_inp = params['default_reps_nodes_inp' ]\n",
    "default_reps_nodes_edge= params['default_reps_nodes_edge']\n",
    "default_reps_nodes_out = params['default_reps_nodes_out' ]\n",
    "\n",
    "\n",
    "\n",
    "default_decay_rate = params['default_decay_rate' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = params_run['batch_size']\n",
    "max_epoch  = params_run['max_epoch']\n",
    "\n",
    "y_var = params_data['y_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n",
    "\n",
    "if 'None' != params_data['y_resid_strat']:\n",
    "    save_prefix = save_prefix+'_'+params_data['y_resid_strat']\n",
    "\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu_num = 0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dist_scale_function(out, dist, decay_rate):\n",
    "    scale = 1/(1+decay_rate*dist)\n",
    "    out = round(scale * out)\n",
    "    out = max(1, out)\n",
    "    return out\n",
    "\n",
    "# _dist_scale_function(\n",
    "#     out = 100,\n",
    "#     dist = 2,\n",
    "#     decay_rate = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_node_shortcut(vnn_helper, query = 'y_hat'):\n",
    "    # define new entries\n",
    "    if True in [True if e in vnn_helper.edge_dict.keys() else False for e in \n",
    "                [f'{query}_res_-2', f'{query}_res_-1']\n",
    "                ]:\n",
    "        print('Warning! New node name already exists! Overwriting existing node!')\n",
    "\n",
    "    # Add residual connection in graph\n",
    "    vnn_helper.edge_dict[f'{query}_res_-2'] = myvnn.edge_dict[query] \n",
    "    vnn_helper.edge_dict[f'{query}_res_-1'] = [f'{query}_res_-2']\n",
    "    vnn_helper.edge_dict[query]             = [f'{query}_res_-2', f'{query}_res_-1']\n",
    "\n",
    "    # Add new nodes, copying information from query node\n",
    "    vnn_helper.node_props[f'{query}_res_-2'] = vnn_helper.node_props[query] \n",
    "    vnn_helper.node_props[f'{query}_res_-1'] = vnn_helper.node_props[query]\n",
    "\n",
    "    return vnn_helper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep ----\n",
    "obs_geno_lookup          = get_data('obs_geno_lookup')\n",
    "phno                     = get_data('phno')\n",
    "ACGT_gene_slice_list     = get_data('KEGG_slices')\n",
    "parsed_kegg_gene_entries = get_data('KEGG_entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make holdout sets\n",
    "holdout_parents = params_data['holdout_parents']\n",
    "\n",
    "# create a mask for parent genotype\n",
    "mask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)\n",
    "\n",
    "train_mask = mask.sum(axis=1) == 0\n",
    "test_mask  = mask.sum(axis=1) > 0\n",
    "\n",
    "train_idx = train_mask.loc[train_mask].index\n",
    "test_idx  = test_mask.loc[test_mask].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y to residual if needed\n",
    "\n",
    "if params_data['y_resid'] == 'None':\n",
    "    pass\n",
    "else:\n",
    "    if params_data['y_resid_strat'] == 'naive_mean':\n",
    "        # use only data in the training set (especially since testers will be more likely to be found across envs)\n",
    "        # get enviromental means, subtract from observed value\n",
    "        tmp = phno.loc[train_idx, ]\n",
    "        env_mean = tmp.groupby(['Env_Idx']\n",
    "                     ).agg(Env_Mean = (y_var, 'mean')\n",
    "                     ).reset_index()\n",
    "        tmp = phno.merge(env_mean)\n",
    "        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n",
    "        phno = tmp.drop(columns='Env_Mean')\n",
    "\n",
    "    if params_data['y_resid_strat'] == 'filter_mean':\n",
    "        # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations\n",
    "        def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],\n",
    "                                    year = 2014):\n",
    "            # Within each year what hybrids are most common?\n",
    "            tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')\n",
    "\n",
    "            all_envs = set(tmp.Env)\n",
    "            # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?\n",
    "            # site counts for sets which contain all envs\n",
    "            i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year >= i), 'Env'])) == len(all_envs)])\n",
    "\n",
    "            before = len(set(tmp.loc[:, 'Hybrid']))\n",
    "            after  = len(set(tmp.loc[(tmp.Year >= i), 'Hybrid']))\n",
    "            print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')\n",
    "            tmp = tmp.loc[(tmp.Year >= i), ['Env', 'Hybrid']].reset_index(drop=True)\n",
    "            return tmp\n",
    "\n",
    "\n",
    "        tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]\n",
    "        filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) \n",
    "                          for i in list(set(phno.Year)) ]\n",
    "        env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')\n",
    "\n",
    "        env_mean = env_mean.groupby(['Env_Idx']\n",
    "                          ).agg(Env_Mean = (y_var, 'mean')\n",
    "                          ).reset_index()\n",
    "\n",
    "        tmp = phno.merge(env_mean)\n",
    "        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n",
    "        phno = tmp.drop(columns='Env_Mean')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and y value data\n",
    "assert 0 == phno.loc[:, y_var].isna().sum()\n",
    "\n",
    "y = phno.loc[:, y_var]\n",
    "# use train index to prevent information leakage\n",
    "y_c = y[train_idx].mean()\n",
    "y_s = y[train_idx].std()\n",
    "\n",
    "y = (y - y_c)/y_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Using VNNHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up KEGG Pathways -------------------------------------------------------\n",
    "# Same setup as above to create kegg_gene_brite\n",
    "# Restrict to only those with pathway\n",
    "kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n",
    "\n",
    "# also require to have a non-empty path\n",
    "kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n",
    "\n",
    "print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n",
    "    )+'/'+str(len(parsed_kegg_gene_entries)\n",
    "    )+' Entries'\n",
    "    )\n",
    "# kegg_gene_brite[1]['BRITE']['BRITE_PATHS']\n",
    "\n",
    "\n",
    "kegg_connections = kegg_connections_build(kegg_gene_brite = kegg_gene_brite, \n",
    "                                          n_genes = len(kegg_gene_brite)) \n",
    "kegg_connections = kegg_connections_clean(         kegg_connections = kegg_connections)\n",
    "#TODO think about removing \n",
    "# \"Not Included In\n",
    "# Pathway Or Brite\"\n",
    "# or reinstate 'Others'\n",
    "\n",
    "kegg_connections = kegg_connections_append_y_hat(  kegg_connections = kegg_connections)\n",
    "kegg_connections = kegg_connections_sanitize_names(kegg_connections = kegg_connections, \n",
    "                                                   replace_chars = {'.':'_'})\n",
    "\n",
    "\n",
    "# Initialize helper for input nodes --------------------------------------------\n",
    "myvnn = VNNHelper(edge_dict = kegg_connections)\n",
    "\n",
    "# Get a mapping of brite names to tensor list index\n",
    "find_names = myvnn.nodes_inp # e.g. ['100383860', '100278565', ... ]\n",
    "lookup_dict = {}\n",
    "\n",
    "# the only difference lookup_dict and brite_node_to_list_idx_dict above is that this is made using the full set of genes in the list \n",
    "# whereas that is made using kegg_gene_brite which is a subset\n",
    "for i in range(len(parsed_kegg_gene_entries)):\n",
    "    if 'BRITE' not in parsed_kegg_gene_entries[i].keys():\n",
    "        pass\n",
    "    elif parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'] == []:\n",
    "        pass\n",
    "    else:\n",
    "        name = parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'][0][-1]\n",
    "        if name in find_names:\n",
    "            lookup_dict[name] = i\n",
    "# lookup_dict    \n",
    "\n",
    "brite_node_to_list_idx_dict = {}\n",
    "for i in range(len(kegg_gene_brite)):\n",
    "    brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i        \n",
    "\n",
    "# Get the input sizes for the graph\n",
    "size_in_zip = zip(myvnn.nodes_inp, [np.prod(ACGT_gene_slice_list[lookup_dict[e]].shape[1:]) for e  in myvnn.nodes_inp])\n",
    "\n",
    "# Set node defaults ------------------------------------------------------------\n",
    "# init input node sizes\n",
    "myvnn.set_node_props(key = 'inp', node_val_zip = size_in_zip)\n",
    "\n",
    "# init node output sizes\n",
    "myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_inp, [default_out_nodes_inp  for e in myvnn.nodes_inp]))\n",
    "myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_edge,[default_out_nodes_edge for e in myvnn.nodes_edge]))\n",
    "myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_out, [default_out_nodes_out  for e in myvnn.nodes_out]))\n",
    "\n",
    "# # options should be controlled by node_props\n",
    "myvnn.set_node_props(key = 'flatten', node_val_zip = zip(myvnn.nodes_inp, [True for e in myvnn.nodes_inp]))\n",
    "\n",
    "myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_inp, [default_reps_nodes_inp  for e in myvnn.nodes_inp]))\n",
    "myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_edge,[default_reps_nodes_edge for e in myvnn.nodes_edge]))\n",
    "myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_out, [default_reps_nodes_out  for e in myvnn.nodes_out]))\n",
    "\n",
    "myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_inp, [default_drop_nodes_inp  for e in myvnn.nodes_inp]))\n",
    "myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_edge,[default_drop_nodes_edge for e in myvnn.nodes_edge]))\n",
    "myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_out, [default_drop_nodes_out  for e in myvnn.nodes_out]))\n",
    "\n",
    "\n",
    "# Scale node outputs by distance -----------------------------------------------\n",
    "dist = sparsevnn.core.vertex_from_end(\n",
    "    edge_dict = myvnn.edge_dict,\n",
    "    end =myvnn.dependancy_order[-1]\n",
    ")\n",
    "\n",
    "# overwrite node outputs with a size inversely proportional to distance from prediction node\n",
    "for query in list(dist.keys()):\n",
    "    myvnn.node_props[query]['out'] = _dist_scale_function(\n",
    "        out = myvnn.node_props[query]['out'],\n",
    "        dist = dist[query],\n",
    "        decay_rate = default_decay_rate)\n",
    "    \n",
    "\n",
    "# Expand out node replicates ---------------------------------------------------\n",
    "kegg_connections_expanded = sparsevnn.core.expand_edge_dict(vnn_helper = myvnn, edge_dict = myvnn.edge_dict)\n",
    "\n",
    "# ======================================================= #\n",
    "# one place to add residual connections would be here.    #\n",
    "# edit the links before instatinating the new VNNHelper   #\n",
    "# the important thing to do is to edit the graph before   #\n",
    "# calulating the inputs for each node.                    #\n",
    "# ======================================================= #\n",
    "\n",
    "# expand then copy over the properties that have already been defined.\n",
    "myvnn_exp = VNNHelper(edge_dict = kegg_connections_expanded)\n",
    "\n",
    "import re\n",
    "for new_key in list(myvnn_exp.node_props.keys()):\n",
    "    if new_key in myvnn.node_props.keys():\n",
    "        # copy directly\n",
    "        myvnn_exp.node_props[new_key] = myvnn.node_props[new_key]\n",
    "    else:\n",
    "        # check for a key that matches the query key after removing the replicate information\n",
    "        query = new_key \n",
    "        suffix = re.findall('_rep_\\d+$', query)[0]\n",
    "\n",
    "        query = query.removesuffix(suffix)\n",
    "        if query in myvnn.node_props.keys():\n",
    "            myvnn_exp.node_props[new_key] = myvnn.node_props[query]\n",
    "        else:\n",
    "            print(f'WARNING: no entry {query} found for {new_key}') \n",
    "\n",
    "# now main vnn is the expanded version\n",
    "myvnn = myvnn_exp\n",
    "\n",
    "\n",
    "# Cleanup ----------------------------------------------------------------------\n",
    "# now the original VNNHelper isn't needed\n",
    "myvnn = myvnn_exp\n",
    "\n",
    "# init edge node input size (propagate forward input/edge outpus)\n",
    "myvnn.calc_edge_inp()\n",
    "\n",
    "# replace lookup so that it matches the lenght of the input tensors\n",
    "new_lookup_dict = {}\n",
    "for i in range(len(myvnn.nodes_inp)):\n",
    "    new_lookup_dict[myvnn.nodes_inp[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate nodes membership in each matrix and positions within each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_props = myvnn.node_props\n",
    "# Linear_block = Linear_block_reps,\n",
    "edge_dict = myvnn.edge_dict\n",
    "dependancy_order = myvnn.dependancy_order\n",
    "node_to_inp_num_dict = new_lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dependancy dictionary --------------------------------------------------\n",
    "# check dep order\n",
    "tally = []\n",
    "for d in dependancy_order:\n",
    "    if edge_dict[d] == []:\n",
    "        tally.append(d)\n",
    "    elif False not in [True if e in tally else False for e in edge_dict[d]]:\n",
    "        tally.append(d)\n",
    "    else:\n",
    "        print('error!')\n",
    "        break\n",
    "\n",
    "\n",
    "# build output nodes \n",
    "d_out = {0:[]}\n",
    "for d in dependancy_order:\n",
    "    if edge_dict[d] == []:\n",
    "        d_out[min(d_out.keys())].append(d)\n",
    "    else:\n",
    "        # print((d, edge_dict[d]))\n",
    "\n",
    "        d_out_i = 1+max(sum([[key for key in d_out.keys() if e in d_out[key]]\n",
    "                   for e in edge_dict[d]], []))\n",
    "        \n",
    "        if d_out_i not in d_out.keys():\n",
    "            d_out[d_out_i] = []\n",
    "        d_out[d_out_i].append(d)\n",
    "\n",
    "\n",
    "# build input nodes NOPE. THE PASSHTROUGHS! \n",
    "d_eye = {}\n",
    "tally = []\n",
    "for i in range(max(d_out.keys()), min(d_out.keys()), -1):\n",
    "    # print(i)\n",
    "    nodes_needed = sum([edge_dict[e] for e in d_out[i]], [])+tally\n",
    "    # check against what is there and then dedupe\n",
    "    nodes_needed = [e for e in nodes_needed if e not in d_out[i-1]]\n",
    "    nodes_needed = list(set(nodes_needed))\n",
    "    tally = nodes_needed\n",
    "    d_eye[i] = nodes_needed\n",
    "\n",
    "# d_inp[0]= d_out[0]\n",
    "# [len(d_eye[i]) for i in d_eye.keys()]\n",
    "# [(key, len(d_out[key])) for key in d_out.keys()]\n",
    "\n",
    "\n",
    "dd = {}\n",
    "for i in d_eye.keys():\n",
    "    dd[i] = {'out': d_out[i],\n",
    "             'inp': d_out[i-1],\n",
    "             'eye': d_eye[i]}\n",
    "# plus special 0 layer that handles the snps\n",
    "    \n",
    "dd[0] = {'out': d_out[0],\n",
    "         'inp': d_out[0],\n",
    "         'eye': []}\n",
    "\n",
    "\n",
    "# check that the output nodes' inputs are satisfied by the same layer's inputs (inp and eye)\n",
    "for i in dd.keys():\n",
    "    # out node in each\n",
    "    for e in dd[i]['out']:\n",
    "        # node depends in inp/eye\n",
    "        node_pass_list = [True if ee in dd[i]['inp']+dd[i]['eye'] else False \n",
    "                          for ee in edge_dict[e]]\n",
    "        if False not in node_pass_list:\n",
    "            pass\n",
    "        else:\n",
    "            print('exit') \n",
    "\n",
    "\n",
    "# print(\"Layer\\t#In\\t#Out\")\n",
    "# for i in range(min(dd.keys()), max(dd.keys())+1, 1):\n",
    "#     node_in      = [node_props[e]['out'] for e in dd[i]['inp']+dd[i  ]['eye'] ]\n",
    "#     if i == max(dd.keys()):\n",
    "#         node_out = [node_props[e]['out'] for e in dd[i]['out'] ]\n",
    "#     else:\n",
    "#         node_out = [node_props[e]['out'] for e in dd[i]['out']+dd[i+1]['eye']]\n",
    "#     print(f'{i}:\\t{sum(node_in)}\\t{sum(node_out)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt('Build dependancy dictionary', 'h1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Structured Matrices for Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_list = [structured_layer_info(i = ii, node_groups = dd, node_props= node_props, edge_dict = edge_dict, as_sparse=True) for ii in range(0, max(dd.keys())+1)]\n",
    "# [list(e.weight.shape) for e in M_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Dataloader using `M_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict\n",
    "\n",
    "vals = get_data('KEGG_slices')\n",
    "vals = [torch.from_numpy(e).to(torch.float) for e in vals]\n",
    "# restrict to the tensors that will be used\n",
    "vals = torch.concat([vals[lookup_dict[i]].reshape(4926, -1) \n",
    "                     for i in M_list[0].row_inp\n",
    "                    #  for i in dd[0]['inp'] # matches\n",
    "                     ], axis = 1)\n",
    "\n",
    "vals = vals.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n",
    "    lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n",
    "    G =           vals,\n",
    "    G_type = 'raw',\n",
    "    send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = batch_size\n",
    "    shuffle = True \n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n",
    "    lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n",
    "    G =           vals,\n",
    "    G_type = 'raw',\n",
    "    send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = batch_size\n",
    "    shuffle = False \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_list):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer_list = nn.ModuleList(layer_list)\n",
    " \n",
    "    def forward(self, x):\n",
    "        for l in self.layer_list:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "layer_list = []\n",
    "for i in range(len(M_list)):\n",
    "    \n",
    "    apply_relu = None\n",
    "    if i+1 != len(M_list): # apply relu to all but the last layer\n",
    "        apply_relu = F.relu\n",
    "    \n",
    "\n",
    "    l = SparseLinearCustom(\n",
    "        M_list[i].weight.shape[1], # have to transpose this?\n",
    "        M_list[i].weight.shape[0],\n",
    "        connectivity   = torch.LongTensor(M_list[i].weight.coalesce().indices()),\n",
    "        custom_weights = M_list[i].weight.coalesce().values(), \n",
    "        custom_bias    = M_list[i].bias.clone().detach(), \n",
    "        weight_grad_bool = M_list[i].weight_grad_bool, \n",
    "        bias_grad_bool   = M_list[i].bias_grad_bool, #.to_sparse()#.indices()\n",
    "        dropout_p        = M_list[i].dropout_p,\n",
    "        nonlinear_transform= apply_relu\n",
    "        )\n",
    "\n",
    "    layer_list += [l]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prefix += '_inv' \n",
    "save_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~12 mins to run \n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "VNN = plDNN_general(model)  \n",
    "\n",
    "optimizer = VNN.configure_optimizers()\n",
    "\n",
    "logger = CSVLogger(\"nifa_tb\", name=save_prefix)\n",
    "logger.log_hyperparams(params={\n",
    "    'params': params,\n",
    "    'params_data': params_data,\n",
    "    'params_run' : params_run,\n",
    "    'misc': {\n",
    "        'n_train': len(train_idx),\n",
    "        'n_test':  len(test_idx)\n",
    "    }\n",
    "})\n",
    "\n",
    "# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "trainer = pl.Trainer(max_epochs=max_epoch)\n",
    "\n",
    "trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny Test Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data ====\n",
    "\n",
    "training_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs  = torch.from_numpy(get_data('obs_env_lookup')[:, 0]),\n",
    "    lookup_env  = torch.from_numpy(get_data('obs_env_lookup')),\n",
    "    y =           torch.from_numpy(get_data('phno')['Yield_Mg_ha'].to_numpy()).to(torch.float32)[:, None],\n",
    "    S           = torch.from_numpy(get_data('SMat')).to(torch.float32),\n",
    "    send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = 256,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs  = torch.from_numpy(get_data('obs_env_lookup')[:, 0]),\n",
    "    lookup_env  = torch.from_numpy(get_data('obs_env_lookup')),\n",
    "    y =           torch.from_numpy(get_data('phno')['Yield_Mg_ha'].to_numpy()).to(torch.float32)[:, None],\n",
    "    S           = torch.from_numpy(get_data('SMat')).to(torch.float32),\n",
    "    send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = 256,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "\n",
    "[e.shape for e in next(iter(training_dataloader))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings ====\n",
    "hyps_trials = 3\n",
    "\n",
    "run_hyps = 3\n",
    "run_hyps_force = False # should we run more trials even if the target number has been reached?\n",
    "max_hyps = 5\n",
    "\n",
    "max_epoch = 2\n",
    "lightning_log_dir = \"test_tb\"\n",
    "exp_name = \"fcn_simple\"\n",
    "params_list = [\n",
    "        {\n",
    "            \"name\": \"size\",\n",
    "            \"type\": \"range\",\n",
    "            \"bounds\": [1, 256],\n",
    "            \"value_type\": \"int\",  # Optional, defaults to inference from type of \"bounds\".\n",
    "            \"log_scale\": False,  # Optional, defaults to False.\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"drop\",\n",
    "            \"type\": \"range\",\n",
    "            \"value_type\": \"float\",  # Optional, defaults to inference from type of \"bounds\".\n",
    "            \"bounds\": [0.0, 1.0],\n",
    "        }\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to convert parametrization to model\n",
    "class fcn_simple(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.Lin1 = nn.Linear(23, params['size'])\n",
    "        self.Drp1 = nn.Dropout(p = params['drop'])\n",
    "        self.Lin2 = nn.Linear(params['size'], 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.Lin1(x)\n",
    "        x = self.Drp1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Lin2(x)\n",
    "        return x\n",
    "    \n",
    "# model = fcn_simple(parameterization)\n",
    "# model.to('cuda')(next(iter(training_dataloader))[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a very funny trick. I'm going to call lighning from within Ax. \n",
    "# That way I can save out traces while also relying on Ax to choose new hyps. \n",
    "\n",
    "# logger = CSVLogger(\"test_tb\", name=exp_name)\n",
    "# # logger.log_hyperparams(params={\n",
    "# #     'params': params,\n",
    "# #     'params_data': params_data,\n",
    "# #     'params_run' : params_run,\n",
    "# #     'misc': {\n",
    "# #         'n_train': len(train_idx),\n",
    "# #         'n_test':  len(test_idx)\n",
    "# #     }\n",
    "# # })\n",
    "\n",
    "def evaluate(parameterization):\n",
    "    # draw from global\n",
    "\n",
    "    # max_epoch = 20\n",
    "    # lightning_log_dir = \"test_tb\"\n",
    "    \n",
    "    model = fcn_simple(parameterization)\n",
    "\n",
    "    VNN = plDNN_general(model)  \n",
    "    optimizer = VNN.configure_optimizers()\n",
    "    logger = CSVLogger(lightning_log_dir, name=exp_name)\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "    trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n",
    "\n",
    "\n",
    "    # if we were optimizing number of training epochs this would be an effective loss to use.\n",
    "    # trainer.callback_metrics['train_loss']\n",
    "    # float(trainer.callback_metrics['train_loss'])\n",
    "\n",
    "    # To potentially _overtrain_ models and still let the selction be based on their best possible performance,\n",
    "    # I'll use the lowest average error in an epoch\n",
    "    log_path = lightning_log_dir+'/'+exp_name\n",
    "    fls = os.listdir(log_path)\n",
    "    nums = [int(e.split('_')[-1]) for e in fls] \n",
    "\n",
    "    M = pd.read_csv(log_path+f\"/version_{max(nums)}/metrics.csv\")\n",
    "    M = M.loc[:, ['epoch', 'train_loss']].dropna()\n",
    "\n",
    "    M = M.groupby('epoch').agg(\n",
    "        train_loss = ('train_loss', 'mean'),\n",
    "        train_loss_sd = ('train_loss', 'std'),\n",
    "        ).reset_index()\n",
    "\n",
    "    train_metric = M.train_loss.min()\n",
    "    print(train_metric)\n",
    "    return {\"train_loss\": (train_metric, 0.0)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generated variables ====\n",
    "# using sql database\n",
    "sql_url = \"sqlite:///\"+f\"./{lightning_log_dir}/{exp_name}.db\"\n",
    "\n",
    "# If the database exists, load it and begin from there\n",
    "loaded_db = False\n",
    "if os.path.exists(sql_url.split('///')[-1]): # must cleave off the sql dialect prefix\n",
    "    # alternate way to load an experiment (after `init_engine_and_session_factory` has been run)\n",
    "    # experiment = load_experiment(exp_name) # if this doesn't work, check if the database is named something else and try that.\n",
    "    db_settings = DBSettings(url=sql_url)\n",
    "    # Instead of URL, can provide a `creator function`; can specify custom encoders/decoders if necessary.\n",
    "    ax_client = AxClient(db_settings=db_settings)\n",
    "    ax_client.load_experiment_from_database(exp_name)\n",
    "    loaded_db = True\n",
    "\n",
    "else:\n",
    "    ax_client = AxClient()\n",
    "    ax_client.create_experiment(\n",
    "        name=exp_name,\n",
    "        parameters=params_list,\n",
    "        objectives={\"train_loss\": ObjectiveProperties(minimize=True)}\n",
    "    )\n",
    "\n",
    "run_trials_bool = True\n",
    "if run_hyps_force == False:\n",
    "    if loaded_db: \n",
    "        # check if we've reached the max number of hyperparamters combinations to test\n",
    "        if max_hyps <= (ax_client.generation_strategy.trials_as_df.index.max()+1):\n",
    "            run_trials_bool = False\n",
    "\n",
    "if run_trials_bool:\n",
    "    # run the trials\n",
    "    for i in range(run_hyps):\n",
    "        parameterization, trial_index = ax_client.get_next_trial()\n",
    "        # Local evaluation here can be replaced with deployment to external system.\n",
    "        ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameterization))\n",
    "\n",
    "    if loaded_db == False:\n",
    "        init_engine_and_session_factory(url=sql_url)\n",
    "        engine = get_engine()\n",
    "        create_all_tables(engine)\n",
    "\n",
    "    # save the trials\n",
    "    experiment = ax_client.experiment\n",
    "    save_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.generation_strategy.trials_as_df#.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render(ax_client.get_contour_plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(ax_client.get_optimization_trace(objective_optimum=0.0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I need to check what tables are in the sqlite\n",
    "# import sqlite3\n",
    "# con = sqlite3.connect(\"./foo.db\")\n",
    "# cur = con.cursor()\n",
    "# # cur.execute(\".tables;\") # should work, doesn't\n",
    "# cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "# con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
