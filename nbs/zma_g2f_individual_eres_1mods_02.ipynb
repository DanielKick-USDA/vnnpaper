{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#---\n",
    "#skip_exec: true\n",
    "#---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visible Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ----\n",
    "from dataG2F.core import get_data\n",
    "from dataG2F.qol  import ensure_dir_path_exists\n",
    "\n",
    "# Data Utilities ----\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "# Model Building  ----\n",
    "## General ====\n",
    "import torch\n",
    "from   torch import nn\n",
    "import torch.nn.functional as F\n",
    "from   torch.utils.data import Dataset\n",
    "from   torch.utils.data import DataLoader\n",
    "\n",
    "from vnnpaper.zma import \\\n",
    "    BigDataset,    \\\n",
    "    plDNN_general, \\\n",
    "    mask_parents,  \\\n",
    "    vnn_factory_1, \\\n",
    "    vnn_factory_2, \\\n",
    "    vnn_factory_3\n",
    "\n",
    "# Hyperparameter Tuning ----\n",
    "import os # needed for checking history (saved by lightning) \n",
    "\n",
    "## Logging with Pytorch Lightning ====\n",
    "import lightning.pytorch as pl\n",
    "from   lightning.pytorch.loggers import CSVLogger # used to save the history of each trial (used by ax)\n",
    "\n",
    "## Adaptive Experimentation Platform ====\n",
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "# from ax.utils.notebook.plotting import init_notebook_plotting, render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_notebook_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/zma_g2f_individual_eres_1mods/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings ====\n",
    "# run_hyps = 32 \n",
    "# run_hyps_force = False # should we run more trials even if the target number has been reached?\n",
    "# max_hyps = 64\n",
    "\n",
    "# Run settings: \n",
    "params_run = {\n",
    "    'batch_size': 256,\n",
    "    'max_epoch' : 16, #256,    \n",
    "}\n",
    "\n",
    "# data settings\n",
    "params_data = {\n",
    "    # 'y_var': 'Yield_Mg_ha',\n",
    "    'y_var': [\n",
    "        # Description quoted from competition data readme\n",
    "        # 'Yield_Mg_ha',     # Grain yield in Mg per ha at 15.5% grain moisture, using plot area without alley (Mg/ha).\n",
    "        'Pollen_DAP_days', # Number of days after planting that 50% of plants in the plot began shedding pollen.\n",
    "        # 'Silk_DAP_days',   # Number of days after planting that 50% of plants in the plot had visible silks.\n",
    "        # 'Plant_Height_cm', # Measured as the distance between the base of a plant and the ligule of the flag leaf (centimeter).\n",
    "        # 'Ear_Height_cm',   # Measured as the distance from the ground to the primary ear bearing node (centimeter).\n",
    "        # 'Grain_Moisture',  # Water content in grain at harvest (percentage).\n",
    "        # 'Twt_kg_m3'        # Shelled grain test weight (kg/m3), a measure of grain density.\n",
    "    ],\n",
    "\n",
    "    'y_resid': 'Env', # None, Env, Geno\n",
    "    'y_resid_strat': 'naive_mean', # None, naive_mean, filter_mean, ...\n",
    "    'holdout_parents': [\n",
    "        ## 2022 ====\n",
    "        'LH244',\n",
    "        ## 2021 ====\n",
    "        'PHZ51',\n",
    "        # 'PHP02',\n",
    "        # 'PHK76',\n",
    "        ## 2019 ====\n",
    "        # 'PHT69',\n",
    "        'LH195',\n",
    "        ## 2017 ====\n",
    "        # 'PHW52',\n",
    "        # 'PHN82',\n",
    "        ## 2016 ====\n",
    "        # 'DK3IIH6',\n",
    "        ## 2015 ====\n",
    "        # 'PHB47',\n",
    "        # 'LH82',\n",
    "        ## 2014 ====\n",
    "        # 'LH198',\n",
    "        # 'LH185',\n",
    "        # 'PB80',\n",
    "        # 'CG102',\n",
    " ],    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_list = [    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_log_dir = cache_path+\"lightning\"\n",
    "exp_name = [e for e in cache_path.split('/') if e != ''][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameterization is needed for setup. These values will be overwritten by Ax if tuning is occuring. \n",
    "# in this file I define params later. I've included it here to gurantee that we can merge other params dicts into it.\n",
    "params = {\n",
    "'default_out_nodes_inp'  : 1,\n",
    "'default_out_nodes_edge' : 1,\n",
    "'default_out_nodes_out'  : 1, #len(params_data['y_var']) if type(params_data['y_var']) == list else 1,\n",
    "\n",
    "'default_drop_nodes_inp' : 0.0,\n",
    "'default_drop_nodes_edge': 0.0,\n",
    "'default_drop_nodes_out' : 0.0,\n",
    "\n",
    "'default_reps_nodes_inp' : 1,\n",
    "'default_reps_nodes_edge': 1,\n",
    "'default_reps_nodes_out' : 1,\n",
    "\n",
    "'default_decay_rate'     : 1\n",
    "}\n",
    "\n",
    "default_out_nodes_inp  = params['default_out_nodes_inp' ]\n",
    "default_out_nodes_edge = params['default_out_nodes_edge'] \n",
    "default_out_nodes_out  = params['default_out_nodes_out' ]\n",
    "\n",
    "default_drop_nodes_inp = params['default_drop_nodes_inp' ] \n",
    "default_drop_nodes_edge= params['default_drop_nodes_edge'] \n",
    "default_drop_nodes_out = params['default_drop_nodes_out' ] \n",
    "\n",
    "default_reps_nodes_inp = params['default_reps_nodes_inp' ]\n",
    "default_reps_nodes_edge= params['default_reps_nodes_edge']\n",
    "default_reps_nodes_out = params['default_reps_nodes_out' ]\n",
    "\n",
    "default_decay_rate = params['default_decay_rate' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = params_run['batch_size']\n",
    "max_epoch  = params_run['max_epoch']\n",
    "\n",
    "y_var = params_data['y_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n",
    "\n",
    "if 'None' != params_data['y_resid_strat']:\n",
    "    save_prefix = save_prefix+'_'+params_data['y_resid_strat']\n",
    "\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "use_gpu_num = 0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep ----\n",
    "obs_geno_lookup          = get_data('obs_geno_lookup')\n",
    "phno                     = get_data('phno')\n",
    "ACGT_gene_slice_list     = get_data('KEGG_slices')\n",
    "parsed_kegg_gene_entries = get_data('KEGG_entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the given y variable is there\n",
    "# single column version\n",
    "# phno = phno.loc[(phno[y_var].notna()), ].copy()\n",
    "# phno = phno.reset_index().drop(columns='index')\n",
    "\n",
    "# multicolumn\n",
    "# mask based on the y variables\n",
    "na_array = phno[y_var].isna().to_numpy().sum(axis=1)\n",
    "mask_no_na = list(0 == na_array)\n",
    "\n",
    "phno = phno.loc[mask_no_na, ].copy()\n",
    "phno = phno.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update obs_geno_lookup\n",
    "\n",
    "tmp = phno.reset_index().rename(columns={'index': 'Phno_Idx_new'}).loc[:, ['Phno_Idx_new', 'Geno_Idx']]\n",
    "tmp = pd.merge(tmp,\n",
    "          tmp.drop(columns='Phno_Idx_new').drop_duplicates().reset_index().rename(columns={'index': 'Phno_Idx_Orig_new'}))\n",
    "tmp = tmp.sort_values('Phno_Idx_new').reset_index(drop=True)\n",
    "\n",
    "obs_geno_lookup = tmp.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make holdout sets\n",
    "holdout_parents = params_data['holdout_parents']\n",
    "\n",
    "# create a mask for parent genotype\n",
    "mask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)\n",
    "\n",
    "train_mask = mask.sum(axis=1) == 0\n",
    "test_mask  = mask.sum(axis=1) > 0\n",
    "\n",
    "train_idx = train_mask.loc[train_mask].index\n",
    "test_idx  = test_mask.loc[test_mask].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y to residual if needed\n",
    "\n",
    "if params_data['y_resid'] == 'None':\n",
    "    pass\n",
    "else:\n",
    "    if params_data['y_resid_strat'] == 'naive_mean':\n",
    "        # use only data in the training set (especially since testers will be more likely to be found across envs)\n",
    "        # get enviromental means, subtract from observed value\n",
    "        for i in range(len(y_var)):\n",
    "            tmp = phno.loc[train_idx, ]\n",
    "            env_mean = tmp.groupby(['Env_Idx']\n",
    "                        ).agg(Env_Mean = (y_var[i], 'mean')\n",
    "                        ).reset_index()\n",
    "            tmp = phno.merge(env_mean)\n",
    "            tmp.loc[:, y_var[i]] = tmp.loc[:, y_var[i]] - tmp.loc[:, 'Env_Mean']\n",
    "            phno = tmp.drop(columns='Env_Mean')\n",
    "\n",
    "    if params_data['y_resid_strat'] == 'filter_mean':\n",
    "        # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations\n",
    "        def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],\n",
    "                                    year = 2014):\n",
    "            # Within each year what hybrids are most common?\n",
    "            tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')\n",
    "\n",
    "            all_envs = set(tmp.Env)\n",
    "            # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?\n",
    "            # site counts for sets which contain all envs\n",
    "            i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year >= i), 'Env'])) == len(all_envs)])\n",
    "\n",
    "            before = len(set(tmp.loc[:, 'Hybrid']))\n",
    "            after  = len(set(tmp.loc[(tmp.Year >= i), 'Hybrid']))\n",
    "            print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')\n",
    "            tmp = tmp.loc[(tmp.Year >= i), ['Env', 'Hybrid']].reset_index(drop=True)\n",
    "            return tmp\n",
    "\n",
    "        for i in range(len(y_var)):\n",
    "            tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]\n",
    "            filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) \n",
    "                            for i in list(set(phno.Year)) ]\n",
    "            env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')\n",
    "\n",
    "            env_mean = env_mean.groupby(['Env_Idx']\n",
    "                            ).agg(Env_Mean = (y_var[i], 'mean')\n",
    "                            ).reset_index()\n",
    "\n",
    "            tmp = phno.merge(env_mean)\n",
    "            tmp.loc[:, y_var[i]] = tmp.loc[:, y_var[i]] - tmp.loc[:, 'Env_Mean']\n",
    "            phno = tmp.drop(columns='Env_Mean')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and y value data\n",
    "assert 0 == phno.loc[:, y_var].isna().sum().sum() # second sum is for multiple y_vars\n",
    "\n",
    "y = phno.loc[:, y_var].to_numpy() # added to make multiple ys work\n",
    "# use train index to prevent information leakage\n",
    "y_c = y[train_idx].mean(axis=0)\n",
    "y_s = y[train_idx].std(axis=0)\n",
    "\n",
    "y = (y - y_c)/y_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Using VNNHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "{'default_out_nodes_inp': 1, 'default_out_nodes_edge': 1, 'default_out_nodes_out': 1, 'default_drop_nodes_inp': 0.0, 'default_drop_nodes_edge': 0.0, 'default_drop_nodes_out': 0.0, 'default_reps_nodes_inp': 1, 'default_reps_nodes_edge': 1, 'default_reps_nodes_out': 1, 'default_decay_rate': 1}\n",
      "################################################################################\n",
      "Retaining 43.53%, 6067/13939 Entries\n",
      "Removed node \"Others\"\n"
     ]
    }
   ],
   "source": [
    "myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = params, ACGT_gene_slice_list = ACGT_gene_slice_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate nodes membership in each matrix and positions within each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Structured Matrices for Layers\n",
    "M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Dataloader using `M_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = new_lookup_dict\n",
    "\n",
    "vals = get_data('KEGG_slices')\n",
    "vals = [torch.from_numpy(e).to(torch.float) for e in vals]\n",
    "# restrict to the tensors that will be used\n",
    "vals = torch.concat([vals[lookup_dict[i]].reshape(4926, -1) \n",
    "                     for i in M_list[0].row_inp\n",
    "                    #  for i in dd[0]['inp'] # matches\n",
    "                     ], axis = 1)\n",
    "\n",
    "vals = vals.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n",
    "    lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "    y =           torch.from_numpy(y).to(torch.float32)[:, None].squeeze(),\n",
    "    G =           vals,\n",
    "    G_type = 'raw',\n",
    "    send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True \n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n",
    "    lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "    y =           torch.from_numpy(y).to(torch.float32)[:, None].squeeze(),\n",
    "    G =           vals,\n",
    "    G_type = 'raw',\n",
    "    send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_list):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer_list = nn.ModuleList(layer_list)\n",
    " \n",
    "    def forward(self, x):\n",
    "        for l in self.layer_list:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny Test Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prep_data_to_dls(\n",
    "        obs_geno_lookup,\n",
    "        vals,\n",
    "        batch_size,\n",
    "        params_data\n",
    "        ):\n",
    "    y_var = params_data['y_var']\n",
    "\n",
    "    # Data Prep ----\n",
    "    phno                     = get_data('phno')\n",
    "    # make sure that the given y variable is there\n",
    "    # single column version\n",
    "    # phno = phno.loc[(phno[y_var].notna()), ].copy()\n",
    "    # phno = phno.reset_index().drop(columns='index')\n",
    "\n",
    "    # multicolumn\n",
    "    # mask based on the y variables\n",
    "    na_array = phno[y_var].isna().to_numpy().sum(axis=1)\n",
    "    mask_no_na = list(0 == na_array)\n",
    "\n",
    "    phno = phno.loc[mask_no_na, ].copy()\n",
    "    phno = phno.reset_index().drop(columns='index')\n",
    "\n",
    "\n",
    "    # update obs_geno_lookup\n",
    "    tmp = phno.reset_index().rename(columns={'index': 'Phno_Idx_new'}).loc[:, ['Phno_Idx_new', 'Geno_Idx']]\n",
    "    tmp = pd.merge(tmp,\n",
    "            tmp.drop(columns='Phno_Idx_new').drop_duplicates().reset_index().rename(columns={'index': 'Phno_Idx_Orig_new'}))\n",
    "    tmp = tmp.sort_values('Phno_Idx_new').reset_index(drop=True)\n",
    "\n",
    "    obs_geno_lookup = tmp.to_numpy()\n",
    "    \n",
    "    \n",
    "    # make holdout sets\n",
    "    holdout_parents = params_data['holdout_parents']\n",
    "\n",
    "    \n",
    "    # create a mask for parent genotype\n",
    "    mask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)\n",
    "\n",
    "    train_mask = mask.sum(axis=1) == 0\n",
    "    test_mask  = mask.sum(axis=1) > 0\n",
    "\n",
    "    train_idx = train_mask.loc[train_mask].index\n",
    "    test_idx  = test_mask.loc[test_mask].index\n",
    "\n",
    "\n",
    "    # convert y to residual if needed\n",
    "    if params_data['y_resid'] == 'None':\n",
    "        pass\n",
    "    else:\n",
    "        if params_data['y_resid_strat'] == 'naive_mean':\n",
    "            # use only data in the training set (especially since testers will be more likely to be found across envs)\n",
    "            # get enviromental means, subtract from observed value\n",
    "            for i in range(len(y_var)):\n",
    "                tmp = phno.loc[train_idx, ]\n",
    "                env_mean = tmp.groupby(['Env_Idx']\n",
    "                            ).agg(Env_Mean = (y_var[i], 'mean')\n",
    "                            ).reset_index()\n",
    "                tmp = phno.merge(env_mean)\n",
    "                tmp.loc[:, y_var[i]] = tmp.loc[:, y_var[i]] - tmp.loc[:, 'Env_Mean']\n",
    "                phno = tmp.drop(columns='Env_Mean')\n",
    "\n",
    "        if params_data['y_resid_strat'] == 'filter_mean':\n",
    "            # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations\n",
    "            def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],\n",
    "                                        year = 2014):\n",
    "                # Within each year what hybrids are most common?\n",
    "                tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')\n",
    "\n",
    "                all_envs = set(tmp.Env)\n",
    "                # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?\n",
    "                # site counts for sets which contain all envs\n",
    "                i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year >= i), 'Env'])) == len(all_envs)])\n",
    "\n",
    "                before = len(set(tmp.loc[:, 'Hybrid']))\n",
    "                after  = len(set(tmp.loc[(tmp.Year >= i), 'Hybrid']))\n",
    "                print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')\n",
    "                tmp = tmp.loc[(tmp.Year >= i), ['Env', 'Hybrid']].reset_index(drop=True)\n",
    "                return tmp\n",
    "\n",
    "            for i in range(len(y_var)):\n",
    "                tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]\n",
    "                filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) \n",
    "                                for i in list(set(phno.Year)) ]\n",
    "                env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')\n",
    "\n",
    "                env_mean = env_mean.groupby(['Env_Idx']\n",
    "                                ).agg(Env_Mean = (y_var[i], 'mean')\n",
    "                                ).reset_index()\n",
    "\n",
    "                tmp = phno.merge(env_mean)\n",
    "                tmp.loc[:, y_var[i]] = tmp.loc[:, y_var[i]] - tmp.loc[:, 'Env_Mean']\n",
    "                phno = tmp.drop(columns='Env_Mean')\n",
    "\n",
    "\n",
    "    # center and y value data\n",
    "    assert 0 == phno.loc[:, y_var].isna().sum().sum() # second sum is for multiple y_vars\n",
    "\n",
    "    y = phno.loc[:, y_var].to_numpy() # added to make multiple ys work\n",
    "    # use train index to prevent information leakage\n",
    "    y_c = y[train_idx].mean(axis=0)\n",
    "    y_s = y[train_idx].std(axis=0)\n",
    "\n",
    "    y = (y - y_c)/y_s\n",
    "\n",
    "\n",
    "\n",
    "    training_dataloader = DataLoader(BigDataset(\n",
    "        lookups_are_filtered = False,\n",
    "        lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n",
    "        lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "        y =           torch.from_numpy(y).to(torch.float32)[:, None],\n",
    "        G =           vals,\n",
    "        G_type = 'raw',\n",
    "        send_batch_to_gpu = 'cuda:0'\n",
    "        ),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True \n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(BigDataset(\n",
    "        lookups_are_filtered = False,\n",
    "        lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n",
    "        lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "        y =           torch.from_numpy(y).to(torch.float32)[:, None],\n",
    "        G =           vals,\n",
    "        G_type = 'raw',\n",
    "        send_batch_to_gpu = 'cuda:0'\n",
    "        ),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = False \n",
    "    )\n",
    "\n",
    "    return training_dataloader, validation_dataloader\n",
    "\n",
    "\n",
    "training_dataloader, validation_dataloader = _prep_data_to_dls(\n",
    "        obs_geno_lookup= obs_geno_lookup,\n",
    "        vals= vals,\n",
    "        batch_size= batch_size,\n",
    "        params_data= params_data\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mk_holdout_lists(\n",
    "    possible_holdouts = [\n",
    "        ## 2022 ====\n",
    "        'LH244', # Held out in Hyps tuning\n",
    "        ## 2021 ====\n",
    "        'PHZ51', # Held out in Hyps tuning\n",
    "        'PHP02',\n",
    "        'PHK76',\n",
    "        ## 2019 ====\n",
    "        'PHT69',\n",
    "        'LH195', # Held out in Hyps tuning\n",
    "        ## 2017 ====\n",
    "        'PHW52',\n",
    "        'PHN82',\n",
    "        ## 2016 ====\n",
    "        'DK3IIH6',\n",
    "        ## 2015 ====\n",
    "        'PHB47',\n",
    "        'LH82',\n",
    "        ## 2014 ====\n",
    "        'LH198',\n",
    "        'LH185',\n",
    "        'PB80',\n",
    "        'CG102',\n",
    " ],\n",
    "    n_holdouts = 3,\n",
    "    np_seed = 6642,\n",
    "):\n",
    "    rng = np.random.default_rng(seed=np_seed)\n",
    "    h = np.array(possible_holdouts)\n",
    "    rng.shuffle(h)\n",
    "\n",
    "    n_blocks = int(np.floor(len(h)/n_holdouts))\n",
    "    blocks = np.repeat(np.array([i for i in range(n_blocks)]), [n_holdouts])\n",
    "\n",
    "    return [h[blocks == i].tolist() for i in range(n_blocks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON path and log dir\n",
    "def read_hyp_exp_results(exp_name, exp_yvar, **kwargs):\n",
    "    if 'log_path' in kwargs:\n",
    "        if kwargs['log_path'] != None:\n",
    "            log_path = kwargs['log_path']\n",
    "        else:\n",
    "            log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}__{exp_yvar}'\n",
    "    else:\n",
    "        log_path = f'../nbs_artifacts/{exp_name}/lightning/{exp_name}__{exp_yvar}'\n",
    "    json_path = log_path+'.json'\n",
    "\n",
    "    # get all versions, confirm that a metrics file exists\n",
    "    version_logs = os.listdir(log_path)\n",
    "    version_logs = [e for e in version_logs if os.path.exists(f'{log_path}/{e}/metrics.csv')]\n",
    "    # extract and sort ints\n",
    "    version_logs = sorted([int(e.split('_')[-1]) for e in version_logs])\n",
    "\n",
    "    # Produce a tidy df for each version\n",
    "    def _get_tidy_metrics(metrics_path, i):\n",
    "        df = pd.read_csv(metrics_path)\n",
    "        df = pd.melt(\n",
    "                    df, \n",
    "                    id_vars= ['step', 'epoch'],\n",
    "                    var_name= 'split',\n",
    "                    value_vars= ['train_loss', 'val_loss'],\n",
    "                    value_name= 'loss'\n",
    "                ).dropna(\n",
    "                ).reset_index(drop=True\n",
    "                ).assign(version = i)\n",
    "        return(df)\n",
    "    # all training histories\n",
    "    metrics = pd.concat([_get_tidy_metrics(metrics_path = f'{log_path}/version_{i}/metrics.csv', \n",
    "                                           i = i) for i in version_logs])\n",
    "    \n",
    "    if os.path.exists(json_path):\n",
    "        return (AxClient.load_from_json_file(filepath = json_path), metrics)\n",
    "    else:\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/ax/storage/json_store/decoder.py:304: AxParameterWarning: `sort_values` is not specified for `ChoiceParameter` \"default_reps_nodes_inp\". Defaulting to `True` for parameters of `ParameterType` INT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "  return _class(\n",
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/ax/storage/json_store/decoder.py:304: AxParameterWarning: `sort_values` is not specified for `ChoiceParameter` \"default_reps_nodes_edge\". Defaulting to `True` for parameters of `ParameterType` INT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "  return _class(\n",
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/ax/storage/json_store/decoder.py:304: AxParameterWarning: `sort_values` is not specified for `ChoiceParameter` \"default_reps_nodes_out\". Defaulting to `True` for parameters of `ParameterType` INT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "  return _class(\n",
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/ax/storage/json_store/decoder.py:304: AxParameterWarning: `sort_values` is not specified for `ChoiceParameter` \"default_decay_rate\". Defaulting to `True` for parameters of `ParameterType` FLOAT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "  return _class(\n",
      "[INFO 06-26 13:44:40] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the `verbose_logging` argument to `False`. Note that float values in the logs are rounded to 6 decimal points.\n",
      "[WARNING 06-26 13:44:40] ax.modelbridge.cross_validation: Metric train_loss was unable to be reliably fit.\n",
      "[WARNING 06-26 13:44:40] ax.service.utils.best_point: Model fit is poor; falling back on raw data for best point.\n",
      "[WARNING 06-26 13:44:40] ax.service.utils.best_point: Model fit is poor and data on objective metric train_loss is noisy; interpret best points results carefully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'default_out_nodes_inp': 3,\n",
       " 'default_out_nodes_edge': 25,\n",
       " 'default_drop_nodes_inp': 0.10965091358870267,\n",
       " 'default_drop_nodes_edge': 0.9536853883601725,\n",
       " 'default_drop_nodes_out': 0.9462229676358401,\n",
       " 'default_reps_nodes_inp': 1,\n",
       " 'default_reps_nodes_edge': 3,\n",
       " 'default_reps_nodes_out': 3,\n",
       " 'default_decay_rate': 7.0,\n",
       " 'default_out_nodes_out': 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the experiment name from cache path and then swap the suffix to be the hyperparams step\n",
    "hyps_dir = [e for e in cache_path.split('/') if e != ''][-1].replace('1mods', '0hyps')\n",
    "\n",
    "ax_client, metrics = read_hyp_exp_results(\n",
    "    exp_name = hyps_dir,\n",
    "    # the string if only one yvar, the whole list if multi\n",
    "    exp_yvar = (lambda x: x[0] if len(x) == 1 else x)(y_var), \n",
    "    # Pass in log_path if we have a multi-y model (this will prevent log_path from being  dynamically set)\n",
    "    # If None log_path will be \n",
    "    log_path = (lambda x: \n",
    "                None if len(x) == 1 \n",
    "                else '../nbs_artifacts/'+hyps_dir+'/lightning/'+hyps_dir\n",
    "                )(y_var)\n",
    ")\n",
    "\n",
    "idx, params, loss = ax_client.get_best_trial()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdouts = _mk_holdout_lists(\n",
    "    n_holdouts = 3,\n",
    "    np_seed = 6642, # the 1st entry here is the same cv that I used for \n",
    ")\n",
    "\n",
    "\n",
    "for holdout in holdouts:\n",
    "    tmp_params = {\n",
    "                k:(params_data[k]\n",
    "                if k != 'holdout_parents'\n",
    "                else holdout )\n",
    "                for k in params_data \n",
    "                }\n",
    "    training_dataloader, validation_dataloader = _prep_data_to_dls(\n",
    "            obs_geno_lookup= obs_geno_lookup,\n",
    "            vals= vals,\n",
    "            batch_size= batch_size,\n",
    "            # Here I'm using a dictionary comprehension to make up a replacement to params_data on the fly. \n",
    "            # I'm not overwriting holdout_parents to make bugs easier to track\n",
    "            params_data= tmp_params\n",
    "            )\n",
    "\n",
    "    # pulled from evaluate(parameterization)\n",
    "    myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = params, ACGT_gene_slice_list = ACGT_gene_slice_list)\n",
    "    M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)\n",
    "    layer_list =  vnn_factory_3(M_list = M_list)\n",
    "    model = NeuralNetwork(layer_list = layer_list)\n",
    "\n",
    "    VNN = plDNN_general(model)  \n",
    "    optimizer = VNN.configure_optimizers()\n",
    "    logger = CSVLogger(lightning_log_dir, name=exp_name)\n",
    "    logger.log_hyperparams(params={\n",
    "        'params': params,\n",
    "        'params_data': tmp_params\n",
    "    })\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "    trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n",
    "\n",
    "\n",
    "    # base name \n",
    "    prefix = ''.join([\n",
    "        'vnn',\n",
    "        '__'+(lambda x: x[0] if len(x) == 1 else 'multitarget')(x=y_var)+'__',\n",
    "        '_'.join(holdout)\n",
    "        ]\n",
    "        )\n",
    "    ensure_dir_path_exists(cache_path+'/models/')\n",
    "    \n",
    "\n",
    "    # add a pseudorep number\n",
    "    pseudo_reps = sorted([e for e in os.listdir(cache_path+'/models/') if e[0:len(prefix)] == prefix])\n",
    "    if pseudo_reps == []:\n",
    "        save_name = prefix+'_rep00.pt'\n",
    "    else:\n",
    "        # increment replicate number\n",
    "        rep_num = pseudo_reps[-1].split('_')[-1].replace('rep', '').replace('.pt', '')\n",
    "        rep_num = (lambda x: '0'+x if len(x) == 1 else x)( str(1+int(rep_num)) )\n",
    "        save_name = prefix+f'_rep{rep_num}.pt'\n",
    "\n",
    "    # save the fitted model\n",
    "    torch.save(model, cache_path+'models/'+save_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
