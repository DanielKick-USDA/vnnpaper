{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#---\n",
    "#skip_exec: true\n",
    "#---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Data for GAPIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ----\n",
    "from dataG2F.core import get_data\n",
    "from dataG2F.qol  import ensure_dir_path_exists\n",
    "\n",
    "# Data Utilities ----\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "# Model Building  ----\n",
    "## General ====\n",
    "# import torch\n",
    "# from   torch import nn\n",
    "# import torch.nn.functional as F\n",
    "# from   torch.utils.data import Dataset\n",
    "# from   torch.utils.data import DataLoader\n",
    "\n",
    "# from vnnpaper.zma import \\\n",
    "#     BigDataset,    \\\n",
    "#     plDNN_general, \\\n",
    "#     mask_parents,  \\\n",
    "#     vnn_factory_1, \\\n",
    "#     vnn_factory_2, \\\n",
    "#     vnn_factory_3\n",
    "\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_notebook_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/zma_g2f_individual_gapit/genotypes_holding/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings ====\n",
    "# data settings\n",
    "params_data = {\n",
    "    'y_var': [\n",
    "        # Description quoted from competition data readme\n",
    "        'Yield_Mg_ha',     # Grain yield in Mg per ha at 15.5% grain moisture, using plot area without alley (Mg/ha).\n",
    "        'Pollen_DAP_days', # Number of days after planting that 50% of plants in the plot began shedding pollen.\n",
    "        'Silk_DAP_days',   # Number of days after planting that 50% of plants in the plot had visible silks.\n",
    "        'Plant_Height_cm', # Measured as the distance between the base of a plant and the ligule of the flag leaf (centimeter).\n",
    "        'Ear_Height_cm',   # Measured as the distance from the ground to the primary ear bearing node (centimeter).\n",
    "        'Grain_Moisture',  # Water content in grain at harvest (percentage).\n",
    "        'Twt_kg_m3'        # Shelled grain test weight (kg/m3), a measure of grain density.\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n",
    "\n",
    "# if 'None' != params_data['y_resid_strat']:\n",
    "#     save_prefix = save_prefix+'_'+params_data['y_resid_strat']\n",
    "\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep ----\n",
    "# obs_geno_lookup          = get_data('obs_geno_lookup')\n",
    "phno                     = get_data('phno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if '5_Genotype_Data_All_Years_vnn.hmp.txt' not in os.listdir(cache_path):\n",
    "#     # write out a reference for TASSEL\n",
    "#     # if the target doesn't exist then write this txt file and then filter the full dataset to these chromosome/positions using tassel\n",
    "#     import re\n",
    "#     parsed_kegg_gene_entries = get_data('KEGG_entries')\n",
    "    \n",
    "#     for ee in parsed_kegg_gene_entries:\n",
    "#         e = ee['POSITION']\n",
    "#         chrm, _ = e.split(':')\n",
    "#         _ = [int(i) for i in re.findall('\\d+\\.\\.\\d+', _)[0].split('..')]\n",
    "#         with open('./zma_filter_pos.txt', 'a') as f:\n",
    "#             f.writelines( (f'{chrm}\\t{i}\\n' for i in range(_[0], _[1])) )\n",
    "\n",
    "\n",
    "if '5_Genotype_Data_All_Years_acgt.hmp.txt' not in os.listdir(cache_path):\n",
    "    # write out a reference for TASSEL\n",
    "    # if the target doesn't exist then write this txt file and then filter the full dataset to these chromosome/positions using tassel\n",
    "    # This differes from the previous version in that we're writing out exactly the locations from dataG2F instead of parsing the kegg data and providing a list. \n",
    "    ACGT_gene_slice_loci     = get_data('KEGG_slices_names') # 'KEGG_slices_names': 'ACGT_gene_site_name_list.pkl',\n",
    "    tmp = sum(ACGT_gene_slice_loci, [])\n",
    "    \n",
    "    with open(cache_path+'/genotypes_holding/'+'zma_filter_pos_acgt.txt', 'w') as f:\n",
    "        f.writelines( ('\\t'.join(e.replace('S', '').split('_'))+'\\n' for e in tmp) )\n",
    "\n",
    "    # After this file is written, use Tassel to filter this:  \n",
    "    # dataG2F/data_ext/zma/g2fc/Training_Data/5_Genotype_Data_All_Years.hmp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a reduced set of SNPs for GWAS\n",
    "hmp_path = cache_path+'5_Genotype_Data_All_Years_acgt.hmp.txt'\n",
    "with open(hmp_path, 'r') as f:\n",
    "    dat = f.readline()\n",
    "# taxa = dat.split('\\t')[11:]\n",
    "# mask = phno.Hybrid.isin(taxa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hmp_path, 'r') as f:\n",
    "    dat = f.readlines()\n",
    "    dat = pd.DataFrame([e.split('\\t')[2:4] for e in dat][1:], columns=['chrom', 'pos'])\n",
    "    dat['chrom'] = dat['chrom'].astype(int)\n",
    "    dat['pos']   = dat['pos'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dist_to_snp(dat0):\n",
    "    cols = list(dat0)\n",
    "    dat0 = np.asarray(dat0)\n",
    "\n",
    "    # diff to next\n",
    "    dist_next = dat0[:, 1] - np.concatenate([np.asarray(np.nan)[None], dat0[:-1, 1]])\n",
    "    dist_prev = np.concatenate([dat0[1:, 1],  np.asarray(np.nan)[None]]) - dat0[:, 1]\n",
    "\n",
    "    # Looking for the maximum distance so that we perfer the edges of a gene instead of the center\n",
    "    # Consider:\n",
    "    # \n",
    "    # a            b b b           c     d\n",
    "    #\n",
    "    # We'll keep the obs on the edge. Decreasing the number of obs to use, we would discard all the bs before c even though the \n",
    "    # b gene is further away from a and c than c is from d. b snps are close to themselves so b will appear to be close.\n",
    "\n",
    "    maxdist = pd.DataFrame(\n",
    "        np.concatenate([dist_next[:, None], dist_prev[:, None]], axis=1).max(axis=1), columns=['Dist']\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(np.concatenate([dat0, maxdist], axis = 1), columns=cols+['dist']) \n",
    "\n",
    "# _dist_to_snp(dat0 = dat.loc[mask, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 30000 SNPS, all SNPS are at least 1782 from the next closest\n"
     ]
    }
   ],
   "source": [
    "# Get a certain number of snps, spaced as far apart as possible in each chromosome.\n",
    "\n",
    "n_snps = 30000\n",
    "\n",
    "x = pd.concat([_dist_to_snp(dat0 = dat.loc[(dat.chrom == chrom), ]) for chrom in dat.chrom.unique()])\n",
    "x.loc[(x.dist.isna()), 'dist'] = x.dist.max() # code first and last as max dist\n",
    "x = x.sort_values('dist', ascending=False).reset_index(drop=True)\n",
    "x = x.loc[(x.index < n_snps), ]\n",
    "x = x.sort_values(['chrom', 'pos']).reset_index(drop=True)\n",
    "\n",
    "print(f'Using {n_snps} SNPS, all SNPS are at least {int(x.dist.min())} from the next closest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out a reference file for filtering\n",
    "for i in x.index:\n",
    "    chrom, pos, *rest = x.loc[i, ]\n",
    "    with open(cache_path+f'zma_filter_pos_{n_snps}.txt', 'a') as f:\n",
    "        f.writelines( f'{int(chrom)}\\t{int(pos)}\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get taxa\n",
    "hmp_path = cache_path+'5_Genotype_Data_All_Years_vnn.hmp.txt'\n",
    "with open(hmp_path, 'r') as f:\n",
    "    dat = f.readline()\n",
    "taxa = dat.split('\\t')[11:]\n",
    "mask = phno.Hybrid.isin(taxa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create phenotype data for all y vars\n",
    "for y, eb in [(y, eb) for y in params_data['y_var'] for eb in [False, True]]:\n",
    "    x_start = phno.loc[mask, ['Hybrid', 'Year', 'Env', y]].reset_index(drop = True)\n",
    "\n",
    "    if not eb: # eb is enviromental residual bool\n",
    "        # Average over observations to get a set of values small enought for BLINK/TASSEL.\n",
    "        # Average withing Env -> Year -> Hybrid\n",
    "        x = x_start.groupby(['Hybrid', 'Year', 'Env',]).agg('mean').reset_index().drop(columns='Env'\n",
    "            ).groupby(['Hybrid', 'Year',       ]).agg('mean').reset_index().drop(columns='Year'\n",
    "            ).groupby(['Hybrid',               ]).agg('mean').reset_index()\n",
    "    else:\n",
    "        env_means = x_start.drop(columns='Hybrid').groupby(['Year', 'Env',]).agg('mean').reset_index().rename(columns={y: 'env_mean'})\n",
    "        x = x_start.merge(env_means)\n",
    "        x[y] = x[y]-x['env_mean']\n",
    "        x = x.drop(columns=['env_mean'])\n",
    "        x = x.loc[:, ['Hybrid', y]]\n",
    "        x = x.groupby(['Hybrid',]).agg('mean').reset_index()\n",
    "\n",
    "    x = x.rename(columns={'Hybrid':'Taxa'})\n",
    "    # Now save as a file that tassel can work with that will look like:\n",
    "    # \"\"\"\n",
    "    # <Phenotype>\n",
    "    # taxa    data\n",
    "    # Taxa    YVarName\n",
    "    # CML61   0.9\n",
    "    # CI31A   0.8\n",
    "    # CML61   1.0\n",
    "    # \"\"\"\n",
    "\n",
    "\n",
    "    out_path = cache_path+'_'.join([\n",
    "        'phno'+(lambda x: '_eres' if x else '')(eb\n",
    "        )+f'_{y}.txt'\n",
    "        ]\n",
    "    )\n",
    "    with open(out_path, 'w') as f:\n",
    "        f.write(\"<Phenotype>\\ntaxa\\tdata\\n\")\n",
    "        f.write('\\t'.join(list(x))+'\\n')\n",
    "        for taxa, yvar in (x.loc[i] for i in x.index):\n",
    "            f.write(f\"{taxa}\\t{yvar}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
