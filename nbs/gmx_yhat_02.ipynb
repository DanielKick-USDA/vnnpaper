{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visible Neural Network - Hyperparamter tuning applied to many yvars individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ----\n",
    "from dataGMX.core import get_data # <- Soybean Data\n",
    "from dataG2F.qol  import ensure_dir_path_exists\n",
    "\n",
    "# Data Utilities ----\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "from EnvDL.dlfn import BigDataset, plDNN_general\n",
    "from EnvDL.sets import mask_columns\n",
    "\n",
    "# Model Building  ----\n",
    "## General ====\n",
    "import torch\n",
    "from   torch import nn\n",
    "import torch.nn.functional as F\n",
    "from   torch.utils.data import Dataset\n",
    "from   torch.utils.data import DataLoader\n",
    "\n",
    "## VNN ====\n",
    "import sparsevnn\n",
    "from   sparsevnn.core import\\\n",
    "    VNNHelper, \\\n",
    "    structured_layer_info, \\\n",
    "    SparseLinearCustom\n",
    "from   sparsevnn.kegg import \\\n",
    "    kegg_connections_build, \\\n",
    "    kegg_connections_clean, \\\n",
    "    kegg_connections_append_y_hat, \\\n",
    "    kegg_connections_sanitize_names\n",
    "\n",
    "# Hyperparameter Tuning ----\n",
    "import os # needed for checking history (saved by lightning) \n",
    "\n",
    "## Logging with Pytorch Lightning ====\n",
    "import lightning.pytorch as pl\n",
    "from   lightning.pytorch.loggers import CSVLogger # used to save the history of each trial (used by ax)\n",
    "\n",
    "## Adaptive Experimentation Platform ====\n",
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "from ax.utils.notebook.plotting import init_notebook_plotting, render\n",
    "\n",
    "# For logging experiment results in sql database\n",
    "from ax.storage.sqa_store.db import init_engine_and_session_factory\n",
    "from ax.storage.sqa_store.db import get_engine, create_all_tables\n",
    "from ax.storage.sqa_store.save import save_experiment # saving\n",
    "from ax.storage.sqa_store.structs import DBSettings # loading\n",
    "# from ax.storage.sqa_store.load import load_experiment # loading alternate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_notebook_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/gmx_yhat_02/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run settings: \n",
    "params_run = {\n",
    "    'batch_size': 256, \n",
    "    'max_epoch' : 256 #512, # 256, \n",
    "}\n",
    "\n",
    "# data settings\n",
    "params_data = {\n",
    "    'y_var': [\n",
    "        'sdwt100', 'ProteinDry', 'OilDry', 'AshDry', 'FiberDry', 'LysineDry', \n",
    "        'CysteineDry', 'MethionineDry', 'ThreonineDry', 'TryptophanDry', 'IsoleucineDry', \n",
    "        'LeucineDry', 'HistidineDry', 'PhenylalanineDry', 'ValineDry', 'AlanineDry', \n",
    "        'ArginineDry', 'AsparticacidDry', 'GlutamicacidDry', 'GlycineDry', 'ProlineDry', \n",
    "        'SerineDry', 'TyrosineDry', 'SucroseDry', 'Linolenic', 'Linoleic', 'Oleic', \n",
    "        'Palmitic', 'Moisture', 'RaffinoseDry', 'StachyoseDry', 'Stearic', 'SeedAS', \n",
    "        'SeedPL', 'SeedW', 'SeedLWR', 'SeedCS', 'SeedL', 'SampleWeight', 'B11', 'Na23', \n",
    "        'Mg26', 'Al27', 'P31', 'S34', 'K39', 'Ca44', 'Mn55', 'Ni60', 'Cu63', 'Zn66', \n",
    "        'Fe54', 'Co59', 'Se78', 'Rb85', 'Sr88', 'Mo98', 'Cd111', 'As75'],\n",
    "    'y_resid': 'None', # None, Env, Geno\n",
    "    'y_resid_strat': 'None', # None, naive_mean, filter_mean, ...\n",
    "    'holdout_parents': { # For this dataset a percent of genotypes are randomly held out. The seed value makes this reproducible. May switch to similarity based approach.\n",
    "        'rng_seed': 9874325,\n",
    "        'pr': 0.2} \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings ====\n",
    "run_hyps = 2#5 \n",
    "run_hyps_force = False # should we run more trials even if the target number has been reached?\n",
    "max_hyps = 50\n",
    "\n",
    "params_list = [    \n",
    "    ## Output Size ====\n",
    "    {\n",
    "    'name': 'default_out_nodes_inp',\n",
    "    'type': 'range',\n",
    "    'bounds': [1, 8],\n",
    "    'value_type': 'int',\n",
    "    'log_scale': False\n",
    "    },\n",
    "    {\n",
    "    'name': 'default_out_nodes_edge',\n",
    "    'type': 'range',\n",
    "    'bounds': [1, 32],\n",
    "    'value_type': 'int',\n",
    "    'log_scale': False\n",
    "    },\n",
    "    {\n",
    "    'name': 'default_out_nodes_out',\n",
    "    'type': 'fixed',\n",
    "    'value': 1, # len(params_data['y_var']) if type(params_data['y_var']) else 1,\n",
    "    'value_type': 'int',\n",
    "    'log_scale': False\n",
    "    },\n",
    "    ## Dropout ====\n",
    "    {\n",
    "    'name': 'default_drop_nodes_inp',\n",
    "    'type': 'range',\n",
    "    'bounds': [0.01, 0.99],\n",
    "    'value_type': 'float',\n",
    "    'log_scale': False\n",
    "    },\n",
    "    {\n",
    "    'name': 'default_drop_nodes_edge',\n",
    "    'type': 'range',\n",
    "    'bounds': [0.01, 0.99],\n",
    "    'value_type': 'float',\n",
    "    'log_scale': False\n",
    "    },\n",
    "    {\n",
    "    'name': 'default_drop_nodes_out',\n",
    "    'type': 'range',\n",
    "    'bounds': [0.01, 0.99],\n",
    "    'value_type': 'float',\n",
    "    'log_scale': False,\n",
    "    'sort_values':True\n",
    "    },\n",
    "    ## Node Repeats ====\n",
    "    {\n",
    "    'name': 'default_reps_nodes_inp',\n",
    "    'type': 'choice',\n",
    "    'values': [1, 2, 3],\n",
    "    'value_type': 'int',\n",
    "    'is_ordered': True,\n",
    "    'sort_values':True\n",
    "    },\n",
    "    {\n",
    "    'name': 'default_reps_nodes_edge',\n",
    "    'type': 'choice',\n",
    "    'values': [1, 2, 3],\n",
    "    'value_type': 'int',\n",
    "    'is_ordered': True,\n",
    "    'sort_values':True\n",
    "    },\n",
    "    {\n",
    "    'name': 'default_reps_nodes_out',\n",
    "    'type': 'choice',\n",
    "    'values': [1, 2, 3],\n",
    "    'value_type': 'int',\n",
    "    'is_ordered': True,\n",
    "    'sort_values':True\n",
    "    },\n",
    "    ## Node Output Size Scaling ====\n",
    "    {\n",
    "    'name': 'default_decay_rate',\n",
    "    'type': 'choice',\n",
    "    'values': [0+(0.1*i) for i in range(10)]+[1.+(1*i) for i in range(11)],\n",
    "    'value_type': 'float',\n",
    "    'is_ordered': True,\n",
    "    'sort_values':True\n",
    "    }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_log_dir = cache_path+\"lightning\"\n",
    "exp_name = [e for e in cache_path.split('/') if e != ''][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameterization is needed for setup. These values will be overwritten by Ax if tuning is occuring. \n",
    "# in this file I define params later. I've included it here to gurantee that we can merge other params dicts into it.\n",
    "params = {\n",
    "'default_out_nodes_inp'  : 1,\n",
    "'default_out_nodes_edge' : 1,\n",
    "'default_out_nodes_out'  : len(params_data['y_var']) if type(params_data['y_var']) else 1,\n",
    "\n",
    "'default_drop_nodes_inp' : 0.0,\n",
    "'default_drop_nodes_edge': 0.0,\n",
    "'default_drop_nodes_out' : 0.0,\n",
    "\n",
    "'default_reps_nodes_inp' : 1,\n",
    "'default_reps_nodes_edge': 1,\n",
    "'default_reps_nodes_out' : 1,\n",
    "\n",
    "'default_decay_rate'     : 1\n",
    "}\n",
    "\n",
    "default_out_nodes_inp  = params['default_out_nodes_inp' ]\n",
    "default_out_nodes_edge = params['default_out_nodes_edge'] \n",
    "default_out_nodes_out  = params['default_out_nodes_out' ]\n",
    "\n",
    "default_drop_nodes_inp = params['default_drop_nodes_inp' ] \n",
    "default_drop_nodes_edge= params['default_drop_nodes_edge'] \n",
    "default_drop_nodes_out = params['default_drop_nodes_out' ] \n",
    "\n",
    "default_reps_nodes_inp = params['default_reps_nodes_inp' ]\n",
    "default_reps_nodes_edge= params['default_reps_nodes_edge']\n",
    "default_reps_nodes_out = params['default_reps_nodes_out' ]\n",
    "\n",
    "default_decay_rate = params['default_decay_rate' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = params_run['batch_size']\n",
    "max_epoch  = params_run['max_epoch']\n",
    "\n",
    "y_var = params_data['y_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n",
    "\n",
    "if 'None' != params_data['y_resid_strat']:\n",
    "    save_prefix = save_prefix+'_'+params_data['y_resid_strat']\n",
    "\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu_num = 0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dist_scale_function(out, dist, decay_rate):\n",
    "    scale = 1/(1+decay_rate*dist)\n",
    "    out = round(scale * out)\n",
    "    out = max(1, out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_node_shortcut(vnn_helper, query = 'y_hat'):\n",
    "    # define new entries\n",
    "    if True in [True if e in vnn_helper.edge_dict.keys() else False for e in \n",
    "                [f'{query}_res_-2', f'{query}_res_-1']\n",
    "                ]:\n",
    "        print('Warning! New node name already exists! Overwriting existing node!')\n",
    "\n",
    "    # Add residual connection in graph\n",
    "    vnn_helper.edge_dict[f'{query}_res_-2'] = myvnn.edge_dict[query] \n",
    "    vnn_helper.edge_dict[f'{query}_res_-1'] = [f'{query}_res_-2']\n",
    "    vnn_helper.edge_dict[query]             = [f'{query}_res_-2', f'{query}_res_-1']\n",
    "\n",
    "    # Add new nodes, copying information from query node\n",
    "    vnn_helper.node_props[f'{query}_res_-2'] = vnn_helper.node_props[query] \n",
    "    vnn_helper.node_props[f'{query}_res_-1'] = vnn_helper.node_props[query]\n",
    "\n",
    "    return vnn_helper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep ----\n",
    "obs_geno_lookup          = get_data('obs_geno_lookup')\n",
    "phno                     = get_data('phno')\n",
    "ACGT_gene_slice_list     = get_data('KEGG_slices')\n",
    "parsed_kegg_gene_entries = get_data('KEGG_entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dataset is very close to balanced wrt to the genotypes with 9-18 obs. \n",
    "# This code will turn a seed value and percent to be held out into a list of genotypse\n",
    "rng = np.random.default_rng( params_data['holdout_parents']['rng_seed'] )\n",
    "tmp = get_data(name = 'phno')\n",
    "\n",
    "taxa = sorted(list(set(tmp.Taxa)))\n",
    "rng.shuffle(taxa)\n",
    "taxa = pd.DataFrame(taxa).reset_index().rename(columns={0:'Taxa', 'index':'DrawOrder'})\n",
    "\n",
    "tmp = pd.merge(taxa, tmp).loc[:,['DrawOrder', 'Taxa']].assign(n=lambda x: 1).groupby(['DrawOrder', 'Taxa']).count().reset_index()\n",
    "tmp['cdf'] = tmp['n'].cumsum(0)/tmp['n'].sum()\n",
    "# filter\n",
    "holdout_parents = list(tmp.loc[(tmp.cdf <= params_data['holdout_parents']['pr'] ), 'Taxa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make holdout sets\n",
    "# holdout_parents = params_data['holdout_parents']\n",
    "\n",
    "# create a mask for parent genotype\n",
    "mask = mask_columns(df= phno, col_name= 'Taxa', holdouts= holdout_parents)\n",
    "\n",
    "\n",
    "train_mask = mask.sum(axis=1) == 0\n",
    "test_mask  = mask.sum(axis=1) > 0\n",
    "\n",
    "train_idx = train_mask.loc[train_mask].index\n",
    "test_idx  = test_mask.loc[test_mask].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y to residual if needed\n",
    "\n",
    "if params_data['y_resid'] == 'None':\n",
    "    pass\n",
    "# TODO update for GMX\n",
    "# else:\n",
    "#     if params_data['y_resid_strat'] == 'naive_mean':\n",
    "#         # use only data in the training set (especially since testers will be more likely to be found across envs)\n",
    "#         # get enviromental means, subtract from observed value\n",
    "#         tmp = phno.loc[train_idx, ]\n",
    "#         env_mean = tmp.groupby(['Env_Idx']\n",
    "#                      ).agg(Env_Mean = (y_var, 'mean')\n",
    "#                      ).reset_index()\n",
    "#         tmp = phno.merge(env_mean)\n",
    "#         tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n",
    "#         phno = tmp.drop(columns='Env_Mean')\n",
    "\n",
    "#     if params_data['y_resid_strat'] == 'filter_mean':\n",
    "#         # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations\n",
    "#         def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],\n",
    "#                                     year = 2014):\n",
    "#             # Within each year what hybrids are most common?\n",
    "#             tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')\n",
    "\n",
    "#             all_envs = set(tmp.Env)\n",
    "#             # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?\n",
    "#             # site counts for sets which contain all envs\n",
    "#             i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year >= i), 'Env'])) == len(all_envs)])\n",
    "\n",
    "#             before = len(set(tmp.loc[:, 'Hybrid']))\n",
    "#             after  = len(set(tmp.loc[(tmp.Year >= i), 'Hybrid']))\n",
    "#             print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')\n",
    "#             tmp = tmp.loc[(tmp.Year >= i), ['Env', 'Hybrid']].reset_index(drop=True)\n",
    "#             return tmp\n",
    "\n",
    "\n",
    "#         tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]\n",
    "#         filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) \n",
    "#                           for i in list(set(phno.Year)) ]\n",
    "#         env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')\n",
    "\n",
    "#         env_mean = env_mean.groupby(['Env_Idx']\n",
    "#                           ).agg(Env_Mean = (y_var, 'mean')\n",
    "#                           ).reset_index()\n",
    "\n",
    "#         tmp = phno.merge(env_mean)\n",
    "#         tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n",
    "#         phno = tmp.drop(columns='Env_Mean')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and y value data\n",
    "assert 0 == phno.loc[:, y_var].isna().sum().sum() # second sum is for multiple y_vars\n",
    "\n",
    "y = phno.loc[:, y_var].to_numpy() # added to make multiple ys work\n",
    "# use train index to prevent information leakage\n",
    "y_c = y[train_idx].mean(axis=0)\n",
    "y_s = y[train_idx].std(axis=0)\n",
    "\n",
    "y = (y - y_c)/y_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Using VNNHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vnn_factory_1(parsed_kegg_gene_entries, params):\n",
    "\n",
    "    print(''.join('#' for i in range(80)))\n",
    "    print(params)\n",
    "    print(''.join('#' for i in range(80)))\n",
    "    \n",
    "    \n",
    "    default_out_nodes_inp  = params['default_out_nodes_inp' ]\n",
    "    default_out_nodes_edge = params['default_out_nodes_edge'] \n",
    "    default_out_nodes_out  = params['default_out_nodes_out' ]\n",
    "\n",
    "    default_drop_nodes_inp = params['default_drop_nodes_inp' ] \n",
    "    default_drop_nodes_edge= params['default_drop_nodes_edge'] \n",
    "    default_drop_nodes_out = params['default_drop_nodes_out' ] \n",
    "\n",
    "    default_reps_nodes_inp = params['default_reps_nodes_inp' ]\n",
    "    default_reps_nodes_edge= params['default_reps_nodes_edge']\n",
    "    default_reps_nodes_out = params['default_reps_nodes_out' ]\n",
    "\n",
    "\n",
    "\n",
    "    default_decay_rate = params['default_decay_rate' ]\n",
    "\n",
    "\n",
    "\n",
    "    # Clean up KEGG Pathways -------------------------------------------------------\n",
    "    # Same setup as above to create kegg_gene_brite\n",
    "    # Restrict to only those with pathway\n",
    "    kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n",
    "\n",
    "    # also require to have a non-empty path\n",
    "    kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n",
    "\n",
    "    print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n",
    "        )+'/'+str(len(parsed_kegg_gene_entries)\n",
    "        )+' Entries'\n",
    "        )\n",
    "    # kegg_gene_brite[1]['BRITE']['BRITE_PATHS']\n",
    "\n",
    "\n",
    "    kegg_connections = kegg_connections_build(kegg_gene_brite = kegg_gene_brite, \n",
    "                                            n_genes = len(kegg_gene_brite)) \n",
    "    kegg_connections = kegg_connections_clean(         kegg_connections = kegg_connections)\n",
    "    #TODO think about removing \n",
    "    # \"Not Included In\n",
    "    # Pathway Or Brite\"\n",
    "    # or reinstate 'Others'\n",
    "\n",
    "    kegg_connections = kegg_connections_append_y_hat(  kegg_connections = kegg_connections)\n",
    "    kegg_connections = kegg_connections_sanitize_names(kegg_connections = kegg_connections, \n",
    "                                                    replace_chars = {'.':'_'})\n",
    "\n",
    "\n",
    "    # Initialize helper for input nodes --------------------------------------------\n",
    "    myvnn = VNNHelper(edge_dict = kegg_connections)\n",
    "\n",
    "    # Get a mapping of brite names to tensor list index\n",
    "    find_names = myvnn.nodes_inp # e.g. ['100383860', '100278565', ... ]\n",
    "    lookup_dict = {}\n",
    "\n",
    "    # the only difference lookup_dict and brite_node_to_list_idx_dict above is that this is made using the full set of genes in the list \n",
    "    # whereas that is made using kegg_gene_brite which is a subset\n",
    "    for i in range(len(parsed_kegg_gene_entries)):\n",
    "        if 'BRITE' not in parsed_kegg_gene_entries[i].keys():\n",
    "            pass\n",
    "        elif parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'] == []:\n",
    "            pass\n",
    "        else:\n",
    "            name = parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'][0][-1]\n",
    "            if name in find_names:\n",
    "                lookup_dict[name] = i\n",
    "    # lookup_dict    \n",
    "\n",
    "    brite_node_to_list_idx_dict = {}\n",
    "    for i in range(len(kegg_gene_brite)):\n",
    "        brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i        \n",
    "\n",
    "    # Get the input sizes for the graph\n",
    "    size_in_zip = zip(myvnn.nodes_inp, [np.prod(ACGT_gene_slice_list[lookup_dict[e]].shape[1:]) for e  in myvnn.nodes_inp])\n",
    "\n",
    "    # Set node defaults ------------------------------------------------------------\n",
    "    # init input node sizes\n",
    "    myvnn.set_node_props(key = 'inp', node_val_zip = size_in_zip)\n",
    "\n",
    "    # init node output sizes\n",
    "    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_inp, [default_out_nodes_inp  for e in myvnn.nodes_inp]))\n",
    "    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_edge,[default_out_nodes_edge for e in myvnn.nodes_edge]))\n",
    "    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_out, [default_out_nodes_out  for e in myvnn.nodes_out]))\n",
    "\n",
    "    # # options should be controlled by node_props\n",
    "    myvnn.set_node_props(key = 'flatten', node_val_zip = zip(myvnn.nodes_inp, [True for e in myvnn.nodes_inp]))\n",
    "\n",
    "    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_inp, [default_reps_nodes_inp  for e in myvnn.nodes_inp]))\n",
    "    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_edge,[default_reps_nodes_edge for e in myvnn.nodes_edge]))\n",
    "    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_out, [default_reps_nodes_out  for e in myvnn.nodes_out]))\n",
    "\n",
    "    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_inp, [default_drop_nodes_inp  for e in myvnn.nodes_inp]))\n",
    "    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_edge,[default_drop_nodes_edge for e in myvnn.nodes_edge]))\n",
    "    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_out, [default_drop_nodes_out  for e in myvnn.nodes_out]))\n",
    "\n",
    "\n",
    "    # Scale node outputs by distance -----------------------------------------------\n",
    "    dist = sparsevnn.core.vertex_from_end(\n",
    "        edge_dict = myvnn.edge_dict,\n",
    "        end =myvnn.dependancy_order[-1]\n",
    "    )\n",
    "\n",
    "    # overwrite node outputs with a size inversely proportional to distance from prediction node\n",
    "    for query in list(dist.keys()):\n",
    "        myvnn.node_props[query]['out'] = _dist_scale_function(\n",
    "            out = myvnn.node_props[query]['out'],\n",
    "            dist = dist[query],\n",
    "            decay_rate = default_decay_rate)\n",
    "        \n",
    "\n",
    "    # Expand out node replicates ---------------------------------------------------\n",
    "    # kegg_connections_expanded = sparsevnn.core.expand_edge_dict(vnn_helper = myvnn, edge_dict = myvnn.edge_dict)\n",
    "\n",
    "    # ======================================================= #\n",
    "    # one place to add residual connections would be here.    #\n",
    "    # edit the links before instatinating the new VNNHelper   #\n",
    "    # the important thing to do is to edit the graph before   #\n",
    "    # calulating the inputs for each node.                    #\n",
    "    # ======================================================= #\n",
    "\n",
    "            # # expand then copy over the properties that have already been defined.\n",
    "            # myvnn_exp = VNNHelper(edge_dict = kegg_connections_expanded)\n",
    "\n",
    "            # import re\n",
    "            # for new_key in list(myvnn_exp.node_props.keys()):\n",
    "            #     if new_key in myvnn.node_props.keys():\n",
    "            #         # copy directly\n",
    "            #         myvnn_exp.node_props[new_key] = myvnn.node_props[new_key]\n",
    "            #     else:\n",
    "            #         # check for a key that matches the query key after removing the replicate information\n",
    "            #         query = new_key \n",
    "            #         suffix = re.findall('_rep_\\d+$', query)[0]\n",
    "\n",
    "            #         query = query.removesuffix(suffix)\n",
    "            #         if query in myvnn.node_props.keys():\n",
    "            #             myvnn_exp.node_props[new_key] = myvnn.node_props[query]\n",
    "            #         else:\n",
    "            #             print(f'WARNING: no entry {query} found for {new_key}') \n",
    "\n",
    "            # # now main vnn is the expanded version\n",
    "            # myvnn = myvnn_exp\n",
    "\n",
    "\n",
    "            # # Cleanup ----------------------------------------------------------------------\n",
    "            # # now the original VNNHelper isn't needed\n",
    "            # myvnn = myvnn_exp\n",
    "\n",
    "\n",
    "    # expand out graph.\n",
    "#     update_edge_links = {}\n",
    "#     nodes = myvnn.dependancy_order\n",
    "\n",
    "#     for query in [e for e in reversed(nodes)]:\n",
    "#         if myvnn.node_props[query]['reps'] == 1:\n",
    "#             pass\n",
    "#         else:\n",
    "#             reps = myvnn.node_props[query]['reps']\n",
    "#             # set to 1 so that we can copy all the props (except input) over\n",
    "#             myvnn.node_props[query]['reps'] = 1\n",
    "            \n",
    "#             for i in range(reps-1,0,-1):\n",
    "#                 if i == 0:\n",
    "#                     # no replicates\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     if i == 1:\n",
    "#                         # print({f'{query}_{i}':[f'{query}']})\n",
    "#                         update_edge_links[f'{query}_{i}'] = f'{query}'\n",
    "#                     else:\n",
    "#                         # print({f'{query}_{i}':[f'{query}_{i-1}']})\n",
    "#                         update_edge_links[f'{query}_{i}'] = f'{query}_{i-1}'\n",
    "\n",
    "#                     # copy over all properties except input (input will either be set for the data or calculated on the fly)\n",
    "#                     myvnn.node_props[f'{query}_{i}'] = {k:myvnn.node_props[query][k] for k in myvnn.node_props[query] if k != 'inp'}\n",
    "\n",
    "\n",
    "#     # Now there should be new nodes in the helper but the links need to be updated to point to the right names. \n",
    "\n",
    "#     if True:\n",
    "#         # update existing links\n",
    "#         # create a lookup dictionary to map the old names to new names\n",
    "#         old_to_new = {update_edge_links[k]:k for k in update_edge_links}\n",
    "#         # old_to_new\n",
    "\n",
    "#         for k in myvnn.edge_dict:\n",
    "#             myvnn.edge_dict[k] = [e if e not in old_to_new.keys() else old_to_new[e] for e in myvnn.edge_dict[k]]\n",
    "\n",
    "#         # add in new nodes\n",
    "#         for k in update_edge_links:\n",
    "#             myvnn.edge_dict[k] = [update_edge_links[k]]\n",
    "\n",
    "#         # overwrite dependancy order\n",
    "#         # myvnn.dependancy_order = VNNHelper(edge_dict= myvnn.edge_dict).dependancy_order\n",
    "# # myvnn_updated = VNNHelper(edge_dict= myvnn.edge_dict)\n",
    "# # myvnn_updated.node_props = myvnn.node_props\n",
    "#         # myvnn = myvnn_updated\n",
    "\n",
    "    # expand out graph.\n",
    "    update_edge_links = {}\n",
    "    nodes = [node for node in myvnn.dependancy_order if myvnn.node_props[node]['reps'] > 1]\n",
    "\n",
    "    node_expansion_dict = {\n",
    "        node: [node if i==0 else f'{node}_{i}' for i in range(myvnn.node_props[node]['reps'])]\n",
    "        for node in nodes}\n",
    "    #   current       1st          2nd (new)      3rd (new)\n",
    "    # {'100798274': ['100798274', '100798274_1', '100798274_2'], ...\n",
    "\n",
    "    # the keys don't change here. The values will be updated and then new k:v will be inserted\n",
    "    myvnn.edge_dict = {k:[e if e not in node_expansion_dict.keys() \n",
    "        else node_expansion_dict[e][-1]\n",
    "        for e in myvnn.edge_dict[k] ] for k in myvnn.edge_dict}\n",
    "\n",
    "    # now insert connectsion to new nodes: A -> A_rep_1 -> A_rep_2\n",
    "    for node in node_expansion_dict:\n",
    "        for pair in zip(node_expansion_dict[node][1:], node_expansion_dict[node]):\n",
    "            myvnn.edge_dict[pair[0]] = [pair[1]]\n",
    "\n",
    "    # now add those new nodes\n",
    "    # create a new node for all the nodes\n",
    "    for node in node_expansion_dict:\n",
    "        for new_node in node_expansion_dict[node][1:]:\n",
    "            myvnn.node_props[new_node] = {k:myvnn.node_props[node][k] for k in myvnn.node_props[node] if k != 'inp'}\n",
    "\n",
    "    new_vnn = VNNHelper(edge_dict= myvnn.edge_dict)\n",
    "    new_vnn.node_props = myvnn.node_props\n",
    "    myvnn = new_vnn\n",
    "\n",
    "\n",
    "    # init edge node input size (propagate forward input/edge outpus)\n",
    "    myvnn.calc_edge_inp()\n",
    "\n",
    "    # replace lookup so that it matches the lenght of the input tensors\n",
    "    new_lookup_dict = {}\n",
    "    for i in range(len(myvnn.nodes_inp)):\n",
    "        new_lookup_dict[myvnn.nodes_inp[i]] = i\n",
    "    \n",
    "    return myvnn,  lookup_dict #new_lookup_dict\n",
    "\n",
    "myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate nodes membership in each matrix and positions within each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vnn_factory_2(vnn_helper, node_to_inp_num_dict):\n",
    "    myvnn = vnn_helper\n",
    "\n",
    "    node_props = myvnn.node_props\n",
    "    # Linear_block = Linear_block_reps,\n",
    "    edge_dict = myvnn.edge_dict\n",
    "    dependancy_order = myvnn.dependancy_order\n",
    "    node_to_inp_num_dict = new_lookup_dict\n",
    "\n",
    "    # Build dependancy dictionary --------------------------------------------------\n",
    "    # check dep order\n",
    "    tally = []\n",
    "    for d in dependancy_order:\n",
    "        if edge_dict[d] == []:\n",
    "            tally.append(d)\n",
    "        elif False not in [True if e in tally else False for e in edge_dict[d]]:\n",
    "            tally.append(d)\n",
    "        else:\n",
    "            print('error!')\n",
    "            break\n",
    "\n",
    "\n",
    "    # build output nodes \n",
    "    d_out = {0:[]}\n",
    "    for d in dependancy_order:\n",
    "        if edge_dict[d] == []:\n",
    "            d_out[min(d_out.keys())].append(d)\n",
    "        else:\n",
    "            # print((d, edge_dict[d]))\n",
    "\n",
    "            d_out_i = 1+max(sum([[key for key in d_out.keys() if e in d_out[key]]\n",
    "                    for e in edge_dict[d]], []))\n",
    "            \n",
    "            if d_out_i not in d_out.keys():\n",
    "                d_out[d_out_i] = []\n",
    "            d_out[d_out_i].append(d)\n",
    "\n",
    "\n",
    "    # build input nodes NOPE. THE PASSHTROUGHS! \n",
    "    d_eye = {}\n",
    "    tally = []\n",
    "    for i in range(max(d_out.keys()), min(d_out.keys()), -1):\n",
    "        # print(i)\n",
    "        nodes_needed = sum([edge_dict[e] for e in d_out[i]], [])+tally\n",
    "        # check against what is there and then dedupe\n",
    "        nodes_needed = [e for e in nodes_needed if e not in d_out[i-1]]\n",
    "        nodes_needed = list(set(nodes_needed))\n",
    "        tally = nodes_needed\n",
    "        d_eye[i] = nodes_needed\n",
    "\n",
    "    # d_inp[0]= d_out[0]\n",
    "    # [len(d_eye[i]) for i in d_eye.keys()]\n",
    "    # [(key, len(d_out[key])) for key in d_out.keys()]\n",
    "\n",
    "\n",
    "    dd = {}\n",
    "    for i in d_eye.keys():\n",
    "        dd[i] = {'out': d_out[i],\n",
    "                'inp': d_out[i-1],\n",
    "                'eye': d_eye[i]}\n",
    "    # plus special 0 layer that handles the snps\n",
    "        \n",
    "    dd[0] = {'out': d_out[0],\n",
    "            'inp': d_out[0],\n",
    "            'eye': []}\n",
    "\n",
    "\n",
    "    # check that the output nodes' inputs are satisfied by the same layer's inputs (inp and eye)\n",
    "    for i in dd.keys():\n",
    "        # out node in each\n",
    "        for e in dd[i]['out']:\n",
    "            # node depends in inp/eye\n",
    "            node_pass_list = [True if ee in dd[i]['inp']+dd[i]['eye'] else False \n",
    "                            for ee in edge_dict[e]]\n",
    "            if False not in node_pass_list:\n",
    "                pass\n",
    "            else:\n",
    "                print('exit') \n",
    "\n",
    "\n",
    "    # print(\"Layer\\t#In\\t#Out\")\n",
    "    # for i in range(min(dd.keys()), max(dd.keys())+1, 1):\n",
    "    #     node_in      = [node_props[e]['out'] for e in dd[i]['inp']+dd[i  ]['eye'] ]\n",
    "    #     if i == max(dd.keys()):\n",
    "    #         node_out = [node_props[e]['out'] for e in dd[i]['out'] ]\n",
    "    #     else:\n",
    "    #         node_out = [node_props[e]['out'] for e in dd[i]['out']+dd[i+1]['eye']]\n",
    "    #     print(f'{i}:\\t{sum(node_in)}\\t{sum(node_out)}')\n",
    "\n",
    "    M_list = [structured_layer_info(i = ii, node_groups = dd, node_props= node_props, edge_dict = edge_dict, as_sparse=True) for ii in range(0, max(dd.keys())+1)]\n",
    "    return M_list\n",
    "\n",
    "### Creating Structured Matrices for Layers\n",
    "M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Dataloader using `M_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = new_lookup_dict\n",
    "\n",
    "vals = get_data('KEGG_slices')\n",
    "vals = [torch.from_numpy(e).to(torch.float) for e in vals]\n",
    "# restrict to the tensors that will be used\n",
    "vals = torch.concat([vals[lookup_dict[i]].reshape(vals[0].shape[0], -1) \n",
    "                     for i in M_list[0].row_inp\n",
    "                    #  for i in dd[0]['inp'] # matches\n",
    "                     ], axis = 1)\n",
    "\n",
    "vals = vals.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n",
    "    lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "    y =           torch.from_numpy(y).to(torch.float32),\n",
    "    G =           vals,\n",
    "    G_type = 'raw',\n",
    "    send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True \n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n",
    "    lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "    y =           torch.from_numpy(y).to(torch.float32),\n",
    "    G =           vals,\n",
    "    G_type = 'raw',\n",
    "    send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_list):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer_list = nn.ModuleList(layer_list)\n",
    " \n",
    "    def forward(self, x):\n",
    "        for l in self.layer_list:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vnn_factory_3(M_list):\n",
    "    layer_list = []\n",
    "    for i in range(len(M_list)):\n",
    "        \n",
    "        apply_relu = None\n",
    "        if i+1 != len(M_list): # apply relu to all but the last layer\n",
    "            apply_relu = F.relu\n",
    "        \n",
    "\n",
    "        l = SparseLinearCustom(\n",
    "            M_list[i].weight.shape[1], # have to transpose this?\n",
    "            M_list[i].weight.shape[0],\n",
    "            connectivity   = torch.LongTensor(M_list[i].weight.coalesce().indices()),\n",
    "            custom_weights = M_list[i].weight.coalesce().values(), \n",
    "            custom_bias    = M_list[i].bias.clone().detach(), \n",
    "            weight_grad_bool = M_list[i].weight_grad_bool, \n",
    "            bias_grad_bool   = M_list[i].bias_grad_bool, #.to_sparse()#.indices()\n",
    "            dropout_p        = M_list[i].dropout_p,\n",
    "            nonlinear_transform= apply_relu\n",
    "            )\n",
    "\n",
    "        layer_list += [l]\n",
    "        \n",
    "    return layer_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny Test Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| type  | value key | value type  |\n",
    "|------:|--|--|\n",
    "| range | bounds | list |\n",
    "| choice| values | list |\n",
    "| fixed | value  | atomic |\n",
    "\n",
    "\n",
    "<!-- {\n",
    "#     \"name\": \"size\",\n",
    "#     \"type\": \"range\",\n",
    "#     \"bounds\": [1, 256],\n",
    "#     \"value_type\": \"int\",  \n",
    "#     \"log_scale\": False,  # For range, defaults to False.\n",
    "# },\n",
    "# {\n",
    "#     \"name\": \"drop\",\n",
    "#     \"type\": \"choice\",\n",
    "#     \"values\": [],    \n",
    "#     \"value_type\":\"int\", #\"float\", \"bool\", \"str\"\n",
    "#     \"is_ordered\": False, # For choice    \n",
    "    \n",
    "#     \"value_type\": \"float\",  # Optional, defaults to inference from type of \"bounds\".\n",
    "#     \"bounds\": [0.0, 1.0],\n",
    "# },\n",
    "# {\n",
    "#     \"name\": \"drop\",\n",
    "#     \"type\": \"fixed\",\n",
    "#     \"value\": [],     \n",
    "#     \"value_type\": \"float\",  # Optional, defaults to inference from type of \"bounds\".\n",
    "#     \"bounds\": [0.0, 1.0],\n",
    "# }\n",
    "\n",
    "# # constraints can be passed to ax_clien.create_experiment for `parameter_constraints` and `outcome_constraints`. For example see https://ax.dev/tutorials/gpei_hartmann_service.html -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a very funny trick. I'm going to call lighning from within Ax. \n",
    "# That way I can save out traces while also relying on Ax to choose new hyps. \n",
    "\n",
    "\n",
    "def evaluate(parameterization):\n",
    "    # draw from global\n",
    "    # max_epoch = 20\n",
    "    # lightning_log_dir = \"test_tb\"\n",
    "    myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = parameterization)\n",
    "    M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)\n",
    "    layer_list =  vnn_factory_3(M_list = M_list)\n",
    "    model = NeuralNetwork(layer_list = layer_list)\n",
    "    \n",
    "    VNN = plDNN_general(model)  \n",
    "    optimizer = VNN.configure_optimizers()\n",
    "    logger = CSVLogger(lightning_log_dir, name=exp_name)\n",
    "    logger.log_hyperparams(params={\n",
    "        'params': parameterization\n",
    "    })\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "    trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n",
    "\n",
    "\n",
    "    # if we were optimizing number of training epochs this would be an effective loss to use.\n",
    "    # trainer.callback_metrics['train_loss']\n",
    "    # float(trainer.callback_metrics['train_loss'])\n",
    "\n",
    "    # To potentially _overtrain_ models and still let the selction be based on their best possible performance,\n",
    "    # I'll use the lowest average error in an epoch\n",
    "    log_path = lightning_log_dir+'/'+exp_name\n",
    "    fls = os.listdir(log_path)\n",
    "    nums = [int(e.split('_')[-1]) for e in fls] \n",
    "\n",
    "    M = pd.read_csv(log_path+f\"/version_{max(nums)}/metrics.csv\")\n",
    "    M = M.loc[:, ['epoch', 'train_loss']].dropna()\n",
    "\n",
    "    M = M.groupby('epoch').agg(\n",
    "        train_loss = ('train_loss', 'mean'),\n",
    "        train_loss_sd = ('train_loss', 'std'),\n",
    "        ).reset_index()\n",
    "\n",
    "    train_metric = M.train_loss.min()\n",
    "    print(train_metric)\n",
    "    return {\"train_loss\": (train_metric, 0.0)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generated variables ====\n",
    "# # using sql database\n",
    "# sql_url = \"sqlite:///\"+f\"./{lightning_log_dir}/{exp_name}.sqlite\"\n",
    "\n",
    "# # If the database exists, load it and begin from there\n",
    "# loaded_db = False\n",
    "# if os.path.exists(sql_url.split('///')[-1]): # must cleave off the sql dialect prefix\n",
    "#     # alternate way to load an experiment (after `init_engine_and_session_factory` has been run)\n",
    "#     # experiment = load_experiment(exp_name) # if this doesn't work, check if the database is named something else and try that.\n",
    "#     db_settings = DBSettings(url=sql_url)\n",
    "#     # Instead of URL, can provide a `creator function`; can specify custom encoders/decoders if necessary.\n",
    "#     ax_client = AxClient(db_settings=db_settings)\n",
    "#     ax_client.load_experiment_from_database(exp_name)\n",
    "#     loaded_db = True\n",
    "\n",
    "# else:\n",
    "#     ax_client = AxClient()\n",
    "#     ax_client.create_experiment(\n",
    "#         name=exp_name,\n",
    "#         parameters=params_list,\n",
    "#         objectives={\"train_loss\": ObjectiveProperties(minimize=True)}\n",
    "#     )\n",
    "\n",
    "# run_trials_bool = True\n",
    "# if run_hyps_force == False:\n",
    "#     if loaded_db: \n",
    "#         # check if we've reached the max number of hyperparamters combinations to test\n",
    "#         if max_hyps <= (ax_client.generation_strategy.trials_as_df.index.max()+1):\n",
    "#             run_trials_bool = False\n",
    "\n",
    "# if run_trials_bool:\n",
    "#     # run the trials\n",
    "#     for i in range(run_hyps):\n",
    "#         parameterization, trial_index = ax_client.get_next_trial()\n",
    "#         # Local evaluation here can be replaced with deployment to external system.\n",
    "#         ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameterization))\n",
    "\n",
    "#     if loaded_db == False:\n",
    "#         init_engine_and_session_factory(url=sql_url)\n",
    "#         engine = get_engine()\n",
    "#         create_all_tables(engine)\n",
    "\n",
    "#     # save the trials\n",
    "#     experiment = ax_client.experiment\n",
    "#     save_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prep_dls(\n",
    "        y_var,\n",
    "        train_idx,\n",
    "        obs_geno_lookup,\n",
    "        vals,\n",
    "        batch_size,\n",
    "        test_idx):\n",
    "\n",
    "    phno = get_data('phno')\n",
    "    # center and y value data\n",
    "    assert 0 == phno.loc[:, y_var].isna().sum().sum() # second sum is for multiple y_vars\n",
    "\n",
    "    y = phno.loc[:, y_var].to_numpy() # added to make multiple ys work\n",
    "    # use train index to prevent information leakage\n",
    "    y_c = y[train_idx].mean(axis=0)\n",
    "    y_s = y[train_idx].std(axis=0)\n",
    "\n",
    "    y = (y - y_c)/y_s\n",
    "\n",
    "\n",
    "\n",
    "    training_dataloader = DataLoader(BigDataset(\n",
    "        lookups_are_filtered = False,\n",
    "        lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n",
    "        lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "        y =           torch.from_numpy(y).to(torch.float32),\n",
    "        G =           vals,\n",
    "        G_type = 'raw',\n",
    "        send_batch_to_gpu = 'cuda:0'\n",
    "        ),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True \n",
    "    )\n",
    "\n",
    "    validation_dataloader = DataLoader(BigDataset(\n",
    "        lookups_are_filtered = False,\n",
    "        lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n",
    "        lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "        y =           torch.from_numpy(y).to(torch.float32),\n",
    "        G =           vals,\n",
    "        G_type = 'raw',\n",
    "        send_batch_to_gpu = 'cuda:0'\n",
    "        ),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = False \n",
    "    )\n",
    "\n",
    "    return training_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ith_y_var in params_data['y_var'][0:2]:\n",
    "\n",
    "    y_var = ith_y_var\n",
    "    # update exp_name\n",
    "    exp_name = [e for e in cache_path.split('/') if e != ''][-1]\n",
    "    exp_name += '__'+y_var\n",
    "\n",
    "    print(''.join(['#' for i in range(80)]))\n",
    "    print(f'experiment: {exp_name}')\n",
    "    print(''.join(['#' for i in range(80)]))\n",
    "    print('\\n')\n",
    "\n",
    "    training_dataloader, validation_dataloader = _prep_dls(\n",
    "            y_var = y_var,\n",
    "            train_idx = train_idx,\n",
    "            obs_geno_lookup = obs_geno_lookup,\n",
    "            vals = vals,\n",
    "            batch_size = batch_size,\n",
    "            test_idx = test_idx)\n",
    "    \n",
    "\n",
    "    ## Generated variables ====\n",
    "    # using sql database\n",
    "    sql_url = \"sqlite:///\"+f\"./{lightning_log_dir}/{exp_name}.sqlite\"\n",
    "\n",
    "    # If the database exists, load it and begin from there\n",
    "    loaded_db = False\n",
    "    if os.path.exists(sql_url.split('///')[-1]): # must cleave off the sql dialect prefix\n",
    "        # alternate way to load an experiment (after `init_engine_and_session_factory` has been run)\n",
    "        # experiment = load_experiment(exp_name) # if this doesn't work, check if the database is named something else and try that.\n",
    "        db_settings = DBSettings(url=sql_url)\n",
    "        # Instead of URL, can provide a `creator function`; can specify custom encoders/decoders if necessary.\n",
    "        ax_client = AxClient(db_settings=db_settings)\n",
    "        ax_client.load_experiment_from_database(exp_name)\n",
    "        loaded_db = True\n",
    "\n",
    "    else:\n",
    "        ax_client = AxClient()\n",
    "        ax_client.create_experiment(\n",
    "            name=exp_name,\n",
    "            parameters=params_list,\n",
    "            objectives={\"train_loss\": ObjectiveProperties(minimize=True)}\n",
    "        )\n",
    "\n",
    "    run_trials_bool = True\n",
    "    if run_hyps_force == False:\n",
    "        if loaded_db: \n",
    "            # check if we've reached the max number of hyperparamters combinations to test\n",
    "            if max_hyps <= (ax_client.generation_strategy.trials_as_df.index.max()+1):\n",
    "                run_trials_bool = False\n",
    "\n",
    "    if run_trials_bool:\n",
    "        # run the trials\n",
    "        for i in range(run_hyps):\n",
    "            parameterization, trial_index = ax_client.get_next_trial()\n",
    "            # Local evaluation here can be replaced with deployment to external system.\n",
    "            ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameterization))\n",
    "\n",
    "        if loaded_db == False:\n",
    "            init_engine_and_session_factory(url=sql_url)\n",
    "            engine = get_engine()\n",
    "            create_all_tables(engine)\n",
    "\n",
    "        # save the trials\n",
    "        experiment = ax_client.experiment\n",
    "        save_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.generation_strategy.trials_as_df#.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render(ax_client.get_contour_plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(ax_client.get_optimization_trace(objective_optimum=0.0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I need to check what tables are in the sqlite\n",
    "# import sqlite3\n",
    "# con = sqlite3.connect(\"./foo.db\")\n",
    "# cur = con.cursor()\n",
    "# # cur.execute(\".tables;\") # should work, doesn't\n",
    "# cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "# con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
