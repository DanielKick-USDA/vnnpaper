{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zea mays functions\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp zma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from   torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import sparsevnn\n",
    "from   sparsevnn.core import\\\n",
    "    VNNHelper, \\\n",
    "    structured_layer_info, \\\n",
    "    SparseLinearCustom\n",
    "from   sparsevnn.kegg import \\\n",
    "    kegg_connections_build, \\\n",
    "    kegg_connections_clean, \\\n",
    "    kegg_connections_append_y_hat, \\\n",
    "    kegg_connections_sanitize_names\n",
    "\n",
    "\n",
    "import lightning.pytorch as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From EnvDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from EnvDL.dlfn import BigDataset, plDNN_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def np_3d_to_hilbert(\n",
    "    in_seq, # This should be a 3d numpy array with dimensions of [samples, sequence, channels] \n",
    "    **kwargs\n",
    "):\n",
    "    \"This is the 3d version of `np_2d_to_hilbert`. The goal is to process all of the samples of an array in one go.\"\n",
    "    import numpy as np\n",
    "    import tqdm\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    import hilbertcurve\n",
    "    from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "\n",
    "    import EnvDL\n",
    "    from EnvDL.dna import calc_needed_hilbert_p\n",
    "    \n",
    "    n_snps = in_seq.shape[1]\n",
    "    n_channels = in_seq.shape[-1]\n",
    "    temp = in_seq\n",
    "\n",
    "    p_needed = calc_needed_hilbert_p(n_needed=n_snps)\n",
    "    \n",
    "    # Data represented need not be continuous -- it need only have int positions\n",
    "    # a sequence or a sequence with gaps can be encoded\n",
    "    hilbert_curve = HilbertCurve(\n",
    "        p = p_needed, # iterations i.e. hold 4^p positions\n",
    "        n = 2    # dimensions\n",
    "        )\n",
    "\n",
    "    points = hilbert_curve.points_from_distances(range(n_snps))\n",
    "\n",
    "    dim_0 = np.max(np.array(points)[:, 0])+1 # add 1 to account for 0 indexing\n",
    "    dim_1 = np.max(np.array(points)[:, 1])+1\n",
    "    temp_mat = np.zeros(shape = [in_seq.shape[0], dim_0, dim_1, n_channels])\n",
    "    temp_mat[temp_mat == 0] = np.nan         #  empty values being used for visualization\n",
    "    \n",
    "    if \"silent\" in kwargs:\n",
    "        for i in range(n_snps):\n",
    "            temp_mat[:,                          # sample\n",
    "                     points[i][0], points[i][1], # x, y\n",
    "                     :] = temp[:, i]             # channels\n",
    "    else:\n",
    "        for i in tqdm(range(n_snps)):\n",
    "            temp_mat[:,                          # sample\n",
    "                     points[i][0], points[i][1], # x, y\n",
    "                     :] = temp[:, i]             # channels\n",
    "\n",
    "    return(temp_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BigDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lookup_obs,\n",
    "        lookups_are_filtered = False, # This is a critical piece of information. For deduplicated lookups to work they must either be filtered so that the \n",
    "                                      # lookup's length matchs y's length (idx will map to the right row) _or_ instead lookup_obs must contain a mapping \n",
    "                                      # from the y's current length to the row in the full dataset. \n",
    "                                      # If False idx -> lookup_obs[idx] -> lookup_env[idx] -> W[idx]\n",
    "                                      # If True  idx ------------------ -> lookup_env[idx] -> W[idx]\n",
    "#         lookup_geno,\n",
    "#         lookup_env,\n",
    "#         y,\n",
    "#         G, \n",
    "#         G_type\n",
    "#         S,\n",
    "#         P,\n",
    "#         W,\n",
    "#         W_type,\n",
    "#         send_batch_to_gpu # 'cuda:0' but '0' and 'cuda0' would also work\n",
    "        transform = None, \n",
    "        target_transform = None,\n",
    "        **kwargs \n",
    "        ):\n",
    "        \"\"\"\n",
    "        This class produces a set with one or more input tensors. For flexibility the only _required_ input is `lookup_obs`, a tensor with the index of observations. \n",
    "        Everything else is provided as a kwarg. Output is a list of tensors1 ordered [y, G, S, W], any of these not initalized will be missing but not empty (e.g. [y, S, W] not [y, None, S, W]).       \n",
    "        Used inputs are:\n",
    "        lookup_obs: index for y, used by __getitem__ for obs_idx\n",
    "        lookup_geno: index for G, row obs_idx, column 1 is geno_idx (geno information is deduplicated, hence the need for a lookup)\n",
    "        lookup_env: index for S & W, , row obs_idx, column 1 is env_idx (env information is deduplicated, hence the need for a lookup)\n",
    "        y: yield\n",
    "        G: Genomic information \n",
    "        G_type: how the infomation should be returned, 'raw', 'hilbert', or 'list' (i.e. of tensors for snps in each gene)\n",
    "        S: Soil information\n",
    "        P: Planting/Harvest date contained in column 0, 1 respectively \n",
    "        W: Weather data\n",
    "        W_type: how the infomation should be returned, 'raw' or 'hilbert'\n",
    "\n",
    "        1 G may also be returned as a list of tensors\n",
    "        \"\"\"\n",
    "        # Lookup info (so that deduplication works)\n",
    "        self.lookup_obs = lookup_obs\n",
    "        self.lookups_are_filtered = lookups_are_filtered\n",
    "        # if 'lookup_obs'  in kwargs: self.lookup_obs  = kwargs['lookup_obs'];\n",
    "        if 'lookup_geno' in kwargs: self.lookup_geno = kwargs['lookup_geno'];\n",
    "        if 'lookup_env'  in kwargs: self.lookup_env  = kwargs['lookup_env'];\n",
    "        # Data\n",
    "        if 'y' in kwargs: self.y = kwargs['y'];\n",
    "        if 'G' in kwargs: self.G = kwargs['G'];\n",
    "        if 'S' in kwargs: self.S = kwargs['S'];\n",
    "        if 'P' in kwargs: self.P = kwargs['P']; # PlantHarvest so that planting can be added into W\n",
    "        if 'W' in kwargs: self.W = kwargs['W'];\n",
    "        # Data prep state information\n",
    "        if 'G_type' in kwargs: self.G_type = kwargs['G_type']; # raw, hilbert, list\n",
    "        if 'W_type' in kwargs: self.W_type = kwargs['W_type']; # raw, hilbert\n",
    "        # Data to be returned\n",
    "        self.out_names = [e for e in ['y', 'G', 'S', 'W'] if e in kwargs]\n",
    "\n",
    "        # Is data starting on the desired device or does it need to be cycled on and off?\n",
    "        # This is based on ACGTDataset\n",
    "        self.send_batch_to_gpu = None\n",
    "        if 'send_batch_to_gpu' in kwargs.keys():\n",
    "            send_batch_to_gpu = kwargs['send_batch_to_gpu']\n",
    "            if type(send_batch_to_gpu) == str: \n",
    "                send_batch_to_gpu = send_batch_to_gpu.lower()\n",
    "                if len(send_batch_to_gpu) >= 1:\n",
    "                    # remove characters in 'cuda:'\n",
    "                    send_batch_to_gpu = ''.join([e for e in send_batch_to_gpu if e not in ['c', 'u', 'd', 'a', ':']])\n",
    "                self.send_batch_to_gpu = int(send_batch_to_gpu)\n",
    "            elif type(send_batch_to_gpu) == int: \n",
    "                self.send_batch_to_gpu = send_batch_to_gpu\n",
    "            else:\n",
    "                print('send_batch_to_gpu kwarg ignored. Must be a string or int. Ideally of form \"cuda:0\"')\n",
    "\n",
    "        # Transformations\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lookup_obs)\n",
    "    \n",
    "\n",
    "    # These used to be in __getitem__ but separating them like this allows for them to be overwritten more easily\n",
    "    def get_y(self, idx):\n",
    "        y_idx = self.y[idx]\n",
    "        if self.transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "        return(y_idx)\n",
    "        \n",
    "    def get_G(self, idx):\n",
    "        geno_idx = self.lookup_geno[idx, 1]\n",
    "        if self.G_type in ['raw', 'hilbert']:\n",
    "            G_idx = self.G[geno_idx]\n",
    "        if 'list' == self.G_type:\n",
    "            G_idx = [e[geno_idx] for e in self.G]\n",
    "        if self.transform:\n",
    "            G_idx = self.transform(G_idx)\n",
    "        return(G_idx)\n",
    "\n",
    "    def get_S(self, idx):\n",
    "        env_idx = self.lookup_env[idx, 1]\n",
    "        S_idx = self.S[env_idx]\n",
    "        if self.transform:\n",
    "            S_idx = self.transform(S_idx)\n",
    "        return(S_idx)\n",
    "\n",
    "    def get_W(self, idx):\n",
    "        W_device = torch.Tensor(self.W).get_device()\n",
    "\n",
    "        env_idx = self.lookup_env[idx, 1]\n",
    "        # get growing information\n",
    "        WPlant = np.zeros(365)\n",
    "        # WPlant[self.P[obs_idx, 0]:self.P[obs_idx, 1]] = 1\n",
    "        WPlant[self.P[idx, 0]:self.P[idx, 1]] = 1\n",
    "        if self.W_type == 'raw':\n",
    "            WPlant = torch.from_numpy(WPlant).to(torch.float)\n",
    "            # if needed send to gpu\n",
    "            if W_device != -1: WPlant = WPlant.to(W_device)            \n",
    "            W_idx = torch.concatenate([self.W[env_idx], WPlant[None, :]], axis = 0)\n",
    "        if self.W_type == 'hilbert':\n",
    "            # convert growing info to hilbert curve\n",
    "            WPlant_hilb = np_3d_to_hilbert(WPlant[None, :, None], silent = True)\n",
    "            WPlant_hilb = WPlant_hilb.squeeze(axis = 3)\n",
    "            WPlant_hilb[np.isnan(WPlant_hilb)] = 0\n",
    "            WPlant_hilb = torch.from_numpy(WPlant_hilb).to(torch.float)\n",
    "            # if needed send to gpu\n",
    "            if W_device != -1: WPlant_hilb = WPlant_hilb.to(W_device)\n",
    "            W_idx = torch.concatenate([self.W[env_idx], WPlant_hilb], axis = 0)\n",
    "        if self.transform:\n",
    "            W_idx = self.transform(W_idx)\n",
    "        return(W_idx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        out = []\n",
    "        obs_idx = idx\n",
    "        if self.lookups_are_filtered == False:\n",
    "            obs_idx = self.lookup_obs[idx]\n",
    "\n",
    "        if 'y' in self.out_names: out += [self.get_y(obs_idx)]\n",
    "        if 'G' in self.out_names: out += [self.get_G(obs_idx)]\n",
    "        if 'S' in self.out_names: out += [self.get_S(obs_idx)]\n",
    "        if 'W' in self.out_names: out += [self.get_W(obs_idx)]\n",
    "\n",
    "        # send all to gpu    \n",
    "        if self.send_batch_to_gpu is not None:\n",
    "            out = [[ee.to(self.send_batch_to_gpu) for ee in e] if type(e)==list else e.to(self.send_batch_to_gpu) for e in out]     \n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class plDNN_general(pl.LightningModule):\n",
    "    def __init__(self, mod, log_weight_stats = False):\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "        self.log_weight_stats = log_weight_stats\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_i, x_i = batch\n",
    "        pred = self.mod(x_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        if self.log_weight_stats:\n",
    "            with torch.no_grad():\n",
    "                weight_list=[(name, param) for name, param in self.mod.named_parameters() if name.split('.')[-1] == 'weight']\n",
    "                for l in weight_list:\n",
    "                    self.log((\"train_mean\"+l[0]), l[1].mean())\n",
    "                    self.log((\"train_std\"+l[0]), l[1].std())        \n",
    "\n",
    "        return(loss)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_i, x_i = batch\n",
    "        pred = self.mod(x_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log('val_loss', loss)        \n",
    "     \n",
    "    def configure_optimizers(self, **kwargs):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "        return optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mask_columns(df, # A dataframe containing the column to use for the mask\n",
    "                col_name = 'Hybrid', # Column containing the values in `holdouts`\n",
    "                holdouts = ['M0143/LH185', 'M0003/LH185'] # A list of values to match\n",
    "                ):\n",
    "    \"\"\"Create a dataframe containing one mask or more mask for a list of `holdouts`.\"\"\"\n",
    "    out = [pd.DataFrame(df.loc[:, col_name] == holdout\n",
    "            ).rename(columns = {col_name:holdout})\n",
    "            for holdout in holdouts]\n",
    "    \n",
    "    out = pd.concat(out, axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mask_parents(\n",
    "        df, # Dataframe containing a column with a genotype\n",
    "        col_name = 'Hybrid', # The genotype column name\n",
    "        holdout_parents = ['M0143'], # The genotype or genotypes that will be held out\n",
    "        sep = '/' # Separator between parents. If not present (inbred genotype) that's okay.\n",
    "    ):\n",
    "    \"\"\"Create a dataframe containing one mask or more based on a parent's genotype\"\"\"\n",
    "    def  _mask_parent(df_FM, holdout = 'PHZ51'):\n",
    "        holdout=   holdout.upper()\n",
    "        mask_F = df_FM.F.str.upper() == holdout\n",
    "        mask_M = df_FM.M.str.upper() == holdout\n",
    "        mask = (mask_F | mask_M)\n",
    "        return mask\n",
    "\n",
    "    df[['F', 'M']] = df[col_name].str.split(sep, n=1, expand=True)\n",
    "    mask = pd.concat([_mask_parent(df_FM=df, holdout=e) for e in holdout_parents], axis=1\n",
    "            ).rename(columns={i:holdout_parents[i] for i in range(len(holdout_parents))})\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vnnpaper specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _dist_scale_function(out, dist, decay_rate):\n",
    "    scale = 1/(1+decay_rate*dist)\n",
    "    out = round(scale * out)\n",
    "    out = max(1, out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _expand_node_shortcut(vnn_helper, query = 'y_hat'):\n",
    "    # define new entries\n",
    "    if True in [True if e in vnn_helper.edge_dict.keys() else False for e in \n",
    "                [f'{query}_res_-2', f'{query}_res_-1']\n",
    "                ]:\n",
    "        print('Warning! New node name already exists! Overwriting existing node!')\n",
    "\n",
    "    # Add residual connection in graph\n",
    "    vnn_helper.edge_dict[f'{query}_res_-2'] = vnn_helper.edge_dict[query] \n",
    "    vnn_helper.edge_dict[f'{query}_res_-1'] = [f'{query}_res_-2']\n",
    "    vnn_helper.edge_dict[query]             = [f'{query}_res_-2', f'{query}_res_-1']\n",
    "\n",
    "    # Add new nodes, copying information from query node\n",
    "    vnn_helper.node_props[f'{query}_res_-2'] = vnn_helper.node_props[query] \n",
    "    vnn_helper.node_props[f'{query}_res_-1'] = vnn_helper.node_props[query]\n",
    "\n",
    "    return vnn_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def vnn_factory_1(parsed_kegg_gene_entries, params, ACGT_gene_slice_list):\n",
    "\n",
    "    print(''.join('#' for i in range(80)))\n",
    "    print(params)\n",
    "    print(''.join('#' for i in range(80)))\n",
    "    \n",
    "    \n",
    "    default_out_nodes_inp  = params['default_out_nodes_inp' ]\n",
    "    default_out_nodes_edge = params['default_out_nodes_edge'] \n",
    "    default_out_nodes_out  = params['default_out_nodes_out' ]\n",
    "\n",
    "    default_drop_nodes_inp = params['default_drop_nodes_inp' ] \n",
    "    default_drop_nodes_edge= params['default_drop_nodes_edge'] \n",
    "    default_drop_nodes_out = params['default_drop_nodes_out' ] \n",
    "\n",
    "    default_reps_nodes_inp = params['default_reps_nodes_inp' ]\n",
    "    default_reps_nodes_edge= params['default_reps_nodes_edge']\n",
    "    default_reps_nodes_out = params['default_reps_nodes_out' ]\n",
    "\n",
    "\n",
    "\n",
    "    default_decay_rate = params['default_decay_rate' ]\n",
    "\n",
    "\n",
    "\n",
    "    # Clean up KEGG Pathways -------------------------------------------------------\n",
    "    # Same setup as above to create kegg_gene_brite\n",
    "    # Restrict to only those with pathway\n",
    "    kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n",
    "\n",
    "    # also require to have a non-empty path\n",
    "    kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n",
    "\n",
    "    print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n",
    "        )+'/'+str(len(parsed_kegg_gene_entries)\n",
    "        )+' Entries'\n",
    "        )\n",
    "    # kegg_gene_brite[1]['BRITE']['BRITE_PATHS']\n",
    "\n",
    "\n",
    "    kegg_connections = kegg_connections_build(kegg_gene_brite = kegg_gene_brite, \n",
    "                                            n_genes = len(kegg_gene_brite)) \n",
    "    kegg_connections = kegg_connections_clean(         kegg_connections = kegg_connections)\n",
    "    #TODO think about removing \n",
    "    # \"Not Included In\n",
    "    # Pathway Or Brite\"\n",
    "    # or reinstate 'Others'\n",
    "\n",
    "    kegg_connections = kegg_connections_append_y_hat(  kegg_connections = kegg_connections)\n",
    "    kegg_connections = kegg_connections_sanitize_names(kegg_connections = kegg_connections, \n",
    "                                                    replace_chars = {'.':'_'})\n",
    "\n",
    "\n",
    "    # Initialize helper for input nodes --------------------------------------------\n",
    "    myvnn = VNNHelper(edge_dict = kegg_connections)\n",
    "\n",
    "    # Get a mapping of brite names to tensor list index\n",
    "    find_names = myvnn.nodes_inp # e.g. ['100383860', '100278565', ... ]\n",
    "    lookup_dict = {}\n",
    "\n",
    "    # the only difference lookup_dict and brite_node_to_list_idx_dict above is that this is made using the full set of genes in the list \n",
    "    # whereas that is made using kegg_gene_brite which is a subset\n",
    "    for i in range(len(parsed_kegg_gene_entries)):\n",
    "        if 'BRITE' not in parsed_kegg_gene_entries[i].keys():\n",
    "            pass\n",
    "        elif parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'] == []:\n",
    "            pass\n",
    "        else:\n",
    "            name = parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'][0][-1]\n",
    "            if name in find_names:\n",
    "                lookup_dict[name] = i\n",
    "    # lookup_dict    \n",
    "\n",
    "    brite_node_to_list_idx_dict = {}\n",
    "    for i in range(len(kegg_gene_brite)):\n",
    "        brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i        \n",
    "\n",
    "    # Get the input sizes for the graph\n",
    "    size_in_zip = zip(myvnn.nodes_inp, [np.prod(ACGT_gene_slice_list[lookup_dict[e]].shape[1:]) for e  in myvnn.nodes_inp])\n",
    "\n",
    "    # Set node defaults ------------------------------------------------------------\n",
    "    # init input node sizes\n",
    "    myvnn.set_node_props(key = 'inp', node_val_zip = size_in_zip)\n",
    "\n",
    "    # init node output sizes\n",
    "    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_inp, [default_out_nodes_inp  for e in myvnn.nodes_inp]))\n",
    "    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_edge,[default_out_nodes_edge for e in myvnn.nodes_edge]))\n",
    "    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_out, [default_out_nodes_out  for e in myvnn.nodes_out]))\n",
    "\n",
    "    # # options should be controlled by node_props\n",
    "    myvnn.set_node_props(key = 'flatten', node_val_zip = zip(myvnn.nodes_inp, [True for e in myvnn.nodes_inp]))\n",
    "\n",
    "    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_inp, [default_reps_nodes_inp  for e in myvnn.nodes_inp]))\n",
    "    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_edge,[default_reps_nodes_edge for e in myvnn.nodes_edge]))\n",
    "    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_out, [default_reps_nodes_out  for e in myvnn.nodes_out]))\n",
    "\n",
    "    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_inp, [default_drop_nodes_inp  for e in myvnn.nodes_inp]))\n",
    "    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_edge,[default_drop_nodes_edge for e in myvnn.nodes_edge]))\n",
    "    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_out, [default_drop_nodes_out  for e in myvnn.nodes_out]))\n",
    "\n",
    "\n",
    "    # Scale node outputs by distance -----------------------------------------------\n",
    "    dist = sparsevnn.core.vertex_from_end(\n",
    "        edge_dict = myvnn.edge_dict,\n",
    "        end =myvnn.dependancy_order[-1]\n",
    "    )\n",
    "\n",
    "    # overwrite node outputs with a size inversely proportional to distance from prediction node\n",
    "    for query in list(dist.keys()):\n",
    "        myvnn.node_props[query]['out'] = _dist_scale_function(\n",
    "            out = myvnn.node_props[query]['out'],\n",
    "            dist = dist[query],\n",
    "            decay_rate = default_decay_rate)\n",
    "        \n",
    "\n",
    "    # Expand out node replicates ---------------------------------------------------\n",
    "    nodes = [node for node in myvnn.dependancy_order if myvnn.node_props[node]['reps'] > 1]\n",
    "\n",
    "    node_expansion_dict = {\n",
    "        node: [node if i==0 else f'{node}_{i}' for i in range(myvnn.node_props[node]['reps'])]\n",
    "        for node in nodes}\n",
    "    #   current       1st          2nd (new)      3rd (new)\n",
    "    # {'100798274': ['100798274', '100798274_1', '100798274_2'], ...\n",
    "\n",
    "    # the keys don't change here. The values will be updated and then new k:v will be inserted\n",
    "    myvnn.edge_dict = {k:[e if e not in node_expansion_dict.keys() \n",
    "        else node_expansion_dict[e][-1]\n",
    "        for e in myvnn.edge_dict[k] ] for k in myvnn.edge_dict}\n",
    "\n",
    "    # now insert connectsion to new nodes: A -> A_rep_1 -> A_rep_2\n",
    "    for node in node_expansion_dict:\n",
    "        for pair in zip(node_expansion_dict[node][1:], node_expansion_dict[node]):\n",
    "            myvnn.edge_dict[pair[0]] = [pair[1]]\n",
    "\n",
    "    # now add those new nodes\n",
    "    # create a new node for all the nodes\n",
    "    for node in node_expansion_dict:\n",
    "        for new_node in node_expansion_dict[node][1:]:\n",
    "            myvnn.node_props[new_node] = {k:myvnn.node_props[node][k] for k in myvnn.node_props[node] if k != 'inp'}\n",
    "\n",
    "    new_vnn = VNNHelper(edge_dict= myvnn.edge_dict)\n",
    "    new_vnn.node_props = myvnn.node_props\n",
    "    myvnn = new_vnn\n",
    "\n",
    "\n",
    "    # init edge node input size (propagate forward input/edge outpus)\n",
    "    myvnn.calc_edge_inp()\n",
    "\n",
    "    # replace lookup so that it matches the lenght of the input tensors\n",
    "    new_lookup_dict = {}\n",
    "    for i in range(len(myvnn.nodes_inp)):\n",
    "        new_lookup_dict[myvnn.nodes_inp[i]] = i\n",
    "    \n",
    "    return myvnn,  lookup_dict #new_lookup_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def vnn_factory_2(vnn_helper, node_to_inp_num_dict):\n",
    "    myvnn = vnn_helper\n",
    "\n",
    "    node_props = myvnn.node_props\n",
    "    # Linear_block = Linear_block_reps,\n",
    "    edge_dict = myvnn.edge_dict\n",
    "    dependancy_order = myvnn.dependancy_order\n",
    "    # node_to_inp_num_dict = new_lookup_dict\n",
    "\n",
    "    # Build dependancy dictionary --------------------------------------------------\n",
    "    # check dep order\n",
    "    tally = []\n",
    "    for d in dependancy_order:\n",
    "        if edge_dict[d] == []:\n",
    "            tally.append(d)\n",
    "        elif False not in [True if e in tally else False for e in edge_dict[d]]:\n",
    "            tally.append(d)\n",
    "        else:\n",
    "            print('error!')\n",
    "            break\n",
    "\n",
    "\n",
    "    # build output nodes \n",
    "    d_out = {0:[]}\n",
    "    for d in dependancy_order:\n",
    "        if edge_dict[d] == []:\n",
    "            d_out[min(d_out.keys())].append(d)\n",
    "        else:\n",
    "            # print((d, edge_dict[d]))\n",
    "\n",
    "            d_out_i = 1+max(sum([[key for key in d_out.keys() if e in d_out[key]]\n",
    "                    for e in edge_dict[d]], []))\n",
    "            \n",
    "            if d_out_i not in d_out.keys():\n",
    "                d_out[d_out_i] = []\n",
    "            d_out[d_out_i].append(d)\n",
    "\n",
    "\n",
    "    # build input nodes NOPE. THE PASSHTROUGHS! \n",
    "    d_eye = {}\n",
    "    tally = []\n",
    "    for i in range(max(d_out.keys()), min(d_out.keys()), -1):\n",
    "        # print(i)\n",
    "        nodes_needed = sum([edge_dict[e] for e in d_out[i]], [])+tally\n",
    "        # check against what is there and then dedupe\n",
    "        nodes_needed = [e for e in nodes_needed if e not in d_out[i-1]]\n",
    "        nodes_needed = list(set(nodes_needed))\n",
    "        tally = nodes_needed\n",
    "        d_eye[i] = nodes_needed\n",
    "\n",
    "    # d_inp[0]= d_out[0]\n",
    "    # [len(d_eye[i]) for i in d_eye.keys()]\n",
    "    # [(key, len(d_out[key])) for key in d_out.keys()]\n",
    "\n",
    "\n",
    "    dd = {}\n",
    "    for i in d_eye.keys():\n",
    "        dd[i] = {'out': d_out[i],\n",
    "                'inp': d_out[i-1],\n",
    "                'eye': d_eye[i]}\n",
    "    # plus special 0 layer that handles the snps\n",
    "        \n",
    "    dd[0] = {'out': d_out[0],\n",
    "            'inp': d_out[0],\n",
    "            'eye': []}\n",
    "\n",
    "\n",
    "    # check that the output nodes' inputs are satisfied by the same layer's inputs (inp and eye)\n",
    "    for i in dd.keys():\n",
    "        # out node in each\n",
    "        for e in dd[i]['out']:\n",
    "            # node depends in inp/eye\n",
    "            node_pass_list = [True if ee in dd[i]['inp']+dd[i]['eye'] else False \n",
    "                            for ee in edge_dict[e]]\n",
    "            if False not in node_pass_list:\n",
    "                pass\n",
    "            else:\n",
    "                print('exit') \n",
    "\n",
    "\n",
    "    # print(\"Layer\\t#In\\t#Out\")\n",
    "    # for i in range(min(dd.keys()), max(dd.keys())+1, 1):\n",
    "    #     node_in      = [node_props[e]['out'] for e in dd[i]['inp']+dd[i  ]['eye'] ]\n",
    "    #     if i == max(dd.keys()):\n",
    "    #         node_out = [node_props[e]['out'] for e in dd[i]['out'] ]\n",
    "    #     else:\n",
    "    #         node_out = [node_props[e]['out'] for e in dd[i]['out']+dd[i+1]['eye']]\n",
    "    #     print(f'{i}:\\t{sum(node_in)}\\t{sum(node_out)}')\n",
    "\n",
    "    M_list = [structured_layer_info(i = ii, node_groups = dd, node_props= node_props, edge_dict = edge_dict, as_sparse=True) for ii in range(0, max(dd.keys())+1)]\n",
    "    return M_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def vnn_factory_3(M_list):\n",
    "    layer_list = []\n",
    "    for i in range(len(M_list)):\n",
    "        \n",
    "        apply_relu = None\n",
    "        if i+1 != len(M_list): # apply relu to all but the last layer\n",
    "            apply_relu = F.relu\n",
    "        \n",
    "\n",
    "        l = SparseLinearCustom(\n",
    "            M_list[i].weight.shape[1], # have to transpose this?\n",
    "            M_list[i].weight.shape[0],\n",
    "            connectivity   = torch.LongTensor(M_list[i].weight.coalesce().indices()),\n",
    "            custom_weights = M_list[i].weight.coalesce().values(), \n",
    "            custom_bias    = M_list[i].bias.clone().detach(), \n",
    "            weight_grad_bool = M_list[i].weight_grad_bool, \n",
    "            bias_grad_bool   = M_list[i].bias_grad_bool, #.to_sparse()#.indices()\n",
    "            dropout_p        = M_list[i].dropout_p,\n",
    "            nonlinear_transform= apply_relu\n",
    "            )\n",
    "\n",
    "        layer_list += [l]\n",
    "        \n",
    "    return layer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| hide\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnbdev\u001b[39;00m; nbdev\u001b[38;5;241m.\u001b[39mnbdev_export()\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai/lib/python3.11/site-packages/fastcore/script.py:110\u001b[0m, in \u001b[0;36mcall_parse.<locals>._f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    109\u001b[0m     mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(inspect\u001b[38;5;241m.\u001b[39mcurrentframe()\u001b[38;5;241m.\u001b[39mf_back)\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mod: \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SCRIPT_INFO\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;129;01mand\u001b[39;00m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m: SCRIPT_INFO\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sys\u001b[38;5;241m.\u001b[39margv)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m: sys\u001b[38;5;241m.\u001b[39margv\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai/lib/python3.11/site-packages/nbdev/doclinks.py:144\u001b[0m, in \u001b[0;36mnbdev_export\u001b[0;34m(path, procs, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files: nb_export(f, procs\u001b[38;5;241m=\u001b[39mprocs)\n\u001b[1;32m    143\u001b[0m add_init(get_config()\u001b[38;5;241m.\u001b[39mlib_path)\n\u001b[0;32m--> 144\u001b[0m _build_modidx()\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai/lib/python3.11/site-packages/nbdev/doclinks.py:102\u001b[0m, in \u001b[0;36m_build_modidx\u001b[0;34m(dest, nbs_path, skip_exists)\u001b[0m\n\u001b[1;32m    100\u001b[0m code_root \u001b[38;5;241m=\u001b[39m dest\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m globtastic(dest, file_glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.py\u001b[39m\u001b[38;5;124m\"\u001b[39m, skip_file_re\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^_\u001b[39m\u001b[38;5;124m'\u001b[39m, skip_folder_re\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.ipynb_checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 102\u001b[0m     res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msyms\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(_get_modidx((dest\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m/\u001b[39mfile)\u001b[38;5;241m.\u001b[39mresolve(), code_root, nbs_path\u001b[38;5;241m=\u001b[39mnbs_path))\n\u001b[1;32m    103\u001b[0m idxfile\u001b[38;5;241m.\u001b[39mwrite_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Autogenerated by nbdev\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124md = \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mpformat(res, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m140\u001b[39m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, compact\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai/lib/python3.11/site-packages/nbdev/doclinks.py:73\u001b[0m, in \u001b[0;36m_get_modidx\u001b[0;34m(py_path, code_root, nbs_path)\u001b[0m\n\u001b[1;32m     71\u001b[0m _def_types \u001b[38;5;241m=\u001b[39m ast\u001b[38;5;241m.\u001b[39mFunctionDef,ast\u001b[38;5;241m.\u001b[39mAsyncFunctionDef,ast\u001b[38;5;241m.\u001b[39mClassDef\n\u001b[1;32m     72\u001b[0m d \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m _iter_py_cells(py_path):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cell\u001b[38;5;241m.\u001b[39mnb \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     loc \u001b[38;5;241m=\u001b[39m _nbpath2html(cell\u001b[38;5;241m.\u001b[39mnb_path\u001b[38;5;241m.\u001b[39mrelative_to(nbs_path))\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai/lib/python3.11/site-packages/nbdev/doclinks.py:51\u001b[0m, in \u001b[0;36m_iter_py_cells\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     49\u001b[0m cells \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mread_text(encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m cells[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m---> 51\u001b[0m     top,code \u001b[38;5;241m=\u001b[39m cell\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m       \u001b[38;5;241m*\u001b[39mnb,idx \u001b[38;5;241m=\u001b[39m top\u001b[38;5;241m.\u001b[39msplit()\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
