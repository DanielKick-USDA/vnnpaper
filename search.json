[
  {
    "objectID": "zma_g2f_eres_01.html",
    "href": "zma_g2f_eres_01.html",
    "title": "Visible Neural Network",
    "section": "",
    "text": "# Data ----\nfrom dataG2F.core import get_data\nfrom dataG2F.qol  import ensure_dir_path_exists\n\n# Data Utilities ----\nimport numpy  as np\nimport pandas as pd\n\nfrom EnvDL.dlfn import BigDataset, plDNN_general\nfrom EnvDL.sets import mask_parents\n\n# Model Building  ----\n## General ====\nimport torch\nfrom   torch import nn\nimport torch.nn.functional as F\nfrom   torch.utils.data import Dataset\nfrom   torch.utils.data import DataLoader\n\n## VNN ====\nimport sparsevnn\nfrom   sparsevnn.core import\\\n    VNNHelper, \\\n    structured_layer_info, \\\n    SparseLinearCustom\nfrom   sparsevnn.kegg import \\\n    kegg_connections_build, \\\n    kegg_connections_clean, \\\n    kegg_connections_append_y_hat, \\\n    kegg_connections_sanitize_names\n\n# Hyperparameter Tuning ----\nimport os # needed for checking history (saved by lightning) \n\n## Logging with Pytorch Lightning ====\nimport lightning.pytorch as pl\nfrom   lightning.pytorch.loggers import CSVLogger # used to save the history of each trial (used by ax)\n\n## Adaptive Experimentation Platform ====\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\nfrom ax.utils.notebook.plotting import init_notebook_plotting, render\n\n# For logging experiment results in sql database\nfrom ax.storage.sqa_store.db import init_engine_and_session_factory\nfrom ax.storage.sqa_store.db import get_engine, create_all_tables\nfrom ax.storage.sqa_store.save import save_experiment # saving\nfrom ax.storage.sqa_store.structs import DBSettings # loading\n# from ax.storage.sqa_store.load import load_experiment # loading alternate\ntorch.set_float32_matmul_precision('medium')\ninit_notebook_plotting()",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_eres_01.html#setup",
    "href": "zma_g2f_eres_01.html#setup",
    "title": "Visible Neural Network",
    "section": "Setup",
    "text": "Setup\n\ncache_path = '../nbs_artifacts/zma_g2f_eres_01/'\n\n\n# Run settings: \nparams_run = {\n    'batch_size': 256,\n    'max_epoch' : 256,    \n}\n\n# data settings\nparams_data = {\n    'y_var': 'Yield_Mg_ha',\n    'y_resid': 'Env', # None, Env, Geno\n    'y_resid_strat': 'None', # None, naive_mean, filter_mean, ...\n    'holdout_parents': [\n        ## 2022 ====\n        'LH244',\n        ## 2021 ====\n        'PHZ51',\n        # 'PHP02',\n        # 'PHK76',\n        ## 2019 ====\n        # 'PHT69',\n        'LH195',\n        ## 2017 ====\n        # 'PHW52',\n        # 'PHN82',\n        ## 2016 ====\n        # 'DK3IIH6',\n        ## 2015 ====\n        # 'PHB47',\n        # 'LH82',\n        ## 2014 ====\n        # 'LH198',\n        # 'LH185',\n        # 'PB80',\n        # 'CG102',\n ],    \n}\n\n\n## Settings ====\nrun_hyps = 75 \nrun_hyps_force = False # should we run more trials even if the target number has been reached?\nmax_hyps = 100\n\nparams_list = [    \n    ## Output Size ====\n    {\n    'name': 'default_out_nodes_inp',\n    'type': 'range',\n    'bounds': [1, 8],\n    'value_type': 'int',\n    'log_scale': False\n    },\n    {\n    'name': 'default_out_nodes_edge',\n    'type': 'range',\n    'bounds': [1, 32],\n    'value_type': 'int',\n    'log_scale': False\n    },\n    {\n    'name': 'default_out_nodes_out',\n    'type': 'fixed',\n    'value': 1,\n    'value_type': 'int',\n    'log_scale': False\n    },\n    ## Dropout ====\n    {\n    'name': 'default_drop_nodes_inp',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False\n    },\n    {\n    'name': 'default_drop_nodes_edge',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False\n    },\n    {\n    'name': 'default_drop_nodes_out',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False,\n    'sort_values':True\n    },\n    ## Node Repeats ====\n    {\n    'name': 'default_reps_nodes_inp',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    {\n    'name': 'default_reps_nodes_edge',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    {\n    'name': 'default_reps_nodes_out',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    ## Node Output Size Scaling ====\n    {\n    'name': 'default_decay_rate',\n    'type': 'choice',\n    'values': [0+(0.1*i) for i in range(10)]+[1.+(1*i) for i in range(11)],\n    'value_type': 'float',\n    'is_ordered': True,\n    'sort_values':True\n    }\n    ]\n\n\nlightning_log_dir = cache_path+\"lightning\"\nexp_name = [e for e in cache_path.split('/') if e != ''][-1]\n\n\n# parameterization is needed for setup. These values will be overwritten by Ax if tuning is occuring. \n# in this file I define params later. I've included it here to gurantee that we can merge other params dicts into it.\nparams = {\n'default_out_nodes_inp'  : 1,\n'default_out_nodes_edge' : 1,\n'default_out_nodes_out'  : 1,\n\n'default_drop_nodes_inp' : 0.0,\n'default_drop_nodes_edge': 0.0,\n'default_drop_nodes_out' : 0.0,\n\n'default_reps_nodes_inp' : 1,\n'default_reps_nodes_edge': 1,\n'default_reps_nodes_out' : 1,\n\n'default_decay_rate'     : 1\n}\n\ndefault_out_nodes_inp  = params['default_out_nodes_inp' ]\ndefault_out_nodes_edge = params['default_out_nodes_edge'] \ndefault_out_nodes_out  = params['default_out_nodes_out' ]\n\ndefault_drop_nodes_inp = params['default_drop_nodes_inp' ] \ndefault_drop_nodes_edge= params['default_drop_nodes_edge'] \ndefault_drop_nodes_out = params['default_drop_nodes_out' ] \n\ndefault_reps_nodes_inp = params['default_reps_nodes_inp' ]\ndefault_reps_nodes_edge= params['default_reps_nodes_edge']\ndefault_reps_nodes_out = params['default_reps_nodes_out' ]\n\ndefault_decay_rate = params['default_decay_rate' ]\n\n\nbatch_size = params_run['batch_size']\nmax_epoch  = params_run['max_epoch']\n\ny_var = params_data['y_var']\n\n\nsave_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n\nif 'None' != params_data['y_resid_strat']:\n    save_prefix = save_prefix+'_'+params_data['y_resid_strat']\n\nensure_dir_path_exists(dir_path = cache_path)\n\n\nuse_gpu_num = 0\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif use_gpu_num in [0, 1]: \n    torch.cuda.set_device(use_gpu_num)\nprint(f\"Using {device} device\")\n\n\ndef _dist_scale_function(out, dist, decay_rate):\n    scale = 1/(1+decay_rate*dist)\n    out = round(scale * out)\n    out = max(1, out)\n    return out\n\n\ndef _expand_node_shortcut(vnn_helper, query = 'y_hat'):\n    # define new entries\n    if True in [True if e in vnn_helper.edge_dict.keys() else False for e in \n                [f'{query}_res_-2', f'{query}_res_-1']\n                ]:\n        print('Warning! New node name already exists! Overwriting existing node!')\n\n    # Add residual connection in graph\n    vnn_helper.edge_dict[f'{query}_res_-2'] = myvnn.edge_dict[query] \n    vnn_helper.edge_dict[f'{query}_res_-1'] = [f'{query}_res_-2']\n    vnn_helper.edge_dict[query]             = [f'{query}_res_-2', f'{query}_res_-1']\n\n    # Add new nodes, copying information from query node\n    vnn_helper.node_props[f'{query}_res_-2'] = vnn_helper.node_props[query] \n    vnn_helper.node_props[f'{query}_res_-1'] = vnn_helper.node_props[query]\n\n    return vnn_helper",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_eres_01.html#load-data",
    "href": "zma_g2f_eres_01.html#load-data",
    "title": "Visible Neural Network",
    "section": "Load Data",
    "text": "Load Data\n\n# Data Prep ----\nobs_geno_lookup          = get_data('obs_geno_lookup')\nphno                     = get_data('phno')\nACGT_gene_slice_list     = get_data('KEGG_slices')\nparsed_kegg_gene_entries = get_data('KEGG_entries')\n\n\n# make holdout sets\nholdout_parents = params_data['holdout_parents']\n\n# create a mask for parent genotype\nmask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)\n\ntrain_mask = mask.sum(axis=1) == 0\ntest_mask  = mask.sum(axis=1) &gt; 0\n\ntrain_idx = train_mask.loc[train_mask].index\ntest_idx  = test_mask.loc[test_mask].index\n\n\n# convert y to residual if needed\n\nif params_data['y_resid'] == 'None':\n    pass\nelse:\n    if params_data['y_resid_strat'] == 'naive_mean':\n        # use only data in the training set (especially since testers will be more likely to be found across envs)\n        # get enviromental means, subtract from observed value\n        tmp = phno.loc[train_idx, ]\n        env_mean = tmp.groupby(['Env_Idx']\n                     ).agg(Env_Mean = (y_var, 'mean')\n                     ).reset_index()\n        tmp = phno.merge(env_mean)\n        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n        phno = tmp.drop(columns='Env_Mean')\n\n    if params_data['y_resid_strat'] == 'filter_mean':\n        # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations\n        def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],\n                                    year = 2014):\n            # Within each year what hybrids are most common?\n            tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')\n\n            all_envs = set(tmp.Env)\n            # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?\n            # site counts for sets which contain all envs\n            i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year &gt;= i), 'Env'])) == len(all_envs)])\n\n            before = len(set(tmp.loc[:, 'Hybrid']))\n            after  = len(set(tmp.loc[(tmp.Year &gt;= i), 'Hybrid']))\n            print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')\n            tmp = tmp.loc[(tmp.Year &gt;= i), ['Env', 'Hybrid']].reset_index(drop=True)\n            return tmp\n\n\n        tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]\n        filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) \n                          for i in list(set(phno.Year)) ]\n        env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')\n\n        env_mean = env_mean.groupby(['Env_Idx']\n                          ).agg(Env_Mean = (y_var, 'mean')\n                          ).reset_index()\n\n        tmp = phno.merge(env_mean)\n        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n        phno = tmp.drop(columns='Env_Mean')\n\n\n# center and y value data\nassert 0 == phno.loc[:, y_var].isna().sum()\n\ny = phno.loc[:, y_var]\n# use train index to prevent information leakage\ny_c = y[train_idx].mean()\ny_s = y[train_idx].std()\n\ny = (y - y_c)/y_s",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_eres_01.html#fit-using-vnnhelper",
    "href": "zma_g2f_eres_01.html#fit-using-vnnhelper",
    "title": "Visible Neural Network",
    "section": "Fit Using VNNHelper",
    "text": "Fit Using VNNHelper\n\ndef vnn_factory_1(parsed_kegg_gene_entries, params):\n\n    print(''.join('#' for i in range(80)))\n    print(params)\n    print(''.join('#' for i in range(80)))\n    \n    \n    default_out_nodes_inp  = params['default_out_nodes_inp' ]\n    default_out_nodes_edge = params['default_out_nodes_edge'] \n    default_out_nodes_out  = params['default_out_nodes_out' ]\n\n    default_drop_nodes_inp = params['default_drop_nodes_inp' ] \n    default_drop_nodes_edge= params['default_drop_nodes_edge'] \n    default_drop_nodes_out = params['default_drop_nodes_out' ] \n\n    default_reps_nodes_inp = params['default_reps_nodes_inp' ]\n    default_reps_nodes_edge= params['default_reps_nodes_edge']\n    default_reps_nodes_out = params['default_reps_nodes_out' ]\n\n\n\n    default_decay_rate = params['default_decay_rate' ]\n\n\n\n    # Clean up KEGG Pathways -------------------------------------------------------\n    # Same setup as above to create kegg_gene_brite\n    # Restrict to only those with pathway\n    kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n\n    # also require to have a non-empty path\n    kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n\n    print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n        )+'/'+str(len(parsed_kegg_gene_entries)\n        )+' Entries'\n        )\n    # kegg_gene_brite[1]['BRITE']['BRITE_PATHS']\n\n\n    kegg_connections = kegg_connections_build(kegg_gene_brite = kegg_gene_brite, \n                                            n_genes = len(kegg_gene_brite)) \n    kegg_connections = kegg_connections_clean(         kegg_connections = kegg_connections)\n    #TODO think about removing \n    # \"Not Included In\n    # Pathway Or Brite\"\n    # or reinstate 'Others'\n\n    kegg_connections = kegg_connections_append_y_hat(  kegg_connections = kegg_connections)\n    kegg_connections = kegg_connections_sanitize_names(kegg_connections = kegg_connections, \n                                                    replace_chars = {'.':'_'})\n\n\n    # Initialize helper for input nodes --------------------------------------------\n    myvnn = VNNHelper(edge_dict = kegg_connections)\n\n    # Get a mapping of brite names to tensor list index\n    find_names = myvnn.nodes_inp # e.g. ['100383860', '100278565', ... ]\n    lookup_dict = {}\n\n    # the only difference lookup_dict and brite_node_to_list_idx_dict above is that this is made using the full set of genes in the list \n    # whereas that is made using kegg_gene_brite which is a subset\n    for i in range(len(parsed_kegg_gene_entries)):\n        if 'BRITE' not in parsed_kegg_gene_entries[i].keys():\n            pass\n        elif parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'] == []:\n            pass\n        else:\n            name = parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'][0][-1]\n            if name in find_names:\n                lookup_dict[name] = i\n    # lookup_dict    \n\n    brite_node_to_list_idx_dict = {}\n    for i in range(len(kegg_gene_brite)):\n        brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i        \n\n    # Get the input sizes for the graph\n    size_in_zip = zip(myvnn.nodes_inp, [np.prod(ACGT_gene_slice_list[lookup_dict[e]].shape[1:]) for e  in myvnn.nodes_inp])\n\n    # Set node defaults ------------------------------------------------------------\n    # init input node sizes\n    myvnn.set_node_props(key = 'inp', node_val_zip = size_in_zip)\n\n    # init node output sizes\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_inp, [default_out_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_edge,[default_out_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_out, [default_out_nodes_out  for e in myvnn.nodes_out]))\n\n    # # options should be controlled by node_props\n    myvnn.set_node_props(key = 'flatten', node_val_zip = zip(myvnn.nodes_inp, [True for e in myvnn.nodes_inp]))\n\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_inp, [default_reps_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_edge,[default_reps_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_out, [default_reps_nodes_out  for e in myvnn.nodes_out]))\n\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_inp, [default_drop_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_edge,[default_drop_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_out, [default_drop_nodes_out  for e in myvnn.nodes_out]))\n\n\n    # Scale node outputs by distance -----------------------------------------------\n    dist = sparsevnn.core.vertex_from_end(\n        edge_dict = myvnn.edge_dict,\n        end =myvnn.dependancy_order[-1]\n    )\n\n    # overwrite node outputs with a size inversely proportional to distance from prediction node\n    for query in list(dist.keys()):\n        myvnn.node_props[query]['out'] = _dist_scale_function(\n            out = myvnn.node_props[query]['out'],\n            dist = dist[query],\n            decay_rate = default_decay_rate)\n        \n\n    # Expand out node replicates ---------------------------------------------------\n    nodes = [node for node in myvnn.dependancy_order if myvnn.node_props[node]['reps'] &gt; 1]\n\n    node_expansion_dict = {\n        node: [node if i==0 else f'{node}_{i}' for i in range(myvnn.node_props[node]['reps'])]\n        for node in nodes}\n    #   current       1st          2nd (new)      3rd (new)\n    # {'100798274': ['100798274', '100798274_1', '100798274_2'], ...\n\n    # the keys don't change here. The values will be updated and then new k:v will be inserted\n    myvnn.edge_dict = {k:[e if e not in node_expansion_dict.keys() \n        else node_expansion_dict[e][-1]\n        for e in myvnn.edge_dict[k] ] for k in myvnn.edge_dict}\n\n    # now insert connectsion to new nodes: A -&gt; A_rep_1 -&gt; A_rep_2\n    for node in node_expansion_dict:\n        for pair in zip(node_expansion_dict[node][1:], node_expansion_dict[node]):\n            myvnn.edge_dict[pair[0]] = [pair[1]]\n\n    # now add those new nodes\n    # create a new node for all the nodes\n    for node in node_expansion_dict:\n        for new_node in node_expansion_dict[node][1:]:\n            myvnn.node_props[new_node] = {k:myvnn.node_props[node][k] for k in myvnn.node_props[node] if k != 'inp'}\n\n    new_vnn = VNNHelper(edge_dict= myvnn.edge_dict)\n    new_vnn.node_props = myvnn.node_props\n    myvnn = new_vnn\n\n\n    # init edge node input size (propagate forward input/edge outpus)\n    myvnn.calc_edge_inp()\n\n    # replace lookup so that it matches the lenght of the input tensors\n    new_lookup_dict = {}\n    for i in range(len(myvnn.nodes_inp)):\n        new_lookup_dict[myvnn.nodes_inp[i]] = i\n    \n    return myvnn,  lookup_dict #new_lookup_dict\n\nmyvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = params)\n\n\nCalculate nodes membership in each matrix and positions within each\n\ndef vnn_factory_2(vnn_helper, node_to_inp_num_dict):\n    myvnn = vnn_helper\n\n    node_props = myvnn.node_props\n    # Linear_block = Linear_block_reps,\n    edge_dict = myvnn.edge_dict\n    dependancy_order = myvnn.dependancy_order\n    node_to_inp_num_dict = new_lookup_dict\n\n    # Build dependancy dictionary --------------------------------------------------\n    # check dep order\n    tally = []\n    for d in dependancy_order:\n        if edge_dict[d] == []:\n            tally.append(d)\n        elif False not in [True if e in tally else False for e in edge_dict[d]]:\n            tally.append(d)\n        else:\n            print('error!')\n            break\n\n\n    # build output nodes \n    d_out = {0:[]}\n    for d in dependancy_order:\n        if edge_dict[d] == []:\n            d_out[min(d_out.keys())].append(d)\n        else:\n            # print((d, edge_dict[d]))\n\n            d_out_i = 1+max(sum([[key for key in d_out.keys() if e in d_out[key]]\n                    for e in edge_dict[d]], []))\n            \n            if d_out_i not in d_out.keys():\n                d_out[d_out_i] = []\n            d_out[d_out_i].append(d)\n\n\n    # build input nodes NOPE. THE PASSHTROUGHS! \n    d_eye = {}\n    tally = []\n    for i in range(max(d_out.keys()), min(d_out.keys()), -1):\n        # print(i)\n        nodes_needed = sum([edge_dict[e] for e in d_out[i]], [])+tally\n        # check against what is there and then dedupe\n        nodes_needed = [e for e in nodes_needed if e not in d_out[i-1]]\n        nodes_needed = list(set(nodes_needed))\n        tally = nodes_needed\n        d_eye[i] = nodes_needed\n\n    # d_inp[0]= d_out[0]\n    # [len(d_eye[i]) for i in d_eye.keys()]\n    # [(key, len(d_out[key])) for key in d_out.keys()]\n\n\n    dd = {}\n    for i in d_eye.keys():\n        dd[i] = {'out': d_out[i],\n                'inp': d_out[i-1],\n                'eye': d_eye[i]}\n    # plus special 0 layer that handles the snps\n        \n    dd[0] = {'out': d_out[0],\n            'inp': d_out[0],\n            'eye': []}\n\n\n    # check that the output nodes' inputs are satisfied by the same layer's inputs (inp and eye)\n    for i in dd.keys():\n        # out node in each\n        for e in dd[i]['out']:\n            # node depends in inp/eye\n            node_pass_list = [True if ee in dd[i]['inp']+dd[i]['eye'] else False \n                            for ee in edge_dict[e]]\n            if False not in node_pass_list:\n                pass\n            else:\n                print('exit') \n\n\n    # print(\"Layer\\t#In\\t#Out\")\n    # for i in range(min(dd.keys()), max(dd.keys())+1, 1):\n    #     node_in      = [node_props[e]['out'] for e in dd[i]['inp']+dd[i  ]['eye'] ]\n    #     if i == max(dd.keys()):\n    #         node_out = [node_props[e]['out'] for e in dd[i]['out'] ]\n    #     else:\n    #         node_out = [node_props[e]['out'] for e in dd[i]['out']+dd[i+1]['eye']]\n    #     print(f'{i}:\\t{sum(node_in)}\\t{sum(node_out)}')\n\n    M_list = [structured_layer_info(i = ii, node_groups = dd, node_props= node_props, edge_dict = edge_dict, as_sparse=True) for ii in range(0, max(dd.keys())+1)]\n    return M_list\n\n### Creating Structured Matrices for Layers\nM_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)\n\n\n\nSetup Dataloader using M_list\n\nlookup_dict = new_lookup_dict\n\nvals = get_data('KEGG_slices')\nvals = [torch.from_numpy(e).to(torch.float) for e in vals]\n# restrict to the tensors that will be used\nvals = torch.concat([vals[lookup_dict[i]].reshape(4926, -1) \n                     for i in M_list[0].row_inp\n                    #  for i in dd[0]['inp'] # matches\n                     ], axis = 1)\n\nvals = vals.to('cuda')\n\n\ntraining_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n    lookup_geno = torch.from_numpy(obs_geno_lookup),\n    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n    G =           vals,\n    G_type = 'raw',\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = batch_size,\n    shuffle = True \n)\n\nvalidation_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n    lookup_geno = torch.from_numpy(obs_geno_lookup),\n    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n    G =           vals,\n    G_type = 'raw',\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = batch_size,\n    shuffle = False \n)",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_eres_01.html#structured-layer",
    "href": "zma_g2f_eres_01.html#structured-layer",
    "title": "Visible Neural Network",
    "section": "Structured Layer",
    "text": "Structured Layer\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, layer_list):\n        super(NeuralNetwork, self).__init__()\n        self.layer_list = nn.ModuleList(layer_list)\n \n    def forward(self, x):\n        for l in self.layer_list:\n            x = l(x)\n        return x\n\n\ndef vnn_factory_3(M_list):\n    layer_list = []\n    for i in range(len(M_list)):\n        \n        apply_relu = None\n        if i+1 != len(M_list): # apply relu to all but the last layer\n            apply_relu = F.relu\n        \n\n        l = SparseLinearCustom(\n            M_list[i].weight.shape[1], # have to transpose this?\n            M_list[i].weight.shape[0],\n            connectivity   = torch.LongTensor(M_list[i].weight.coalesce().indices()),\n            custom_weights = M_list[i].weight.coalesce().values(), \n            custom_bias    = M_list[i].bias.clone().detach(), \n            weight_grad_bool = M_list[i].weight_grad_bool, \n            bias_grad_bool   = M_list[i].bias_grad_bool, #.to_sparse()#.indices()\n            dropout_p        = M_list[i].dropout_p,\n            nonlinear_transform= apply_relu\n            )\n\n        layer_list += [l]\n        \n    return layer_list",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_eres_01.html#tiny-test-study",
    "href": "zma_g2f_eres_01.html#tiny-test-study",
    "title": "Visible Neural Network",
    "section": "Tiny Test Study",
    "text": "Tiny Test Study\n\n\n\ntype\nvalue key\nvalue type\n\n\n\n\nrange\nbounds\nlist\n\n\nchoice\nvalues\nlist\n\n\nfixed\nvalue\natomic\n\n\n\n\n\n# this is a very funny trick. I'm going to call lighning from within Ax. \n# That way I can save out traces while also relying on Ax to choose new hyps. \n\n\ndef evaluate(parameterization):\n    # draw from global\n    # max_epoch = 20\n    # lightning_log_dir = \"test_tb\"\n    myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = parameterization)\n    M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)\n    layer_list =  vnn_factory_3(M_list = M_list)\n    model = NeuralNetwork(layer_list = layer_list)\n    \n    VNN = plDNN_general(model)  \n    optimizer = VNN.configure_optimizers()\n    logger = CSVLogger(lightning_log_dir, name=exp_name)\n    logger.log_hyperparams(params={\n        'params': parameterization\n    })\n\n    trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n    trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n\n\n    # if we were optimizing number of training epochs this would be an effective loss to use.\n    # trainer.callback_metrics['train_loss']\n    # float(trainer.callback_metrics['train_loss'])\n\n    # To potentially _overtrain_ models and still let the selction be based on their best possible performance,\n    # I'll use the lowest average error in an epoch\n    log_path = lightning_log_dir+'/'+exp_name\n    fls = os.listdir(log_path)\n    nums = [int(e.split('_')[-1]) for e in fls] \n\n    M = pd.read_csv(log_path+f\"/version_{max(nums)}/metrics.csv\")\n    M = M.loc[:, ['epoch', 'train_loss']].dropna()\n\n    M = M.groupby('epoch').agg(\n        train_loss = ('train_loss', 'mean'),\n        train_loss_sd = ('train_loss', 'std'),\n        ).reset_index()\n\n    train_metric = M.train_loss.min()\n    print(train_metric)\n    return {\"train_loss\": (train_metric, 0.0)}\n\n\n## Generated variables ====\n# using sql database\nsql_url = \"sqlite:///\"+f\"./{lightning_log_dir}/{exp_name}.sqlite\"\n\n# If the database exists, load it and begin from there\nloaded_db = False\nif os.path.exists(sql_url.split('///')[-1]): # must cleave off the sql dialect prefix\n    # alternate way to load an experiment (after `init_engine_and_session_factory` has been run)\n    # experiment = load_experiment(exp_name) # if this doesn't work, check if the database is named something else and try that.\n    db_settings = DBSettings(url=sql_url)\n    # Instead of URL, can provide a `creator function`; can specify custom encoders/decoders if necessary.\n    ax_client = AxClient(db_settings=db_settings)\n    ax_client.load_experiment_from_database(exp_name)\n    loaded_db = True\n\nelse:\n    ax_client = AxClient()\n    ax_client.create_experiment(\n        name=exp_name,\n        parameters=params_list,\n        objectives={\"train_loss\": ObjectiveProperties(minimize=True)}\n    )\n\nrun_trials_bool = True\nif run_hyps_force == False:\n    if loaded_db: \n        # check if we've reached the max number of hyperparamters combinations to test\n        if max_hyps &lt;= (ax_client.generation_strategy.trials_as_df.index.max()+1):\n            run_trials_bool = False\n\nif run_trials_bool:\n    # run the trials\n    for i in range(run_hyps):\n        parameterization, trial_index = ax_client.get_next_trial()\n        # Local evaluation here can be replaced with deployment to external system.\n        ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameterization))\n\n    if loaded_db == False:\n        init_engine_and_session_factory(url=sql_url)\n        engine = get_engine()\n        create_all_tables(engine)\n\n    # save the trials\n    experiment = ax_client.experiment\n    save_experiment(experiment)\n\n\nax_client.generation_strategy.trials_as_df#.tail()\n\n\n# render(ax_client.get_contour_plot())\n\n\nrender(ax_client.get_optimization_trace(objective_optimum=0.0))\n\n\n# If I need to check what tables are in the sqlite\n# import sqlite3\n# con = sqlite3.connect(\"./foo.db\")\n# cur = con.cursor()\n# # cur.execute(\".tables;\") # should work, doesn't\n# cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n# con.close()",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "gmx_yhat_02.html",
    "href": "gmx_yhat_02.html",
    "title": "Visible Neural Network - Hyperparamter tuning applied to many yvars individually",
    "section": "",
    "text": "# Data ----\nfrom dataGMX.core import get_data # &lt;- Soybean Data\nfrom dataG2F.qol  import ensure_dir_path_exists\n\n# Data Utilities ----\nimport numpy  as np\nimport pandas as pd\n\nfrom EnvDL.dlfn import BigDataset, plDNN_general\nfrom EnvDL.sets import mask_columns\n\n# Model Building  ----\n## General ====\nimport torch\nfrom   torch import nn\nimport torch.nn.functional as F\nfrom   torch.utils.data import Dataset\nfrom   torch.utils.data import DataLoader\n\n## VNN ====\nimport sparsevnn\nfrom   sparsevnn.core import\\\n    VNNHelper, \\\n    structured_layer_info, \\\n    SparseLinearCustom\nfrom   sparsevnn.kegg import \\\n    kegg_connections_build, \\\n    kegg_connections_clean, \\\n    kegg_connections_append_y_hat, \\\n    kegg_connections_sanitize_names\n\n# Hyperparameter Tuning ----\nimport os # needed for checking history (saved by lightning) \n\n## Logging with Pytorch Lightning ====\nimport lightning.pytorch as pl\nfrom   lightning.pytorch.loggers import CSVLogger # used to save the history of each trial (used by ax)\n\n## Adaptive Experimentation Platform ====\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\nfrom ax.utils.notebook.plotting import init_notebook_plotting, render\n\n# # For logging experiment results in sql database\n# from ax.storage.sqa_store.db import init_engine_and_session_factory\n# from ax.storage.sqa_store.db import get_engine, create_all_tables\n# from ax.storage.sqa_store.save import save_experiment # saving\n# from ax.storage.sqa_store.structs import DBSettings # loading\n# # from ax.storage.sqa_store.load import load_experiment # loading alternate\ntorch.set_float32_matmul_precision('medium')\ninit_notebook_plotting()",
    "crumbs": [
      "Visible Neural Network - Hyperparamter tuning applied to many yvars individually"
    ]
  },
  {
    "objectID": "gmx_yhat_02.html#setup",
    "href": "gmx_yhat_02.html#setup",
    "title": "Visible Neural Network - Hyperparamter tuning applied to many yvars individually",
    "section": "Setup",
    "text": "Setup\n\ncache_path = '../nbs_artifacts/gmx_yhat_02/'\n\n\n# Run settings: \nparams_run = {\n    'batch_size': 128, # 256, \n    'max_epoch' : 512, # 256,\n}\n\n# data settings\nparams_data = {\n    'y_var': [\n        'sdwt100', 'ProteinDry', 'OilDry', 'AshDry', 'FiberDry', 'LysineDry', \n        'CysteineDry', 'MethionineDry', 'ThreonineDry', 'TryptophanDry', 'IsoleucineDry', \n        'LeucineDry', 'HistidineDry', 'PhenylalanineDry', 'ValineDry', 'AlanineDry', \n        'ArginineDry', 'AsparticacidDry', 'GlutamicacidDry', 'GlycineDry', 'ProlineDry', \n        'SerineDry', 'TyrosineDry', 'SucroseDry', 'Linolenic', 'Linoleic', 'Oleic', \n        'Palmitic', 'Moisture', 'RaffinoseDry', 'StachyoseDry', 'Stearic', 'SeedAS', \n        'SeedPL', 'SeedW', 'SeedLWR', 'SeedCS', 'SeedL', 'SampleWeight', 'B11', 'Na23', \n        'Mg26', 'Al27', 'P31', 'S34', 'K39', 'Ca44', 'Mn55', 'Ni60', 'Cu63', 'Zn66', \n        'Fe54', 'Co59', 'Se78', 'Rb85', 'Sr88', 'Mo98', 'Cd111', 'As75'],\n    'y_resid': 'None', # None, Env, Geno\n    'y_resid_strat': 'None', # None, naive_mean, filter_mean, ...\n    'holdout_parents': { # For this dataset a percent of genotypes are randomly held out. The seed value makes this reproducible. May switch to similarity based approach.\n        'rng_seed': 9874325,\n        'pr': 0.2} \n}\n\n\n## Settings ====\nrun_hyps = 100 \nrun_hyps_force = False # should we run more trials even if the target number has been reached?\nmax_hyps = 100\n\nparams_list = [    \n    ## Output Size ====\n    {\n    'name': 'default_out_nodes_inp',\n    'type': 'range',\n    'bounds': [1, 8],\n    'value_type': 'int',\n    'log_scale': False\n    },\n    {\n    'name': 'default_out_nodes_edge',\n    'type': 'range',\n    'bounds': [1, 32],\n    'value_type': 'int',\n    'log_scale': False\n    },\n    {\n    'name': 'default_out_nodes_out',\n    'type': 'fixed',\n    'value': 1, # len(params_data['y_var']) if type(params_data['y_var']) else 1,\n    'value_type': 'int',\n    'log_scale': False\n    },\n    ## Dropout ====\n    {\n    'name': 'default_drop_nodes_inp',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False\n    },\n    {\n    'name': 'default_drop_nodes_edge',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False\n    },\n    {\n    'name': 'default_drop_nodes_out',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False,\n    'sort_values':True\n    },\n    ## Node Repeats ====\n    {\n    'name': 'default_reps_nodes_inp',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    {\n    'name': 'default_reps_nodes_edge',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    {\n    'name': 'default_reps_nodes_out',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    ## Node Output Size Scaling ====\n    {\n    'name': 'default_decay_rate',\n    'type': 'choice',\n    'values': [0+(0.1*i) for i in range(10)]+[1.+(1*i) for i in range(11)],\n    'value_type': 'float',\n    'is_ordered': True,\n    'sort_values':True\n    }\n    ]\n\n\nlightning_log_dir = cache_path+\"lightning\"\nexp_name = [e for e in cache_path.split('/') if e != ''][-1]\n\n\n# parameterization is needed for setup. These values will be overwritten by Ax if tuning is occuring. \n# in this file I define params later. I've included it here to gurantee that we can merge other params dicts into it.\nparams = {\n'default_out_nodes_inp'  : 1,\n'default_out_nodes_edge' : 1,\n'default_out_nodes_out'  : len(params_data['y_var']) if type(params_data['y_var']) else 1,\n\n'default_drop_nodes_inp' : 0.0,\n'default_drop_nodes_edge': 0.0,\n'default_drop_nodes_out' : 0.0,\n\n'default_reps_nodes_inp' : 1,\n'default_reps_nodes_edge': 1,\n'default_reps_nodes_out' : 1,\n\n'default_decay_rate'     : 1\n}\n\ndefault_out_nodes_inp  = params['default_out_nodes_inp' ]\ndefault_out_nodes_edge = params['default_out_nodes_edge'] \ndefault_out_nodes_out  = params['default_out_nodes_out' ]\n\ndefault_drop_nodes_inp = params['default_drop_nodes_inp' ] \ndefault_drop_nodes_edge= params['default_drop_nodes_edge'] \ndefault_drop_nodes_out = params['default_drop_nodes_out' ] \n\ndefault_reps_nodes_inp = params['default_reps_nodes_inp' ]\ndefault_reps_nodes_edge= params['default_reps_nodes_edge']\ndefault_reps_nodes_out = params['default_reps_nodes_out' ]\n\ndefault_decay_rate = params['default_decay_rate' ]\n\n\nbatch_size = params_run['batch_size']\nmax_epoch  = params_run['max_epoch']\n\ny_var = params_data['y_var']\n\n\nsave_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n\nif 'None' != params_data['y_resid_strat']:\n    save_prefix = save_prefix+'_'+params_data['y_resid_strat']\n\nensure_dir_path_exists(dir_path = cache_path)\n\n\nuse_gpu_num = 0\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif use_gpu_num in [0, 1]: \n    torch.cuda.set_device(use_gpu_num)\nprint(f\"Using {device} device\")\n\n\ndef _dist_scale_function(out, dist, decay_rate):\n    scale = 1/(1+decay_rate*dist)\n    out = round(scale * out)\n    out = max(1, out)\n    return out\n\n\ndef _expand_node_shortcut(vnn_helper, query = 'y_hat'):\n    # define new entries\n    if True in [True if e in vnn_helper.edge_dict.keys() else False for e in \n                [f'{query}_res_-2', f'{query}_res_-1']\n                ]:\n        print('Warning! New node name already exists! Overwriting existing node!')\n\n    # Add residual connection in graph\n    vnn_helper.edge_dict[f'{query}_res_-2'] = myvnn.edge_dict[query] \n    vnn_helper.edge_dict[f'{query}_res_-1'] = [f'{query}_res_-2']\n    vnn_helper.edge_dict[query]             = [f'{query}_res_-2', f'{query}_res_-1']\n\n    # Add new nodes, copying information from query node\n    vnn_helper.node_props[f'{query}_res_-2'] = vnn_helper.node_props[query] \n    vnn_helper.node_props[f'{query}_res_-1'] = vnn_helper.node_props[query]\n\n    return vnn_helper",
    "crumbs": [
      "Visible Neural Network - Hyperparamter tuning applied to many yvars individually"
    ]
  },
  {
    "objectID": "gmx_yhat_02.html#load-data",
    "href": "gmx_yhat_02.html#load-data",
    "title": "Visible Neural Network - Hyperparamter tuning applied to many yvars individually",
    "section": "Load Data",
    "text": "Load Data\n\n# Data Prep ----\nobs_geno_lookup          = get_data('obs_geno_lookup')\nphno                     = get_data('phno')\nACGT_gene_slice_list     = get_data('KEGG_slices')\nparsed_kegg_gene_entries = get_data('KEGG_entries')\n\n\n# this dataset is very close to balanced wrt to the genotypes with 9-18 obs. \n# This code will turn a seed value and percent to be held out into a list of genotypse\nrng = np.random.default_rng( params_data['holdout_parents']['rng_seed'] )\ntmp = get_data(name = 'phno')\n\ntaxa = sorted(list(set(tmp.Taxa)))\nrng.shuffle(taxa)\ntaxa = pd.DataFrame(taxa).reset_index().rename(columns={0:'Taxa', 'index':'DrawOrder'})\n\ntmp = pd.merge(taxa, tmp).loc[:,['DrawOrder', 'Taxa']].assign(n=lambda x: 1).groupby(['DrawOrder', 'Taxa']).count().reset_index()\ntmp['cdf'] = tmp['n'].cumsum(0)/tmp['n'].sum()\n# filter\nholdout_parents = list(tmp.loc[(tmp.cdf &lt;= params_data['holdout_parents']['pr'] ), 'Taxa'])\n\n\n# make holdout sets\n# holdout_parents = params_data['holdout_parents']\n\n# create a mask for parent genotype\nmask = mask_columns(df= phno, col_name= 'Taxa', holdouts= holdout_parents)\n\n\ntrain_mask = mask.sum(axis=1) == 0\ntest_mask  = mask.sum(axis=1) &gt; 0\n\ntrain_idx = train_mask.loc[train_mask].index\ntest_idx  = test_mask.loc[test_mask].index\n\n\n# convert y to residual if needed\n\nif params_data['y_resid'] == 'None':\n    pass\n# TODO update for GMX\n# else:\n#     if params_data['y_resid_strat'] == 'naive_mean':\n#         # use only data in the training set (especially since testers will be more likely to be found across envs)\n#         # get enviromental means, subtract from observed value\n#         tmp = phno.loc[train_idx, ]\n#         env_mean = tmp.groupby(['Env_Idx']\n#                      ).agg(Env_Mean = (y_var, 'mean')\n#                      ).reset_index()\n#         tmp = phno.merge(env_mean)\n#         tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n#         phno = tmp.drop(columns='Env_Mean')\n\n#     if params_data['y_resid_strat'] == 'filter_mean':\n#         # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations\n#         def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],\n#                                     year = 2014):\n#             # Within each year what hybrids are most common?\n#             tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')\n\n#             all_envs = set(tmp.Env)\n#             # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?\n#             # site counts for sets which contain all envs\n#             i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year &gt;= i), 'Env'])) == len(all_envs)])\n\n#             before = len(set(tmp.loc[:, 'Hybrid']))\n#             after  = len(set(tmp.loc[(tmp.Year &gt;= i), 'Hybrid']))\n#             print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')\n#             tmp = tmp.loc[(tmp.Year &gt;= i), ['Env', 'Hybrid']].reset_index(drop=True)\n#             return tmp\n\n\n#         tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]\n#         filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) \n#                           for i in list(set(phno.Year)) ]\n#         env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')\n\n#         env_mean = env_mean.groupby(['Env_Idx']\n#                           ).agg(Env_Mean = (y_var, 'mean')\n#                           ).reset_index()\n\n#         tmp = phno.merge(env_mean)\n#         tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n#         phno = tmp.drop(columns='Env_Mean')\n\n\n# center and y value data\nassert 0 == phno.loc[:, y_var].isna().sum().sum() # second sum is for multiple y_vars\n\ny = phno.loc[:, y_var].to_numpy() # added to make multiple ys work\n# use train index to prevent information leakage\ny_c = y[train_idx].mean(axis=0)\ny_s = y[train_idx].std(axis=0)\n\ny = (y - y_c)/y_s",
    "crumbs": [
      "Visible Neural Network - Hyperparamter tuning applied to many yvars individually"
    ]
  },
  {
    "objectID": "gmx_yhat_02.html#fit-using-vnnhelper",
    "href": "gmx_yhat_02.html#fit-using-vnnhelper",
    "title": "Visible Neural Network - Hyperparamter tuning applied to many yvars individually",
    "section": "Fit Using VNNHelper",
    "text": "Fit Using VNNHelper\n\ndef vnn_factory_1(parsed_kegg_gene_entries, params):\n\n    print(''.join('#' for i in range(80)))\n    print(params)\n    print(''.join('#' for i in range(80)))\n    \n    \n    default_out_nodes_inp  = params['default_out_nodes_inp' ]\n    default_out_nodes_edge = params['default_out_nodes_edge'] \n    default_out_nodes_out  = params['default_out_nodes_out' ]\n\n    default_drop_nodes_inp = params['default_drop_nodes_inp' ] \n    default_drop_nodes_edge= params['default_drop_nodes_edge'] \n    default_drop_nodes_out = params['default_drop_nodes_out' ] \n\n    default_reps_nodes_inp = params['default_reps_nodes_inp' ]\n    default_reps_nodes_edge= params['default_reps_nodes_edge']\n    default_reps_nodes_out = params['default_reps_nodes_out' ]\n\n\n\n    default_decay_rate = params['default_decay_rate' ]\n\n\n\n    # Clean up KEGG Pathways -------------------------------------------------------\n    # Same setup as above to create kegg_gene_brite\n    # Restrict to only those with pathway\n    kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n\n    # also require to have a non-empty path\n    kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n\n    print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n        )+'/'+str(len(parsed_kegg_gene_entries)\n        )+' Entries'\n        )\n    # kegg_gene_brite[1]['BRITE']['BRITE_PATHS']\n\n\n    kegg_connections = kegg_connections_build(kegg_gene_brite = kegg_gene_brite, \n                                            n_genes = len(kegg_gene_brite)) \n    kegg_connections = kegg_connections_clean(         kegg_connections = kegg_connections)\n    #TODO think about removing \n    # \"Not Included In\n    # Pathway Or Brite\"\n    # or reinstate 'Others'\n\n    kegg_connections = kegg_connections_append_y_hat(  kegg_connections = kegg_connections)\n    kegg_connections = kegg_connections_sanitize_names(kegg_connections = kegg_connections, \n                                                    replace_chars = {'.':'_'})\n\n\n    # Initialize helper for input nodes --------------------------------------------\n    myvnn = VNNHelper(edge_dict = kegg_connections)\n\n    # Get a mapping of brite names to tensor list index\n    find_names = myvnn.nodes_inp # e.g. ['100383860', '100278565', ... ]\n    lookup_dict = {}\n\n    # the only difference lookup_dict and brite_node_to_list_idx_dict above is that this is made using the full set of genes in the list \n    # whereas that is made using kegg_gene_brite which is a subset\n    for i in range(len(parsed_kegg_gene_entries)):\n        if 'BRITE' not in parsed_kegg_gene_entries[i].keys():\n            pass\n        elif parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'] == []:\n            pass\n        else:\n            name = parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'][0][-1]\n            if name in find_names:\n                lookup_dict[name] = i\n    # lookup_dict    \n\n    brite_node_to_list_idx_dict = {}\n    for i in range(len(kegg_gene_brite)):\n        brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i        \n\n    # Get the input sizes for the graph\n    size_in_zip = zip(myvnn.nodes_inp, [np.prod(ACGT_gene_slice_list[lookup_dict[e]].shape[1:]) for e  in myvnn.nodes_inp])\n\n    # Set node defaults ------------------------------------------------------------\n    # init input node sizes\n    myvnn.set_node_props(key = 'inp', node_val_zip = size_in_zip)\n\n    # init node output sizes\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_inp, [default_out_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_edge,[default_out_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_out, [default_out_nodes_out  for e in myvnn.nodes_out]))\n\n    # # options should be controlled by node_props\n    myvnn.set_node_props(key = 'flatten', node_val_zip = zip(myvnn.nodes_inp, [True for e in myvnn.nodes_inp]))\n\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_inp, [default_reps_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_edge,[default_reps_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_out, [default_reps_nodes_out  for e in myvnn.nodes_out]))\n\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_inp, [default_drop_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_edge,[default_drop_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_out, [default_drop_nodes_out  for e in myvnn.nodes_out]))\n\n\n    # Scale node outputs by distance -----------------------------------------------\n    dist = sparsevnn.core.vertex_from_end(\n        edge_dict = myvnn.edge_dict,\n        end =myvnn.dependancy_order[-1]\n    )\n\n    # overwrite node outputs with a size inversely proportional to distance from prediction node\n    for query in list(dist.keys()):\n        myvnn.node_props[query]['out'] = _dist_scale_function(\n            out = myvnn.node_props[query]['out'],\n            dist = dist[query],\n            decay_rate = default_decay_rate)\n        \n\n    # Expand out node replicates ---------------------------------------------------\n    # kegg_connections_expanded = sparsevnn.core.expand_edge_dict(vnn_helper = myvnn, edge_dict = myvnn.edge_dict)\n\n    # ======================================================= #\n    # one place to add residual connections would be here.    #\n    # edit the links before instatinating the new VNNHelper   #\n    # the important thing to do is to edit the graph before   #\n    # calulating the inputs for each node.                    #\n    # ======================================================= #\n\n            # # expand then copy over the properties that have already been defined.\n            # myvnn_exp = VNNHelper(edge_dict = kegg_connections_expanded)\n\n            # import re\n            # for new_key in list(myvnn_exp.node_props.keys()):\n            #     if new_key in myvnn.node_props.keys():\n            #         # copy directly\n            #         myvnn_exp.node_props[new_key] = myvnn.node_props[new_key]\n            #     else:\n            #         # check for a key that matches the query key after removing the replicate information\n            #         query = new_key \n            #         suffix = re.findall('_rep_\\d+$', query)[0]\n\n            #         query = query.removesuffix(suffix)\n            #         if query in myvnn.node_props.keys():\n            #             myvnn_exp.node_props[new_key] = myvnn.node_props[query]\n            #         else:\n            #             print(f'WARNING: no entry {query} found for {new_key}') \n\n            # # now main vnn is the expanded version\n            # myvnn = myvnn_exp\n\n\n            # # Cleanup ----------------------------------------------------------------------\n            # # now the original VNNHelper isn't needed\n            # myvnn = myvnn_exp\n\n\n    # expand out graph.\n#     update_edge_links = {}\n#     nodes = myvnn.dependancy_order\n\n#     for query in [e for e in reversed(nodes)]:\n#         if myvnn.node_props[query]['reps'] == 1:\n#             pass\n#         else:\n#             reps = myvnn.node_props[query]['reps']\n#             # set to 1 so that we can copy all the props (except input) over\n#             myvnn.node_props[query]['reps'] = 1\n            \n#             for i in range(reps-1,0,-1):\n#                 if i == 0:\n#                     # no replicates\n#                     pass\n#                 else:\n#                     if i == 1:\n#                         # print({f'{query}_{i}':[f'{query}']})\n#                         update_edge_links[f'{query}_{i}'] = f'{query}'\n#                     else:\n#                         # print({f'{query}_{i}':[f'{query}_{i-1}']})\n#                         update_edge_links[f'{query}_{i}'] = f'{query}_{i-1}'\n\n#                     # copy over all properties except input (input will either be set for the data or calculated on the fly)\n#                     myvnn.node_props[f'{query}_{i}'] = {k:myvnn.node_props[query][k] for k in myvnn.node_props[query] if k != 'inp'}\n\n\n#     # Now there should be new nodes in the helper but the links need to be updated to point to the right names. \n\n#     if True:\n#         # update existing links\n#         # create a lookup dictionary to map the old names to new names\n#         old_to_new = {update_edge_links[k]:k for k in update_edge_links}\n#         # old_to_new\n\n#         for k in myvnn.edge_dict:\n#             myvnn.edge_dict[k] = [e if e not in old_to_new.keys() else old_to_new[e] for e in myvnn.edge_dict[k]]\n\n#         # add in new nodes\n#         for k in update_edge_links:\n#             myvnn.edge_dict[k] = [update_edge_links[k]]\n\n#         # overwrite dependancy order\n#         # myvnn.dependancy_order = VNNHelper(edge_dict= myvnn.edge_dict).dependancy_order\n# # myvnn_updated = VNNHelper(edge_dict= myvnn.edge_dict)\n# # myvnn_updated.node_props = myvnn.node_props\n#         # myvnn = myvnn_updated\n\n    # expand out graph.\n    update_edge_links = {}\n    nodes = [node for node in myvnn.dependancy_order if myvnn.node_props[node]['reps'] &gt; 1]\n\n    node_expansion_dict = {\n        node: [node if i==0 else f'{node}_{i}' for i in range(myvnn.node_props[node]['reps'])]\n        for node in nodes}\n    #   current       1st          2nd (new)      3rd (new)\n    # {'100798274': ['100798274', '100798274_1', '100798274_2'], ...\n\n    # the keys don't change here. The values will be updated and then new k:v will be inserted\n    myvnn.edge_dict = {k:[e if e not in node_expansion_dict.keys() \n        else node_expansion_dict[e][-1]\n        for e in myvnn.edge_dict[k] ] for k in myvnn.edge_dict}\n\n    # now insert connectsion to new nodes: A -&gt; A_rep_1 -&gt; A_rep_2\n    for node in node_expansion_dict:\n        for pair in zip(node_expansion_dict[node][1:], node_expansion_dict[node]):\n            myvnn.edge_dict[pair[0]] = [pair[1]]\n\n    # now add those new nodes\n    # create a new node for all the nodes\n    for node in node_expansion_dict:\n        for new_node in node_expansion_dict[node][1:]:\n            myvnn.node_props[new_node] = {k:myvnn.node_props[node][k] for k in myvnn.node_props[node] if k != 'inp'}\n\n    new_vnn = VNNHelper(edge_dict= myvnn.edge_dict)\n    new_vnn.node_props = myvnn.node_props\n    myvnn = new_vnn\n\n\n    # init edge node input size (propagate forward input/edge outpus)\n    myvnn.calc_edge_inp()\n\n    # replace lookup so that it matches the lenght of the input tensors\n    new_lookup_dict = {}\n    for i in range(len(myvnn.nodes_inp)):\n        new_lookup_dict[myvnn.nodes_inp[i]] = i\n    \n    return myvnn,  lookup_dict #new_lookup_dict\n\nmyvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = params)\n\n\nCalculate nodes membership in each matrix and positions within each\n\ndef vnn_factory_2(vnn_helper, node_to_inp_num_dict):\n    myvnn = vnn_helper\n\n    node_props = myvnn.node_props\n    # Linear_block = Linear_block_reps,\n    edge_dict = myvnn.edge_dict\n    dependancy_order = myvnn.dependancy_order\n    node_to_inp_num_dict = new_lookup_dict\n\n    # Build dependancy dictionary --------------------------------------------------\n    # check dep order\n    tally = []\n    for d in dependancy_order:\n        if edge_dict[d] == []:\n            tally.append(d)\n        elif False not in [True if e in tally else False for e in edge_dict[d]]:\n            tally.append(d)\n        else:\n            print('error!')\n            break\n\n\n    # build output nodes \n    d_out = {0:[]}\n    for d in dependancy_order:\n        if edge_dict[d] == []:\n            d_out[min(d_out.keys())].append(d)\n        else:\n            # print((d, edge_dict[d]))\n\n            d_out_i = 1+max(sum([[key for key in d_out.keys() if e in d_out[key]]\n                    for e in edge_dict[d]], []))\n            \n            if d_out_i not in d_out.keys():\n                d_out[d_out_i] = []\n            d_out[d_out_i].append(d)\n\n\n    # build input nodes NOPE. THE PASSHTROUGHS! \n    d_eye = {}\n    tally = []\n    for i in range(max(d_out.keys()), min(d_out.keys()), -1):\n        # print(i)\n        nodes_needed = sum([edge_dict[e] for e in d_out[i]], [])+tally\n        # check against what is there and then dedupe\n        nodes_needed = [e for e in nodes_needed if e not in d_out[i-1]]\n        nodes_needed = list(set(nodes_needed))\n        tally = nodes_needed\n        d_eye[i] = nodes_needed\n\n    # d_inp[0]= d_out[0]\n    # [len(d_eye[i]) for i in d_eye.keys()]\n    # [(key, len(d_out[key])) for key in d_out.keys()]\n\n\n    dd = {}\n    for i in d_eye.keys():\n        dd[i] = {'out': d_out[i],\n                'inp': d_out[i-1],\n                'eye': d_eye[i]}\n    # plus special 0 layer that handles the snps\n        \n    dd[0] = {'out': d_out[0],\n            'inp': d_out[0],\n            'eye': []}\n\n\n    # check that the output nodes' inputs are satisfied by the same layer's inputs (inp and eye)\n    for i in dd.keys():\n        # out node in each\n        for e in dd[i]['out']:\n            # node depends in inp/eye\n            node_pass_list = [True if ee in dd[i]['inp']+dd[i]['eye'] else False \n                            for ee in edge_dict[e]]\n            if False not in node_pass_list:\n                pass\n            else:\n                print('exit') \n\n\n    # print(\"Layer\\t#In\\t#Out\")\n    # for i in range(min(dd.keys()), max(dd.keys())+1, 1):\n    #     node_in      = [node_props[e]['out'] for e in dd[i]['inp']+dd[i  ]['eye'] ]\n    #     if i == max(dd.keys()):\n    #         node_out = [node_props[e]['out'] for e in dd[i]['out'] ]\n    #     else:\n    #         node_out = [node_props[e]['out'] for e in dd[i]['out']+dd[i+1]['eye']]\n    #     print(f'{i}:\\t{sum(node_in)}\\t{sum(node_out)}')\n\n    M_list = [structured_layer_info(i = ii, node_groups = dd, node_props= node_props, edge_dict = edge_dict, as_sparse=True) for ii in range(0, max(dd.keys())+1)]\n    return M_list\n\n### Creating Structured Matrices for Layers\nM_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)\n\n\n\nSetup Dataloader using M_list\n\nlookup_dict = new_lookup_dict\n\nvals = get_data('KEGG_slices')\nvals = [torch.from_numpy(e).to(torch.float) for e in vals]\n# restrict to the tensors that will be used\nvals = torch.concat([vals[lookup_dict[i]].reshape(vals[0].shape[0], -1) \n                     for i in M_list[0].row_inp\n                    #  for i in dd[0]['inp'] # matches\n                     ], axis = 1)\n\nvals = vals.to('cuda')\n\n\ntraining_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n    lookup_geno = torch.from_numpy(obs_geno_lookup),\n    y =           torch.from_numpy(y).to(torch.float32),\n    G =           vals,\n    G_type = 'raw',\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = batch_size,\n    shuffle = True \n)\n\nvalidation_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n    lookup_geno = torch.from_numpy(obs_geno_lookup),\n    y =           torch.from_numpy(y).to(torch.float32),\n    G =           vals,\n    G_type = 'raw',\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = batch_size,\n    shuffle = False \n)",
    "crumbs": [
      "Visible Neural Network - Hyperparamter tuning applied to many yvars individually"
    ]
  },
  {
    "objectID": "gmx_yhat_02.html#structured-layer",
    "href": "gmx_yhat_02.html#structured-layer",
    "title": "Visible Neural Network - Hyperparamter tuning applied to many yvars individually",
    "section": "Structured Layer",
    "text": "Structured Layer\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, layer_list):\n        super(NeuralNetwork, self).__init__()\n        self.layer_list = nn.ModuleList(layer_list)\n \n    def forward(self, x):\n        for l in self.layer_list:\n            x = l(x)\n        return x\n\n\ndef vnn_factory_3(M_list):\n    layer_list = []\n    for i in range(len(M_list)):\n        \n        apply_relu = None\n        if i+1 != len(M_list): # apply relu to all but the last layer\n            apply_relu = F.relu\n        \n\n        l = SparseLinearCustom(\n            M_list[i].weight.shape[1], # have to transpose this?\n            M_list[i].weight.shape[0],\n            connectivity   = torch.LongTensor(M_list[i].weight.coalesce().indices()),\n            custom_weights = M_list[i].weight.coalesce().values(), \n            custom_bias    = M_list[i].bias.clone().detach(), \n            weight_grad_bool = M_list[i].weight_grad_bool, \n            bias_grad_bool   = M_list[i].bias_grad_bool, #.to_sparse()#.indices()\n            dropout_p        = M_list[i].dropout_p,\n            nonlinear_transform= apply_relu\n            )\n\n        layer_list += [l]\n        \n    return layer_list",
    "crumbs": [
      "Visible Neural Network - Hyperparamter tuning applied to many yvars individually"
    ]
  },
  {
    "objectID": "gmx_yhat_02.html#tiny-test-study",
    "href": "gmx_yhat_02.html#tiny-test-study",
    "title": "Visible Neural Network - Hyperparamter tuning applied to many yvars individually",
    "section": "Tiny Test Study",
    "text": "Tiny Test Study\n\n\n\ntype\nvalue key\nvalue type\n\n\n\n\nrange\nbounds\nlist\n\n\nchoice\nvalues\nlist\n\n\nfixed\nvalue\natomic\n\n\n\n\n\n# this is a very funny trick. I'm going to call lighning from within Ax. \n# That way I can save out traces while also relying on Ax to choose new hyps. \n\n\ndef evaluate(parameterization):\n    # draw from global\n    # max_epoch = 20\n    # lightning_log_dir = \"test_tb\"\n    myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = parameterization)\n    M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)\n    layer_list =  vnn_factory_3(M_list = M_list)\n    model = NeuralNetwork(layer_list = layer_list)\n    \n    VNN = plDNN_general(model)  \n    optimizer = VNN.configure_optimizers()\n    logger = CSVLogger(lightning_log_dir, name=exp_name)\n    logger.log_hyperparams(params={\n        'params': parameterization\n    })\n\n    trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n    trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n\n\n    # if we were optimizing number of training epochs this would be an effective loss to use.\n    # trainer.callback_metrics['train_loss']\n    # float(trainer.callback_metrics['train_loss'])\n\n    # To potentially _overtrain_ models and still let the selction be based on their best possible performance,\n    # I'll use the lowest average error in an epoch\n    log_path = lightning_log_dir+'/'+exp_name\n    fls = os.listdir(log_path)\n    nums = [int(e.split('_')[-1]) for e in fls] \n\n    M = pd.read_csv(log_path+f\"/version_{max(nums)}/metrics.csv\")\n    M = M.loc[:, ['epoch', 'train_loss']].dropna()\n\n    M = M.groupby('epoch').agg(\n        train_loss = ('train_loss', 'mean'),\n        train_loss_sd = ('train_loss', 'std'),\n        ).reset_index()\n\n    train_metric = M.train_loss.min()\n    print(train_metric)\n    return {\"train_loss\": (train_metric, 0.0)}\n\n\n# ## Generated variables ====\n# # using sql database\n# sql_url = \"sqlite:///\"+f\"./{lightning_log_dir}/{exp_name}.sqlite\"\n\n# # If the database exists, load it and begin from there\n# loaded_db = False\n# if os.path.exists(sql_url.split('///')[-1]): # must cleave off the sql dialect prefix\n#     # alternate way to load an experiment (after `init_engine_and_session_factory` has been run)\n#     # experiment = load_experiment(exp_name) # if this doesn't work, check if the database is named something else and try that.\n#     db_settings = DBSettings(url=sql_url)\n#     # Instead of URL, can provide a `creator function`; can specify custom encoders/decoders if necessary.\n#     ax_client = AxClient(db_settings=db_settings)\n#     ax_client.load_experiment_from_database(exp_name)\n#     loaded_db = True\n\n# else:\n#     ax_client = AxClient()\n#     ax_client.create_experiment(\n#         name=exp_name,\n#         parameters=params_list,\n#         objectives={\"train_loss\": ObjectiveProperties(minimize=True)}\n#     )\n\n# run_trials_bool = True\n# if run_hyps_force == False:\n#     if loaded_db: \n#         # check if we've reached the max number of hyperparamters combinations to test\n#         if max_hyps &lt;= (ax_client.generation_strategy.trials_as_df.index.max()+1):\n#             run_trials_bool = False\n\n# if run_trials_bool:\n#     # run the trials\n#     for i in range(run_hyps):\n#         parameterization, trial_index = ax_client.get_next_trial()\n#         # Local evaluation here can be replaced with deployment to external system.\n#         ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameterization))\n\n#     if loaded_db == False:\n#         init_engine_and_session_factory(url=sql_url)\n#         engine = get_engine()\n#         create_all_tables(engine)\n\n#     # save the trials\n#     experiment = ax_client.experiment\n#     save_experiment(experiment)\n\n\ndef _prep_dls(\n        y_var,\n        train_idx,\n        obs_geno_lookup,\n        vals,\n        batch_size,\n        test_idx):\n\n    phno = get_data('phno')\n    # center and y value data\n    assert 0 == phno.loc[:, y_var].isna().sum().sum() # second sum is for multiple y_vars\n\n    y = phno.loc[:, y_var].to_numpy() # added to make multiple ys work\n    # use train index to prevent information leakage\n    y_c = y[train_idx].mean(axis=0)\n    y_s = y[train_idx].std(axis=0)\n\n    y = (y - y_c)/y_s\n\n\n\n    training_dataloader = DataLoader(BigDataset(\n        lookups_are_filtered = False,\n        lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n        lookup_geno = torch.from_numpy(obs_geno_lookup),\n        y =           torch.from_numpy(y).to(torch.float32)[:, None],\n        G =           vals,\n        G_type = 'raw',\n        send_batch_to_gpu = 'cuda:0'\n        ),\n        batch_size = batch_size,\n        shuffle = True \n    )\n\n    validation_dataloader = DataLoader(BigDataset(\n        lookups_are_filtered = False,\n        lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n        lookup_geno = torch.from_numpy(obs_geno_lookup),\n        y =           torch.from_numpy(y).to(torch.float32)[:, None],\n        G =           vals,\n        G_type = 'raw',\n        send_batch_to_gpu = 'cuda:0'\n        ),\n        batch_size = batch_size,\n        shuffle = False \n    )\n\n    return training_dataloader, validation_dataloader\n\n\ny_vars = y_var.copy()\n\n\n# This is an inelegant soltuion. I want to run 59 experiments. They're fast enough and I have enough other work that they can run in serial.\n# If this wasn't the case I would use a helper script to generate json files with settings and use nextflow to run this in parallel.\n# Storing multiple experiments is proving to be a pain. \n# \n# Options:\n# 1. Single DB\n# If I use a single database there isn't good tooling in Ax to check if an experiment exists. \n# We should be able to use sqlite3 to check for the experiment name and either restore or create a new one but that's not working. \n# \n# 2. Named DBs in one location.\n# Unless databases are force init, Ax uses a single one. I've tried specifing a dir inside the lightning folder but that is not working either \n# (possibly because that dir is created by lightning). There may be a solution that I can come to by reading up on SQLAlchemy but that will slow velocity.\n# \n# 3. JSONs\n# Using JSONs should be the easy solution. Ax's documentation doesn't make it clear how to restore the experiment. It should be straightforward. \n# There is an issue with a save field. \n# \n# 4. Hacky shuffling of DBs. \n# In this approach I'll use shutil to move the Ax db to and from a subdir. It has the side benefit of making `max_hyps &lt;= (ax_client.generation_strategy.trials_as_df.index.max()+1)`\n# work as is since only one expeirment will be in each db. \n# This didn't work because (presumably SQLAlchemy) the database is not being correctly changed. using `del` to clean out the scope did not work.\n#\n# 5. JSONs revisited\n# What worked was getting the json save/restore fixed. To restore the client use: \n# restored_ax_client = (AxClient.load_from_json_file(json_path))\n# restored_ax_client.get_best_parameters()\n\nfor y_var in y_vars:\n    # y_var = ith_y_var\n    # update exp_name\n    exp_name = [e for e in cache_path.split('/') if e != ''][-1]\n    exp_name += '__'+y_var\n\n    print(''.join(['-' for i in range(80)]))\n    print(f'experiment: {exp_name}')\n    print(''.join(['-' for i in range(80)]))\n    print('\\n')\n\n    training_dataloader, validation_dataloader = _prep_dls(\n            y_var = y_var,\n            train_idx = train_idx,\n            obs_geno_lookup = obs_geno_lookup,\n            vals = vals,\n            batch_size = batch_size,\n            test_idx = test_idx)\n\n    ## Generated variables ====\n    json_path = f\"./{lightning_log_dir}/{exp_name}.json\"\n\n    loaded_json = False\n    if os.path.exists(json_path): \n        ax_client = (AxClient.load_from_json_file(filepath = json_path))\n        loaded_json = True\n\n    else:\n        ax_client = AxClient()\n        ax_client.create_experiment(\n            name=exp_name,\n            parameters=params_list,\n            objectives={\"train_loss\": ObjectiveProperties(minimize=True)}\n        )\n\n    run_trials_bool = True\n    if run_hyps_force == False:\n        if loaded_json: \n            # check if we've reached the max number of hyperparamters combinations to test\n            if max_hyps &lt;= (ax_client.generation_strategy.trials_as_df.index.max()+1):\n                run_trials_bool = False\n\n    if run_trials_bool:\n        # run the trials\n        for i in range(run_hyps):\n            parameterization, trial_index = ax_client.get_next_trial()\n            # Local evaluation here can be replaced with deployment to external system.\n            ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameterization))\n\n        ax_client.save_to_json_file(filepath = json_path)\n\n\n# ax_client.generation_strategy.trials_as_df#.tail()\n\n\n# render(ax_client.get_contour_plot())\n\n\n# render(ax_client.get_optimization_trace(objective_optimum=0.0))\n\n\n# If I need to check what tables are in the sqlite\n# import sqlite3\n# con = sqlite3.connect(\"./foo.db\")\n# cur = con.cursor()\n# # cur.execute(\".tables;\") # should work, doesn't\n# cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n# con.close()",
    "crumbs": [
      "Visible Neural Network - Hyperparamter tuning applied to many yvars individually"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "zma_g2f_yhat_02.html",
    "href": "zma_g2f_yhat_02.html",
    "title": "Visible Neural Network",
    "section": "",
    "text": "# Data ----\nfrom dataG2F.core import get_data\nfrom dataG2F.qol  import ensure_dir_path_exists\n\n# Data Utilities ----\nimport numpy  as np\nimport pandas as pd\n\nfrom EnvDL.dlfn import BigDataset, plDNN_general\nfrom EnvDL.sets import mask_parents\n\n# Model Building  ----\n## General ====\nimport torch\nfrom   torch import nn\nimport torch.nn.functional as F\nfrom   torch.utils.data import Dataset\nfrom   torch.utils.data import DataLoader\n\n## VNN ====\nimport sparsevnn\nfrom   sparsevnn.core import\\\n    VNNHelper, \\\n    structured_layer_info, \\\n    SparseLinearCustom\nfrom   sparsevnn.kegg import \\\n    kegg_connections_build, \\\n    kegg_connections_clean, \\\n    kegg_connections_append_y_hat, \\\n    kegg_connections_sanitize_names\n\n# Hyperparameter Tuning ----\nimport os # needed for checking history (saved by lightning) \n\n## Logging with Pytorch Lightning ====\nimport lightning.pytorch as pl\nfrom   lightning.pytorch.loggers import CSVLogger # used to save the history of each trial (used by ax)\n\n## Adaptive Experimentation Platform ====\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\nfrom ax.utils.notebook.plotting import init_notebook_plotting, render\n\n# For logging experiment results in sql database\n# from ax.storage.sqa_store.db import init_engine_and_session_factory\n# from ax.storage.sqa_store.db import get_engine, create_all_tables\n# from ax.storage.sqa_store.save import save_experiment # saving\n# from ax.storage.sqa_store.structs import DBSettings # loading\n# # from ax.storage.sqa_store.load import load_experiment # loading alternate\ntorch.set_float32_matmul_precision('medium')\ninit_notebook_plotting()\n\n\n\n\n[INFO 05-29 11:05:17] ax.utils.notebook.plotting: Injecting Plotly library into cell. Do not overwrite or delete cell.\n[INFO 05-29 11:05:17] ax.utils.notebook.plotting: Please see\n    (https://ax.dev/tutorials/visualizations.html#Fix-for-plots-that-are-not-rendering)\n    if visualizations are not rendering.",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_yhat_02.html#setup",
    "href": "zma_g2f_yhat_02.html#setup",
    "title": "Visible Neural Network",
    "section": "Setup",
    "text": "Setup\n\ncache_path = '../nbs_artifacts/zma_g2f_yhat_02/'\n\n\n# Run settings: \nparams_run = {\n    'batch_size': 256, #256,\n    'max_epoch' : 2   #256,    \n}\n\n# data settings\nparams_data = {\n    # 'y_var': 'Yield_Mg_ha',\n    'y_var': 'Pollen_DAP_days',\n    'y_resid': 'None', # None, Env, Geno\n    'y_resid_strat': 'None', # None, naive_mean, filter_mean, ...\n    'holdout_parents': [\n        ## 2022 ====\n        'LH244',\n        ## 2021 ====\n        'PHZ51',\n        # 'PHP02',\n        # 'PHK76',\n        ## 2019 ====\n        # 'PHT69',\n        'LH195',\n        ## 2017 ====\n        # 'PHW52',\n        # 'PHN82',\n        ## 2016 ====\n        # 'DK3IIH6',\n        ## 2015 ====\n        # 'PHB47',\n        # 'LH82',\n        ## 2014 ====\n        # 'LH198',\n        # 'LH185',\n        # 'PB80',\n        # 'CG102',\n ],    \n}\n\n\n## Settings ====\nrun_hyps = 5 #75 \nrun_hyps_force = False # should we run more trials even if the target number has been reached?\nmax_hyps = 100\n\nparams_list = [    \n    ## Output Size ====\n    {\n    'name': 'default_out_nodes_inp',\n    'type': 'range',\n    'bounds': [1, 8],\n    'value_type': 'int',\n    'log_scale': False\n    },\n    {\n    'name': 'default_out_nodes_edge',\n    'type': 'range',\n    'bounds': [1, 32],\n    'value_type': 'int',\n    'log_scale': False\n    },\n    {\n    'name': 'default_out_nodes_out',\n    'type': 'fixed',\n    'value': 1,\n    'value_type': 'int',\n    'log_scale': False\n    },\n    ## Dropout ====\n    {\n    'name': 'default_drop_nodes_inp',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False\n    },\n    {\n    'name': 'default_drop_nodes_edge',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False\n    },\n    {\n    'name': 'default_drop_nodes_out',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False,\n    'sort_values':True\n    },\n    ## Node Repeats ====\n    {\n    'name': 'default_reps_nodes_inp',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    {\n    'name': 'default_reps_nodes_edge',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    {\n    'name': 'default_reps_nodes_out',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    ## Node Output Size Scaling ====\n    {\n    'name': 'default_decay_rate',\n    'type': 'choice',\n    'values': [0+(0.1*i) for i in range(10)]+[1.+(1*i) for i in range(11)],\n    'value_type': 'float',\n    'is_ordered': True,\n    'sort_values':True\n    }\n    ]\n\n\nlightning_log_dir = cache_path+\"lightning\"\nexp_name = [e for e in cache_path.split('/') if e != ''][-1]\n\n\n# parameterization is needed for setup. These values will be overwritten by Ax if tuning is occuring. \n# in this file I define params later. I've included it here to gurantee that we can merge other params dicts into it.\nparams = {\n'default_out_nodes_inp'  : 1,\n'default_out_nodes_edge' : 1,\n'default_out_nodes_out'  : 1,\n\n'default_drop_nodes_inp' : 0.0,\n'default_drop_nodes_edge': 0.0,\n'default_drop_nodes_out' : 0.0,\n\n'default_reps_nodes_inp' : 1,\n'default_reps_nodes_edge': 1,\n'default_reps_nodes_out' : 1,\n\n'default_decay_rate'     : 1\n}\n\ndefault_out_nodes_inp  = params['default_out_nodes_inp' ]\ndefault_out_nodes_edge = params['default_out_nodes_edge'] \ndefault_out_nodes_out  = params['default_out_nodes_out' ]\n\ndefault_drop_nodes_inp = params['default_drop_nodes_inp' ] \ndefault_drop_nodes_edge= params['default_drop_nodes_edge'] \ndefault_drop_nodes_out = params['default_drop_nodes_out' ] \n\ndefault_reps_nodes_inp = params['default_reps_nodes_inp' ]\ndefault_reps_nodes_edge= params['default_reps_nodes_edge']\ndefault_reps_nodes_out = params['default_reps_nodes_out' ]\n\ndefault_decay_rate = params['default_decay_rate' ]\n\n\nbatch_size = params_run['batch_size']\nmax_epoch  = params_run['max_epoch']\n\ny_var = params_data['y_var']\n\n\nsave_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n\nif 'None' != params_data['y_resid_strat']:\n    save_prefix = save_prefix+'_'+params_data['y_resid_strat']\n\nensure_dir_path_exists(dir_path = cache_path)\n\n\nuse_gpu_num = 0\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif use_gpu_num in [0, 1]: \n    torch.cuda.set_device(use_gpu_num)\nprint(f\"Using {device} device\")\n\nUsing cuda device\n\n\n\ndef _dist_scale_function(out, dist, decay_rate):\n    scale = 1/(1+decay_rate*dist)\n    out = round(scale * out)\n    out = max(1, out)\n    return out\n\n\ndef _expand_node_shortcut(vnn_helper, query = 'y_hat'):\n    # define new entries\n    if True in [True if e in vnn_helper.edge_dict.keys() else False for e in \n                [f'{query}_res_-2', f'{query}_res_-1']\n                ]:\n        print('Warning! New node name already exists! Overwriting existing node!')\n\n    # Add residual connection in graph\n    vnn_helper.edge_dict[f'{query}_res_-2'] = myvnn.edge_dict[query] \n    vnn_helper.edge_dict[f'{query}_res_-1'] = [f'{query}_res_-2']\n    vnn_helper.edge_dict[query]             = [f'{query}_res_-2', f'{query}_res_-1']\n\n    # Add new nodes, copying information from query node\n    vnn_helper.node_props[f'{query}_res_-2'] = vnn_helper.node_props[query] \n    vnn_helper.node_props[f'{query}_res_-1'] = vnn_helper.node_props[query]\n\n    return vnn_helper",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_yhat_02.html#load-data",
    "href": "zma_g2f_yhat_02.html#load-data",
    "title": "Visible Neural Network",
    "section": "Load Data",
    "text": "Load Data\n\n# Data Prep ----\nobs_geno_lookup          = get_data('obs_geno_lookup')\nphno                     = get_data('phno')\nACGT_gene_slice_list     = get_data('KEGG_slices')\nparsed_kegg_gene_entries = get_data('KEGG_entries')\n\n\n# make sure that the given y variable is there\nphno = phno.loc[(phno[y_var].notna()), ].copy()\nphno = phno.reset_index().drop(columns='index')\n\n\n# update obs_geno_lookup\n\ntmp = phno.reset_index().rename(columns={'index': 'Phno_Idx_new'}).loc[:, ['Phno_Idx_new', 'Geno_Idx']]\ntmp = pd.merge(tmp,\n          tmp.drop(columns='Phno_Idx_new').drop_duplicates().reset_index().rename(columns={'index': 'Phno_Idx_Orig_new'}))\ntmp = tmp.sort_values('Phno_Idx_new').reset_index(drop=True)\n\nobs_geno_lookup = tmp.to_numpy()\n\n\n# % missing\n# {e:phno[e].isna().mean() for e in list(phno)}\n\n\n# make holdout sets\nholdout_parents = params_data['holdout_parents']\n\n# create a mask for parent genotype\nmask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)\n\ntrain_mask = mask.sum(axis=1) == 0\ntest_mask  = mask.sum(axis=1) &gt; 0\n\ntrain_idx = train_mask.loc[train_mask].index\ntest_idx  = test_mask.loc[test_mask].index\n\n\n# convert y to residual if needed\n\nif params_data['y_resid'] == 'None':\n    pass\nelse:\n    if params_data['y_resid_strat'] == 'naive_mean':\n        # use only data in the training set (especially since testers will be more likely to be found across envs)\n        # get enviromental means, subtract from observed value\n        tmp = phno.loc[train_idx, ]\n        env_mean = tmp.groupby(['Env_Idx']\n                     ).agg(Env_Mean = (y_var, 'mean')\n                     ).reset_index()\n        tmp = phno.merge(env_mean)\n        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n        phno = tmp.drop(columns='Env_Mean')\n\n    if params_data['y_resid_strat'] == 'filter_mean':\n        # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations\n        def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],\n                                    year = 2014):\n            # Within each year what hybrids are most common?\n            tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')\n\n            all_envs = set(tmp.Env)\n            # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?\n            # site counts for sets which contain all envs\n            i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year &gt;= i), 'Env'])) == len(all_envs)])\n\n            before = len(set(tmp.loc[:, 'Hybrid']))\n            after  = len(set(tmp.loc[(tmp.Year &gt;= i), 'Hybrid']))\n            print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')\n            tmp = tmp.loc[(tmp.Year &gt;= i), ['Env', 'Hybrid']].reset_index(drop=True)\n            return tmp\n\n\n        tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]\n        filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) \n                          for i in list(set(phno.Year)) ]\n        env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')\n\n        env_mean = env_mean.groupby(['Env_Idx']\n                          ).agg(Env_Mean = (y_var, 'mean')\n                          ).reset_index()\n\n        tmp = phno.merge(env_mean)\n        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n        phno = tmp.drop(columns='Env_Mean')\n\n\n# center and y value data\nassert 0 == phno.loc[:, y_var].isna().sum()\n\ny = phno.loc[:, y_var]\n# use train index to prevent information leakage\ny_c = y[train_idx].mean()\ny_s = y[train_idx].std()\n\ny = (y - y_c)/y_s",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_yhat_02.html#fit-using-vnnhelper",
    "href": "zma_g2f_yhat_02.html#fit-using-vnnhelper",
    "title": "Visible Neural Network",
    "section": "Fit Using VNNHelper",
    "text": "Fit Using VNNHelper\n\ndef vnn_factory_1(parsed_kegg_gene_entries, params):\n\n    print(''.join('#' for i in range(80)))\n    print(params)\n    print(''.join('#' for i in range(80)))\n    \n    \n    default_out_nodes_inp  = params['default_out_nodes_inp' ]\n    default_out_nodes_edge = params['default_out_nodes_edge'] \n    default_out_nodes_out  = params['default_out_nodes_out' ]\n\n    default_drop_nodes_inp = params['default_drop_nodes_inp' ] \n    default_drop_nodes_edge= params['default_drop_nodes_edge'] \n    default_drop_nodes_out = params['default_drop_nodes_out' ] \n\n    default_reps_nodes_inp = params['default_reps_nodes_inp' ]\n    default_reps_nodes_edge= params['default_reps_nodes_edge']\n    default_reps_nodes_out = params['default_reps_nodes_out' ]\n\n\n\n    default_decay_rate = params['default_decay_rate' ]\n\n\n\n    # Clean up KEGG Pathways -------------------------------------------------------\n    # Same setup as above to create kegg_gene_brite\n    # Restrict to only those with pathway\n    kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n\n    # also require to have a non-empty path\n    kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n\n    print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n        )+'/'+str(len(parsed_kegg_gene_entries)\n        )+' Entries'\n        )\n    # kegg_gene_brite[1]['BRITE']['BRITE_PATHS']\n\n\n    kegg_connections = kegg_connections_build(kegg_gene_brite = kegg_gene_brite, \n                                            n_genes = len(kegg_gene_brite)) \n    kegg_connections = kegg_connections_clean(         kegg_connections = kegg_connections)\n    #TODO think about removing \n    # \"Not Included In\n    # Pathway Or Brite\"\n    # or reinstate 'Others'\n\n    kegg_connections = kegg_connections_append_y_hat(  kegg_connections = kegg_connections)\n    kegg_connections = kegg_connections_sanitize_names(kegg_connections = kegg_connections, \n                                                    replace_chars = {'.':'_'})\n\n\n    # Initialize helper for input nodes --------------------------------------------\n    myvnn = VNNHelper(edge_dict = kegg_connections)\n\n    # Get a mapping of brite names to tensor list index\n    find_names = myvnn.nodes_inp # e.g. ['100383860', '100278565', ... ]\n    lookup_dict = {}\n\n    # the only difference lookup_dict and brite_node_to_list_idx_dict above is that this is made using the full set of genes in the list \n    # whereas that is made using kegg_gene_brite which is a subset\n    for i in range(len(parsed_kegg_gene_entries)):\n        if 'BRITE' not in parsed_kegg_gene_entries[i].keys():\n            pass\n        elif parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'] == []:\n            pass\n        else:\n            name = parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'][0][-1]\n            if name in find_names:\n                lookup_dict[name] = i\n    # lookup_dict    \n\n    brite_node_to_list_idx_dict = {}\n    for i in range(len(kegg_gene_brite)):\n        brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i        \n\n    # Get the input sizes for the graph\n    size_in_zip = zip(myvnn.nodes_inp, [np.prod(ACGT_gene_slice_list[lookup_dict[e]].shape[1:]) for e  in myvnn.nodes_inp])\n\n    # Set node defaults ------------------------------------------------------------\n    # init input node sizes\n    myvnn.set_node_props(key = 'inp', node_val_zip = size_in_zip)\n\n    # init node output sizes\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_inp, [default_out_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_edge,[default_out_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_out, [default_out_nodes_out  for e in myvnn.nodes_out]))\n\n    # # options should be controlled by node_props\n    myvnn.set_node_props(key = 'flatten', node_val_zip = zip(myvnn.nodes_inp, [True for e in myvnn.nodes_inp]))\n\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_inp, [default_reps_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_edge,[default_reps_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_out, [default_reps_nodes_out  for e in myvnn.nodes_out]))\n\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_inp, [default_drop_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_edge,[default_drop_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_out, [default_drop_nodes_out  for e in myvnn.nodes_out]))\n\n\n    # Scale node outputs by distance -----------------------------------------------\n    dist = sparsevnn.core.vertex_from_end(\n        edge_dict = myvnn.edge_dict,\n        end =myvnn.dependancy_order[-1]\n    )\n\n    # overwrite node outputs with a size inversely proportional to distance from prediction node\n    for query in list(dist.keys()):\n        myvnn.node_props[query]['out'] = _dist_scale_function(\n            out = myvnn.node_props[query]['out'],\n            dist = dist[query],\n            decay_rate = default_decay_rate)\n        \n\n    # Expand out node replicates ---------------------------------------------------\n    nodes = [node for node in myvnn.dependancy_order if myvnn.node_props[node]['reps'] &gt; 1]\n\n    node_expansion_dict = {\n        node: [node if i==0 else f'{node}_{i}' for i in range(myvnn.node_props[node]['reps'])]\n        for node in nodes}\n    #   current       1st          2nd (new)      3rd (new)\n    # {'100798274': ['100798274', '100798274_1', '100798274_2'], ...\n\n    # the keys don't change here. The values will be updated and then new k:v will be inserted\n    myvnn.edge_dict = {k:[e if e not in node_expansion_dict.keys() \n        else node_expansion_dict[e][-1]\n        for e in myvnn.edge_dict[k] ] for k in myvnn.edge_dict}\n\n    # now insert connectsion to new nodes: A -&gt; A_rep_1 -&gt; A_rep_2\n    for node in node_expansion_dict:\n        for pair in zip(node_expansion_dict[node][1:], node_expansion_dict[node]):\n            myvnn.edge_dict[pair[0]] = [pair[1]]\n\n    # now add those new nodes\n    # create a new node for all the nodes\n    for node in node_expansion_dict:\n        for new_node in node_expansion_dict[node][1:]:\n            myvnn.node_props[new_node] = {k:myvnn.node_props[node][k] for k in myvnn.node_props[node] if k != 'inp'}\n\n    new_vnn = VNNHelper(edge_dict= myvnn.edge_dict)\n    new_vnn.node_props = myvnn.node_props\n    myvnn = new_vnn\n\n\n    # init edge node input size (propagate forward input/edge outpus)\n    myvnn.calc_edge_inp()\n\n    # replace lookup so that it matches the lenght of the input tensors\n    new_lookup_dict = {}\n    for i in range(len(myvnn.nodes_inp)):\n        new_lookup_dict[myvnn.nodes_inp[i]] = i\n    \n    return myvnn,  lookup_dict #new_lookup_dict\n\nmyvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = params)\n\n################################################################################\n{'default_out_nodes_inp': 1, 'default_out_nodes_edge': 1, 'default_out_nodes_out': 1, 'default_drop_nodes_inp': 0.0, 'default_drop_nodes_edge': 0.0, 'default_drop_nodes_out': 0.0, 'default_reps_nodes_inp': 1, 'default_reps_nodes_edge': 1, 'default_reps_nodes_out': 1, 'default_decay_rate': 1}\n################################################################################\nRetaining 43.53%, 6067/13939 Entries\nRemoved node \"Others\"\n\n\n\nCalculate nodes membership in each matrix and positions within each\n\ndef vnn_factory_2(vnn_helper, node_to_inp_num_dict):\n    myvnn = vnn_helper\n\n    node_props = myvnn.node_props\n    # Linear_block = Linear_block_reps,\n    edge_dict = myvnn.edge_dict\n    dependancy_order = myvnn.dependancy_order\n    node_to_inp_num_dict = new_lookup_dict\n\n    # Build dependancy dictionary --------------------------------------------------\n    # check dep order\n    tally = []\n    for d in dependancy_order:\n        if edge_dict[d] == []:\n            tally.append(d)\n        elif False not in [True if e in tally else False for e in edge_dict[d]]:\n            tally.append(d)\n        else:\n            print('error!')\n            break\n\n\n    # build output nodes \n    d_out = {0:[]}\n    for d in dependancy_order:\n        if edge_dict[d] == []:\n            d_out[min(d_out.keys())].append(d)\n        else:\n            # print((d, edge_dict[d]))\n\n            d_out_i = 1+max(sum([[key for key in d_out.keys() if e in d_out[key]]\n                    for e in edge_dict[d]], []))\n            \n            if d_out_i not in d_out.keys():\n                d_out[d_out_i] = []\n            d_out[d_out_i].append(d)\n\n\n    # build input nodes NOPE. THE PASSHTROUGHS! \n    d_eye = {}\n    tally = []\n    for i in range(max(d_out.keys()), min(d_out.keys()), -1):\n        # print(i)\n        nodes_needed = sum([edge_dict[e] for e in d_out[i]], [])+tally\n        # check against what is there and then dedupe\n        nodes_needed = [e for e in nodes_needed if e not in d_out[i-1]]\n        nodes_needed = list(set(nodes_needed))\n        tally = nodes_needed\n        d_eye[i] = nodes_needed\n\n    # d_inp[0]= d_out[0]\n    # [len(d_eye[i]) for i in d_eye.keys()]\n    # [(key, len(d_out[key])) for key in d_out.keys()]\n\n\n    dd = {}\n    for i in d_eye.keys():\n        dd[i] = {'out': d_out[i],\n                'inp': d_out[i-1],\n                'eye': d_eye[i]}\n    # plus special 0 layer that handles the snps\n        \n    dd[0] = {'out': d_out[0],\n            'inp': d_out[0],\n            'eye': []}\n\n\n    # check that the output nodes' inputs are satisfied by the same layer's inputs (inp and eye)\n    for i in dd.keys():\n        # out node in each\n        for e in dd[i]['out']:\n            # node depends in inp/eye\n            node_pass_list = [True if ee in dd[i]['inp']+dd[i]['eye'] else False \n                            for ee in edge_dict[e]]\n            if False not in node_pass_list:\n                pass\n            else:\n                print('exit') \n\n\n    # print(\"Layer\\t#In\\t#Out\")\n    # for i in range(min(dd.keys()), max(dd.keys())+1, 1):\n    #     node_in      = [node_props[e]['out'] for e in dd[i]['inp']+dd[i  ]['eye'] ]\n    #     if i == max(dd.keys()):\n    #         node_out = [node_props[e]['out'] for e in dd[i]['out'] ]\n    #     else:\n    #         node_out = [node_props[e]['out'] for e in dd[i]['out']+dd[i+1]['eye']]\n    #     print(f'{i}:\\t{sum(node_in)}\\t{sum(node_out)}')\n\n    M_list = [structured_layer_info(i = ii, node_groups = dd, node_props= node_props, edge_dict = edge_dict, as_sparse=True) for ii in range(0, max(dd.keys())+1)]\n    return M_list\n\n### Creating Structured Matrices for Layers\nM_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)\n\n\n\nSetup Dataloader using M_list\n\nlookup_dict = new_lookup_dict\n\nvals = get_data('KEGG_slices')\nvals = [torch.from_numpy(e).to(torch.float) for e in vals]\n# restrict to the tensors that will be used\nvals = torch.concat([vals[lookup_dict[i]].reshape(4926, -1) \n                     for i in M_list[0].row_inp\n                    #  for i in dd[0]['inp'] # matches\n                     ], axis = 1)\n\nvals = vals.to('cuda')\n\n\ntraining_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n    lookup_geno = torch.from_numpy(obs_geno_lookup),\n    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n    G =           vals,\n    G_type = 'raw',\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = batch_size,\n    shuffle = True \n)\n\nvalidation_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n    lookup_geno = torch.from_numpy(obs_geno_lookup),\n    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n    G =           vals,\n    G_type = 'raw',\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = batch_size,\n    shuffle = False \n)",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_yhat_02.html#structured-layer",
    "href": "zma_g2f_yhat_02.html#structured-layer",
    "title": "Visible Neural Network",
    "section": "Structured Layer",
    "text": "Structured Layer\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, layer_list):\n        super(NeuralNetwork, self).__init__()\n        self.layer_list = nn.ModuleList(layer_list)\n \n    def forward(self, x):\n        for l in self.layer_list:\n            x = l(x)\n        return x\n\n\ndef vnn_factory_3(M_list):\n    layer_list = []\n    for i in range(len(M_list)):\n        \n        apply_relu = None\n        if i+1 != len(M_list): # apply relu to all but the last layer\n            apply_relu = F.relu\n        \n\n        l = SparseLinearCustom(\n            M_list[i].weight.shape[1], # have to transpose this?\n            M_list[i].weight.shape[0],\n            connectivity   = torch.LongTensor(M_list[i].weight.coalesce().indices()),\n            custom_weights = M_list[i].weight.coalesce().values(), \n            custom_bias    = M_list[i].bias.clone().detach(), \n            weight_grad_bool = M_list[i].weight_grad_bool, \n            bias_grad_bool   = M_list[i].bias_grad_bool, #.to_sparse()#.indices()\n            dropout_p        = M_list[i].dropout_p,\n            nonlinear_transform= apply_relu\n            )\n\n        layer_list += [l]\n        \n    return layer_list",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_yhat_02.html#tiny-test-study",
    "href": "zma_g2f_yhat_02.html#tiny-test-study",
    "title": "Visible Neural Network",
    "section": "Tiny Test Study",
    "text": "Tiny Test Study\n\n\n\ntype\nvalue key\nvalue type\n\n\n\n\nrange\nbounds\nlist\n\n\nchoice\nvalues\nlist\n\n\nfixed\nvalue\natomic\n\n\n\n\n\n# this is a very funny trick. I'm going to call lighning from within Ax. \n# That way I can save out traces while also relying on Ax to choose new hyps. \n\n\ndef evaluate(parameterization):\n    # draw from global\n    # max_epoch = 20\n    # lightning_log_dir = \"test_tb\"\n    myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = parameterization)\n    M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)\n    layer_list =  vnn_factory_3(M_list = M_list)\n    model = NeuralNetwork(layer_list = layer_list)\n    \n    VNN = plDNN_general(model)  \n    optimizer = VNN.configure_optimizers()\n    logger = CSVLogger(lightning_log_dir, name=exp_name)\n    logger.log_hyperparams(params={\n        'params': parameterization\n    })\n\n    trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n    trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n\n\n    # if we were optimizing number of training epochs this would be an effective loss to use.\n    # trainer.callback_metrics['train_loss']\n    # float(trainer.callback_metrics['train_loss'])\n\n    # To potentially _overtrain_ models and still let the selction be based on their best possible performance,\n    # I'll use the lowest average error in an epoch\n    log_path = lightning_log_dir+'/'+exp_name\n    fls = os.listdir(log_path)\n    nums = [int(e.split('_')[-1]) for e in fls] \n\n    M = pd.read_csv(log_path+f\"/version_{max(nums)}/metrics.csv\")\n    M = M.loc[:, ['epoch', 'train_loss']].dropna()\n\n    M = M.groupby('epoch').agg(\n        train_loss = ('train_loss', 'mean'),\n        train_loss_sd = ('train_loss', 'std'),\n        ).reset_index()\n\n    train_metric = M.train_loss.min()\n    print(train_metric)\n    return {\"train_loss\": (train_metric, 0.0)}\n\n\n## Generated variables ====\njson_path = f\"./{lightning_log_dir}/{exp_name}.json\"\n\nloaded_json = False\nif os.path.exists(json_path): \n    ax_client = (AxClient.load_from_json_file(filepath = json_path))\n    loaded_json = True\n\nelse:\n    ax_client = AxClient()\n    ax_client.create_experiment(\n        name=exp_name,\n        parameters=params_list,\n        objectives={\"train_loss\": ObjectiveProperties(minimize=True)}\n    )\n\nrun_trials_bool = True\nif run_hyps_force == False:\n    if loaded_json: \n        # check if we've reached the max number of hyperparamters combinations to test\n        if max_hyps &lt;= (ax_client.generation_strategy.trials_as_df.index.max()+1):\n            run_trials_bool = False\n\nif run_trials_bool:\n    # run the trials\n    for i in range(run_hyps):\n        parameterization, trial_index = ax_client.get_next_trial()\n        # Local evaluation here can be replaced with deployment to external system.\n        ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameterization))\n\n    ax_client.save_to_json_file(filepath = json_path)\n\n[INFO 05-29 11:06:08] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the `verbose_logging` argument to `False`. Note that float values in the logs are rounded to 6 decimal points.\n[INFO 05-29 11:06:08] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='default_out_nodes_inp', parameter_type=INT, range=[1, 8]), RangeParameter(name='default_out_nodes_edge', parameter_type=INT, range=[1, 32]), FixedParameter(name='default_out_nodes_out', parameter_type=INT, value=1), RangeParameter(name='default_drop_nodes_inp', parameter_type=FLOAT, range=[0.01, 0.99]), RangeParameter(name='default_drop_nodes_edge', parameter_type=FLOAT, range=[0.01, 0.99]), RangeParameter(name='default_drop_nodes_out', parameter_type=FLOAT, range=[0.01, 0.99]), ChoiceParameter(name='default_reps_nodes_inp', parameter_type=INT, values=[1, 2, 3], is_ordered=True, sort_values=True), ChoiceParameter(name='default_reps_nodes_edge', parameter_type=INT, values=[1, 2, 3], is_ordered=True, sort_values=True), ChoiceParameter(name='default_reps_nodes_out', parameter_type=INT, values=[1, 2, 3], is_ordered=True, sort_values=True), ChoiceParameter(name='default_decay_rate', parameter_type=FLOAT, values=[0.0, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0], is_ordered=True, sort_values=True)], parameter_constraints=[]).\n[INFO 05-29 11:06:08] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are no unordered categorical parameters.\n[INFO 05-29 11:06:08] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=9 num_trials=None use_batch_trials=False\n[INFO 05-29 11:06:08] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=18\n[INFO 05-29 11:06:08] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=18\n[INFO 05-29 11:06:08] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n[INFO 05-29 11:06:08] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 18 trials, BoTorch for subsequent trials]). Iterations after 18 will take longer to generate due to model-fitting.\n[INFO 05-29 11:06:08] ax.service.ax_client: Generated new trial 0 with parameters {'default_out_nodes_inp': 4, 'default_out_nodes_edge': 22, 'default_drop_nodes_inp': 0.305327, 'default_drop_nodes_edge': 0.019265, 'default_drop_nodes_out': 0.033094, 'default_reps_nodes_inp': 3, 'default_reps_nodes_edge': 2, 'default_reps_nodes_out': 1, 'default_decay_rate': 0.5, 'default_out_nodes_out': 1} using model Sobol.\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:191: PossibleUserWarning:\n\nThe `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/kickd/miniconda3/envs/fastai/lib/python3.11/si ...\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type          | Params\n---------------------------------------\n0 | mod  | NeuralNetwork | 1.4 B \n---------------------------------------\n735 K     Trainable params\n1.4 B     Non-trainable params\n1.4 B     Total params\n5,600.519 Total estimated model params size (MB)\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n`Trainer.fit` stopped: `max_epochs=2` reached.\n[INFO 05-29 11:07:38] ax.service.ax_client: Completed trial 0 with data: {'train_loss': (0.959924, 0.0)}.\n[INFO 05-29 11:07:38] ax.service.ax_client: Generated new trial 1 with parameters {'default_out_nodes_inp': 6, 'default_out_nodes_edge': 16, 'default_drop_nodes_inp': 0.677411, 'default_drop_nodes_edge': 0.481724, 'default_drop_nodes_out': 0.976614, 'default_reps_nodes_inp': 2, 'default_reps_nodes_edge': 3, 'default_reps_nodes_out': 3, 'default_decay_rate': 7.0, 'default_out_nodes_out': 1} using model Sobol.\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:191: PossibleUserWarning:\n\nThe `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/kickd/miniconda3/envs/fastai/lib/python3.11/si ...\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type          | Params\n---------------------------------------\n0 | mod  | NeuralNetwork | 939 M \n---------------------------------------\n205 K     Trainable params\n939 M     Non-trainable params\n939 M     Total params\n3,757.907 Total estimated model params size (MB)\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n`Trainer.fit` stopped: `max_epochs=2` reached.\n[INFO 05-29 11:08:57] ax.service.ax_client: Completed trial 1 with data: {'train_loss': (0.901685, 0.0)}.\n[INFO 05-29 11:08:57] ax.service.ax_client: Generated new trial 2 with parameters {'default_out_nodes_inp': 7, 'default_out_nodes_edge': 24, 'default_drop_nodes_inp': 0.929816, 'default_drop_nodes_edge': 0.566801, 'default_drop_nodes_out': 0.483199, 'default_reps_nodes_inp': 1, 'default_reps_nodes_edge': 3, 'default_reps_nodes_out': 2, 'default_decay_rate': 0.1, 'default_out_nodes_out': 1} using model Sobol.\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:191: PossibleUserWarning:\n\nThe `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/kickd/miniconda3/envs/fastai/lib/python3.11/si ...\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type          | Params\n---------------------------------------\n0 | mod  | NeuralNetwork | 8.8 B \n---------------------------------------\n5.2 M     Trainable params\n8.8 B     Non-trainable params\n8.8 B     Total params\n35,148.677Total estimated model params size (MB)\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n`Trainer.fit` stopped: `max_epochs=2` reached.\n[INFO 05-29 11:10:58] ax.service.ax_client: Completed trial 2 with data: {'train_loss': (0.968677, 0.0)}.\n[INFO 05-29 11:10:58] ax.service.ax_client: Generated new trial 3 with parameters {'default_out_nodes_inp': 5, 'default_out_nodes_edge': 23, 'default_drop_nodes_inp': 0.886201, 'default_drop_nodes_edge': 0.616639, 'default_drop_nodes_out': 0.072637, 'default_reps_nodes_inp': 2, 'default_reps_nodes_edge': 3, 'default_reps_nodes_out': 1, 'default_decay_rate': 0.5, 'default_out_nodes_out': 1} using model Sobol.\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:191: PossibleUserWarning:\n\nThe `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/kickd/miniconda3/envs/fastai/lib/python3.11/si ...\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type          | Params\n---------------------------------------\n0 | mod  | NeuralNetwork | 2.1 B \n---------------------------------------\n1.2 M     Trainable params\n2.1 B     Non-trainable params\n2.1 B     Total params\n8,369.786 Total estimated model params size (MB)\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n`Trainer.fit` stopped: `max_epochs=2` reached.\n[INFO 05-29 11:12:14] ax.service.ax_client: Completed trial 3 with data: {'train_loss': (0.991319, 0.0)}.\n[INFO 05-29 11:12:14] ax.service.ax_client: Generated new trial 4 with parameters {'default_out_nodes_inp': 5, 'default_out_nodes_edge': 16, 'default_drop_nodes_inp': 0.629104, 'default_drop_nodes_edge': 0.672337, 'default_drop_nodes_out': 0.540011, 'default_reps_nodes_inp': 1, 'default_reps_nodes_edge': 3, 'default_reps_nodes_out': 2, 'default_decay_rate': 0.9, 'default_out_nodes_out': 1} using model Sobol.\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:191: PossibleUserWarning:\n\nThe `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/kickd/miniconda3/envs/fastai/lib/python3.11/si ...\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type          | Params\n---------------------------------------\n0 | mod  | NeuralNetwork | 1.1 B \n---------------------------------------\n376 K     Trainable params\n1.1 B     Non-trainable params\n1.1 B     Total params\n4,243.911 Total estimated model params size (MB)\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n`Trainer.fit` stopped: `max_epochs=2` reached.\n[INFO 05-29 11:13:16] ax.service.ax_client: Completed trial 4 with data: {'train_loss': (0.996643, 0.0)}.\n[INFO 05-29 11:13:16] ax.service.ax_client: Saved JSON-serialized state of optimization to `./../nbs_artifacts/zma_g2f_yhat_02/lightning/zma_g2f_yhat_02.json`.\n\n\n################################################################################\n{'default_out_nodes_inp': 4, 'default_out_nodes_edge': 22, 'default_drop_nodes_inp': 0.3053274953365326, 'default_drop_nodes_edge': 0.019265161678195, 'default_drop_nodes_out': 0.033094192780554296, 'default_reps_nodes_inp': 3, 'default_reps_nodes_edge': 2, 'default_reps_nodes_out': 1, 'default_decay_rate': 0.5, 'default_out_nodes_out': 1}\n################################################################################\nRetaining 43.53%, 6067/13939 Entries\nRemoved node \"Others\"\n0.959923729300499\n################################################################################\n{'default_out_nodes_inp': 6, 'default_out_nodes_edge': 16, 'default_drop_nodes_inp': 0.6774110351875424, 'default_drop_nodes_edge': 0.4817244858853519, 'default_drop_nodes_out': 0.9766144278645515, 'default_reps_nodes_inp': 2, 'default_reps_nodes_edge': 3, 'default_reps_nodes_out': 3, 'default_decay_rate': 7.0, 'default_out_nodes_out': 1}\n################################################################################\nRetaining 43.53%, 6067/13939 Entries\nRemoved node \"Others\"\n0.9016849547624588\n################################################################################\n{'default_out_nodes_inp': 7, 'default_out_nodes_edge': 24, 'default_drop_nodes_inp': 0.929816333912313, 'default_drop_nodes_edge': 0.566800861749798, 'default_drop_nodes_out': 0.4831992239318788, 'default_reps_nodes_inp': 1, 'default_reps_nodes_edge': 3, 'default_reps_nodes_out': 2, 'default_decay_rate': 0.1, 'default_out_nodes_out': 1}\n################################################################################\nRetaining 43.53%, 6067/13939 Entries\nRemoved node \"Others\"\n0.9686766028404236\n################################################################################\n{'default_out_nodes_inp': 5, 'default_out_nodes_edge': 23, 'default_drop_nodes_inp': 0.8862006087228655, 'default_drop_nodes_edge': 0.6166391163691878, 'default_drop_nodes_out': 0.0726371402107179, 'default_reps_nodes_inp': 2, 'default_reps_nodes_edge': 3, 'default_reps_nodes_out': 1, 'default_decay_rate': 0.5, 'default_out_nodes_out': 1}\n################################################################################\nRetaining 43.53%, 6067/13939 Entries\nRemoved node \"Others\"\n0.9913191080093384\n################################################################################\n{'default_out_nodes_inp': 5, 'default_out_nodes_edge': 16, 'default_drop_nodes_inp': 0.629103681370616, 'default_drop_nodes_edge': 0.6723371456936001, 'default_drop_nodes_out': 0.5400109588354826, 'default_reps_nodes_inp': 1, 'default_reps_nodes_edge': 3, 'default_reps_nodes_out': 2, 'default_decay_rate': 0.9, 'default_out_nodes_out': 1}\n################################################################################\nRetaining 43.53%, 6067/13939 Entries\nRemoved node \"Others\"\n0.9966433942317963\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ax_client.generation_strategy.trials_as_df#.tail()\n\n\n# render(ax_client.get_contour_plot())\n\n\n# render(ax_client.get_optimization_trace(objective_optimum=0.0))\n\n\n# If I need to check what tables are in the sqlite\n# import sqlite3\n# con = sqlite3.connect(\"./foo.db\")\n# cur = con.cursor()\n# # cur.execute(\".tables;\") # should work, doesn't\n# cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n# con.close()",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_yhat_01.html",
    "href": "zma_g2f_yhat_01.html",
    "title": "Visible Neural Network",
    "section": "",
    "text": "# Data ----\nfrom dataG2F.core import get_data\nfrom dataG2F.qol  import ensure_dir_path_exists\n\n# Data Utilities ----\nimport numpy  as np\nimport pandas as pd\n\nfrom EnvDL.dlfn import BigDataset, plDNN_general\nfrom EnvDL.sets import mask_parents\n\n# Model Building  ----\n## General ====\nimport torch\nfrom   torch import nn\nimport torch.nn.functional as F\nfrom   torch.utils.data import Dataset\nfrom   torch.utils.data import DataLoader\n\n## VNN ====\nimport sparsevnn\nfrom   sparsevnn.core import\\\n    VNNHelper, \\\n    structured_layer_info, \\\n    SparseLinearCustom\nfrom   sparsevnn.kegg import \\\n    kegg_connections_build, \\\n    kegg_connections_clean, \\\n    kegg_connections_append_y_hat, \\\n    kegg_connections_sanitize_names\n\n# Hyperparameter Tuning ----\nimport os # needed for checking history (saved by lightning) \n\n## Logging with Pytorch Lightning ====\nimport lightning.pytorch as pl\nfrom   lightning.pytorch.loggers import CSVLogger # used to save the history of each trial (used by ax)\n\n## Adaptive Experimentation Platform ====\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\nfrom ax.utils.notebook.plotting import init_notebook_plotting, render\n\n# For logging experiment results in sql database\nfrom ax.storage.sqa_store.db import init_engine_and_session_factory\nfrom ax.storage.sqa_store.db import get_engine, create_all_tables\nfrom ax.storage.sqa_store.save import save_experiment # saving\nfrom ax.storage.sqa_store.structs import DBSettings # loading\n# from ax.storage.sqa_store.load import load_experiment # loading alternate\ntorch.set_float32_matmul_precision('medium')\ninit_notebook_plotting()",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_yhat_01.html#setup",
    "href": "zma_g2f_yhat_01.html#setup",
    "title": "Visible Neural Network",
    "section": "Setup",
    "text": "Setup\n\ncache_path = '../nbs_artifacts/zma_g2f_yhat_01/'\n\n\n# Run settings: \nparams_run = {\n    'batch_size': 256,\n    'max_epoch' : 256,    \n}\n\n# data settings\nparams_data = {\n    'y_var': 'Yield_Mg_ha',\n    'y_resid': 'None', # None, Env, Geno\n    'y_resid_strat': 'None', # None, naive_mean, filter_mean, ...\n    'holdout_parents': [\n        ## 2022 ====\n        'LH244',\n        ## 2021 ====\n        'PHZ51',\n        # 'PHP02',\n        # 'PHK76',\n        ## 2019 ====\n        # 'PHT69',\n        'LH195',\n        ## 2017 ====\n        # 'PHW52',\n        # 'PHN82',\n        ## 2016 ====\n        # 'DK3IIH6',\n        ## 2015 ====\n        # 'PHB47',\n        # 'LH82',\n        ## 2014 ====\n        # 'LH198',\n        # 'LH185',\n        # 'PB80',\n        # 'CG102',\n ],    \n}\n\n\n## Settings ====\nrun_hyps = 75 \nrun_hyps_force = False # should we run more trials even if the target number has been reached?\nmax_hyps = 100\n\nparams_list = [    \n    ## Output Size ====\n    {\n    'name': 'default_out_nodes_inp',\n    'type': 'range',\n    'bounds': [1, 8],\n    'value_type': 'int',\n    'log_scale': False\n    },\n    {\n    'name': 'default_out_nodes_edge',\n    'type': 'range',\n    'bounds': [1, 32],\n    'value_type': 'int',\n    'log_scale': False\n    },\n    {\n    'name': 'default_out_nodes_out',\n    'type': 'fixed',\n    'value': 1,\n    'value_type': 'int',\n    'log_scale': False\n    },\n    ## Dropout ====\n    {\n    'name': 'default_drop_nodes_inp',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False\n    },\n    {\n    'name': 'default_drop_nodes_edge',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False\n    },\n    {\n    'name': 'default_drop_nodes_out',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False,\n    'sort_values':True\n    },\n    ## Node Repeats ====\n    {\n    'name': 'default_reps_nodes_inp',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    {\n    'name': 'default_reps_nodes_edge',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    {\n    'name': 'default_reps_nodes_out',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    ## Node Output Size Scaling ====\n    {\n    'name': 'default_decay_rate',\n    'type': 'choice',\n    'values': [0+(0.1*i) for i in range(10)]+[1.+(1*i) for i in range(11)],\n    'value_type': 'float',\n    'is_ordered': True,\n    'sort_values':True\n    }\n    ]\n\n\nlightning_log_dir = cache_path+\"lightning\"\nexp_name = [e for e in cache_path.split('/') if e != ''][-1]\n\n\n# parameterization is needed for setup. These values will be overwritten by Ax if tuning is occuring. \n# in this file I define params later. I've included it here to gurantee that we can merge other params dicts into it.\nparams = {\n'default_out_nodes_inp'  : 1,\n'default_out_nodes_edge' : 1,\n'default_out_nodes_out'  : 1,\n\n'default_drop_nodes_inp' : 0.0,\n'default_drop_nodes_edge': 0.0,\n'default_drop_nodes_out' : 0.0,\n\n'default_reps_nodes_inp' : 1,\n'default_reps_nodes_edge': 1,\n'default_reps_nodes_out' : 1,\n\n'default_decay_rate'     : 1\n}\n\ndefault_out_nodes_inp  = params['default_out_nodes_inp' ]\ndefault_out_nodes_edge = params['default_out_nodes_edge'] \ndefault_out_nodes_out  = params['default_out_nodes_out' ]\n\ndefault_drop_nodes_inp = params['default_drop_nodes_inp' ] \ndefault_drop_nodes_edge= params['default_drop_nodes_edge'] \ndefault_drop_nodes_out = params['default_drop_nodes_out' ] \n\ndefault_reps_nodes_inp = params['default_reps_nodes_inp' ]\ndefault_reps_nodes_edge= params['default_reps_nodes_edge']\ndefault_reps_nodes_out = params['default_reps_nodes_out' ]\n\ndefault_decay_rate = params['default_decay_rate' ]\n\n\nbatch_size = params_run['batch_size']\nmax_epoch  = params_run['max_epoch']\n\ny_var = params_data['y_var']\n\n\nsave_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n\nif 'None' != params_data['y_resid_strat']:\n    save_prefix = save_prefix+'_'+params_data['y_resid_strat']\n\nensure_dir_path_exists(dir_path = cache_path)\n\n\nuse_gpu_num = 0\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif use_gpu_num in [0, 1]: \n    torch.cuda.set_device(use_gpu_num)\nprint(f\"Using {device} device\")\n\n\ndef _dist_scale_function(out, dist, decay_rate):\n    scale = 1/(1+decay_rate*dist)\n    out = round(scale * out)\n    out = max(1, out)\n    return out\n\n\ndef _expand_node_shortcut(vnn_helper, query = 'y_hat'):\n    # define new entries\n    if True in [True if e in vnn_helper.edge_dict.keys() else False for e in \n                [f'{query}_res_-2', f'{query}_res_-1']\n                ]:\n        print('Warning! New node name already exists! Overwriting existing node!')\n\n    # Add residual connection in graph\n    vnn_helper.edge_dict[f'{query}_res_-2'] = myvnn.edge_dict[query] \n    vnn_helper.edge_dict[f'{query}_res_-1'] = [f'{query}_res_-2']\n    vnn_helper.edge_dict[query]             = [f'{query}_res_-2', f'{query}_res_-1']\n\n    # Add new nodes, copying information from query node\n    vnn_helper.node_props[f'{query}_res_-2'] = vnn_helper.node_props[query] \n    vnn_helper.node_props[f'{query}_res_-1'] = vnn_helper.node_props[query]\n\n    return vnn_helper",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_yhat_01.html#load-data",
    "href": "zma_g2f_yhat_01.html#load-data",
    "title": "Visible Neural Network",
    "section": "Load Data",
    "text": "Load Data\n\n# Data Prep ----\nobs_geno_lookup          = get_data('obs_geno_lookup')\nphno                     = get_data('phno')\nACGT_gene_slice_list     = get_data('KEGG_slices')\nparsed_kegg_gene_entries = get_data('KEGG_entries')\n\n\n# make holdout sets\nholdout_parents = params_data['holdout_parents']\n\n# create a mask for parent genotype\nmask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)\n\ntrain_mask = mask.sum(axis=1) == 0\ntest_mask  = mask.sum(axis=1) &gt; 0\n\ntrain_idx = train_mask.loc[train_mask].index\ntest_idx  = test_mask.loc[test_mask].index\n\n\n# convert y to residual if needed\n\nif params_data['y_resid'] == 'None':\n    pass\nelse:\n    if params_data['y_resid_strat'] == 'naive_mean':\n        # use only data in the training set (especially since testers will be more likely to be found across envs)\n        # get enviromental means, subtract from observed value\n        tmp = phno.loc[train_idx, ]\n        env_mean = tmp.groupby(['Env_Idx']\n                     ).agg(Env_Mean = (y_var, 'mean')\n                     ).reset_index()\n        tmp = phno.merge(env_mean)\n        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n        phno = tmp.drop(columns='Env_Mean')\n\n    if params_data['y_resid_strat'] == 'filter_mean':\n        # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations\n        def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],\n                                    year = 2014):\n            # Within each year what hybrids are most common?\n            tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')\n\n            all_envs = set(tmp.Env)\n            # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?\n            # site counts for sets which contain all envs\n            i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year &gt;= i), 'Env'])) == len(all_envs)])\n\n            before = len(set(tmp.loc[:, 'Hybrid']))\n            after  = len(set(tmp.loc[(tmp.Year &gt;= i), 'Hybrid']))\n            print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')\n            tmp = tmp.loc[(tmp.Year &gt;= i), ['Env', 'Hybrid']].reset_index(drop=True)\n            return tmp\n\n\n        tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]\n        filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) \n                          for i in list(set(phno.Year)) ]\n        env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')\n\n        env_mean = env_mean.groupby(['Env_Idx']\n                          ).agg(Env_Mean = (y_var, 'mean')\n                          ).reset_index()\n\n        tmp = phno.merge(env_mean)\n        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n        phno = tmp.drop(columns='Env_Mean')\n\n\n# center and y value data\nassert 0 == phno.loc[:, y_var].isna().sum()\n\ny = phno.loc[:, y_var]\n# use train index to prevent information leakage\ny_c = y[train_idx].mean()\ny_s = y[train_idx].std()\n\ny = (y - y_c)/y_s",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_yhat_01.html#fit-using-vnnhelper",
    "href": "zma_g2f_yhat_01.html#fit-using-vnnhelper",
    "title": "Visible Neural Network",
    "section": "Fit Using VNNHelper",
    "text": "Fit Using VNNHelper\n\ndef vnn_factory_1(parsed_kegg_gene_entries, params):\n\n    print(''.join('#' for i in range(80)))\n    print(params)\n    print(''.join('#' for i in range(80)))\n    \n    \n    default_out_nodes_inp  = params['default_out_nodes_inp' ]\n    default_out_nodes_edge = params['default_out_nodes_edge'] \n    default_out_nodes_out  = params['default_out_nodes_out' ]\n\n    default_drop_nodes_inp = params['default_drop_nodes_inp' ] \n    default_drop_nodes_edge= params['default_drop_nodes_edge'] \n    default_drop_nodes_out = params['default_drop_nodes_out' ] \n\n    default_reps_nodes_inp = params['default_reps_nodes_inp' ]\n    default_reps_nodes_edge= params['default_reps_nodes_edge']\n    default_reps_nodes_out = params['default_reps_nodes_out' ]\n\n\n\n    default_decay_rate = params['default_decay_rate' ]\n\n\n\n    # Clean up KEGG Pathways -------------------------------------------------------\n    # Same setup as above to create kegg_gene_brite\n    # Restrict to only those with pathway\n    kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n\n    # also require to have a non-empty path\n    kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n\n    print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n        )+'/'+str(len(parsed_kegg_gene_entries)\n        )+' Entries'\n        )\n    # kegg_gene_brite[1]['BRITE']['BRITE_PATHS']\n\n\n    kegg_connections = kegg_connections_build(kegg_gene_brite = kegg_gene_brite, \n                                            n_genes = len(kegg_gene_brite)) \n    kegg_connections = kegg_connections_clean(         kegg_connections = kegg_connections)\n    #TODO think about removing \n    # \"Not Included In\n    # Pathway Or Brite\"\n    # or reinstate 'Others'\n\n    kegg_connections = kegg_connections_append_y_hat(  kegg_connections = kegg_connections)\n    kegg_connections = kegg_connections_sanitize_names(kegg_connections = kegg_connections, \n                                                    replace_chars = {'.':'_'})\n\n\n    # Initialize helper for input nodes --------------------------------------------\n    myvnn = VNNHelper(edge_dict = kegg_connections)\n\n    # Get a mapping of brite names to tensor list index\n    find_names = myvnn.nodes_inp # e.g. ['100383860', '100278565', ... ]\n    lookup_dict = {}\n\n    # the only difference lookup_dict and brite_node_to_list_idx_dict above is that this is made using the full set of genes in the list \n    # whereas that is made using kegg_gene_brite which is a subset\n    for i in range(len(parsed_kegg_gene_entries)):\n        if 'BRITE' not in parsed_kegg_gene_entries[i].keys():\n            pass\n        elif parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'] == []:\n            pass\n        else:\n            name = parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'][0][-1]\n            if name in find_names:\n                lookup_dict[name] = i\n    # lookup_dict    \n\n    brite_node_to_list_idx_dict = {}\n    for i in range(len(kegg_gene_brite)):\n        brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i        \n\n    # Get the input sizes for the graph\n    size_in_zip = zip(myvnn.nodes_inp, [np.prod(ACGT_gene_slice_list[lookup_dict[e]].shape[1:]) for e  in myvnn.nodes_inp])\n\n    # Set node defaults ------------------------------------------------------------\n    # init input node sizes\n    myvnn.set_node_props(key = 'inp', node_val_zip = size_in_zip)\n\n    # init node output sizes\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_inp, [default_out_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_edge,[default_out_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_out, [default_out_nodes_out  for e in myvnn.nodes_out]))\n\n    # # options should be controlled by node_props\n    myvnn.set_node_props(key = 'flatten', node_val_zip = zip(myvnn.nodes_inp, [True for e in myvnn.nodes_inp]))\n\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_inp, [default_reps_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_edge,[default_reps_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_out, [default_reps_nodes_out  for e in myvnn.nodes_out]))\n\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_inp, [default_drop_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_edge,[default_drop_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_out, [default_drop_nodes_out  for e in myvnn.nodes_out]))\n\n\n    # Scale node outputs by distance -----------------------------------------------\n    dist = sparsevnn.core.vertex_from_end(\n        edge_dict = myvnn.edge_dict,\n        end =myvnn.dependancy_order[-1]\n    )\n\n    # overwrite node outputs with a size inversely proportional to distance from prediction node\n    for query in list(dist.keys()):\n        myvnn.node_props[query]['out'] = _dist_scale_function(\n            out = myvnn.node_props[query]['out'],\n            dist = dist[query],\n            decay_rate = default_decay_rate)\n        \n\n    # Expand out node replicates ---------------------------------------------------\n    nodes = [node for node in myvnn.dependancy_order if myvnn.node_props[node]['reps'] &gt; 1]\n\n    node_expansion_dict = {\n        node: [node if i==0 else f'{node}_{i}' for i in range(myvnn.node_props[node]['reps'])]\n        for node in nodes}\n    #   current       1st          2nd (new)      3rd (new)\n    # {'100798274': ['100798274', '100798274_1', '100798274_2'], ...\n\n    # the keys don't change here. The values will be updated and then new k:v will be inserted\n    myvnn.edge_dict = {k:[e if e not in node_expansion_dict.keys() \n        else node_expansion_dict[e][-1]\n        for e in myvnn.edge_dict[k] ] for k in myvnn.edge_dict}\n\n    # now insert connectsion to new nodes: A -&gt; A_rep_1 -&gt; A_rep_2\n    for node in node_expansion_dict:\n        for pair in zip(node_expansion_dict[node][1:], node_expansion_dict[node]):\n            myvnn.edge_dict[pair[0]] = [pair[1]]\n\n    # now add those new nodes\n    # create a new node for all the nodes\n    for node in node_expansion_dict:\n        for new_node in node_expansion_dict[node][1:]:\n            myvnn.node_props[new_node] = {k:myvnn.node_props[node][k] for k in myvnn.node_props[node] if k != 'inp'}\n\n    new_vnn = VNNHelper(edge_dict= myvnn.edge_dict)\n    new_vnn.node_props = myvnn.node_props\n    myvnn = new_vnn\n\n\n    # init edge node input size (propagate forward input/edge outpus)\n    myvnn.calc_edge_inp()\n\n    # replace lookup so that it matches the lenght of the input tensors\n    new_lookup_dict = {}\n    for i in range(len(myvnn.nodes_inp)):\n        new_lookup_dict[myvnn.nodes_inp[i]] = i\n    \n    return myvnn,  lookup_dict #new_lookup_dict\n\nmyvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = params)\n\n\nCalculate nodes membership in each matrix and positions within each\n\ndef vnn_factory_2(vnn_helper, node_to_inp_num_dict):\n    myvnn = vnn_helper\n\n    node_props = myvnn.node_props\n    # Linear_block = Linear_block_reps,\n    edge_dict = myvnn.edge_dict\n    dependancy_order = myvnn.dependancy_order\n    node_to_inp_num_dict = new_lookup_dict\n\n    # Build dependancy dictionary --------------------------------------------------\n    # check dep order\n    tally = []\n    for d in dependancy_order:\n        if edge_dict[d] == []:\n            tally.append(d)\n        elif False not in [True if e in tally else False for e in edge_dict[d]]:\n            tally.append(d)\n        else:\n            print('error!')\n            break\n\n\n    # build output nodes \n    d_out = {0:[]}\n    for d in dependancy_order:\n        if edge_dict[d] == []:\n            d_out[min(d_out.keys())].append(d)\n        else:\n            # print((d, edge_dict[d]))\n\n            d_out_i = 1+max(sum([[key for key in d_out.keys() if e in d_out[key]]\n                    for e in edge_dict[d]], []))\n            \n            if d_out_i not in d_out.keys():\n                d_out[d_out_i] = []\n            d_out[d_out_i].append(d)\n\n\n    # build input nodes NOPE. THE PASSHTROUGHS! \n    d_eye = {}\n    tally = []\n    for i in range(max(d_out.keys()), min(d_out.keys()), -1):\n        # print(i)\n        nodes_needed = sum([edge_dict[e] for e in d_out[i]], [])+tally\n        # check against what is there and then dedupe\n        nodes_needed = [e for e in nodes_needed if e not in d_out[i-1]]\n        nodes_needed = list(set(nodes_needed))\n        tally = nodes_needed\n        d_eye[i] = nodes_needed\n\n    # d_inp[0]= d_out[0]\n    # [len(d_eye[i]) for i in d_eye.keys()]\n    # [(key, len(d_out[key])) for key in d_out.keys()]\n\n\n    dd = {}\n    for i in d_eye.keys():\n        dd[i] = {'out': d_out[i],\n                'inp': d_out[i-1],\n                'eye': d_eye[i]}\n    # plus special 0 layer that handles the snps\n        \n    dd[0] = {'out': d_out[0],\n            'inp': d_out[0],\n            'eye': []}\n\n\n    # check that the output nodes' inputs are satisfied by the same layer's inputs (inp and eye)\n    for i in dd.keys():\n        # out node in each\n        for e in dd[i]['out']:\n            # node depends in inp/eye\n            node_pass_list = [True if ee in dd[i]['inp']+dd[i]['eye'] else False \n                            for ee in edge_dict[e]]\n            if False not in node_pass_list:\n                pass\n            else:\n                print('exit') \n\n\n    # print(\"Layer\\t#In\\t#Out\")\n    # for i in range(min(dd.keys()), max(dd.keys())+1, 1):\n    #     node_in      = [node_props[e]['out'] for e in dd[i]['inp']+dd[i  ]['eye'] ]\n    #     if i == max(dd.keys()):\n    #         node_out = [node_props[e]['out'] for e in dd[i]['out'] ]\n    #     else:\n    #         node_out = [node_props[e]['out'] for e in dd[i]['out']+dd[i+1]['eye']]\n    #     print(f'{i}:\\t{sum(node_in)}\\t{sum(node_out)}')\n\n    M_list = [structured_layer_info(i = ii, node_groups = dd, node_props= node_props, edge_dict = edge_dict, as_sparse=True) for ii in range(0, max(dd.keys())+1)]\n    return M_list\n\n### Creating Structured Matrices for Layers\nM_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)\n\n\n\nSetup Dataloader using M_list\n\nlookup_dict = new_lookup_dict\n\nvals = get_data('KEGG_slices')\nvals = [torch.from_numpy(e).to(torch.float) for e in vals]\n# restrict to the tensors that will be used\nvals = torch.concat([vals[lookup_dict[i]].reshape(4926, -1) \n                     for i in M_list[0].row_inp\n                    #  for i in dd[0]['inp'] # matches\n                     ], axis = 1)\n\nvals = vals.to('cuda')\n\n\ntraining_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n    lookup_geno = torch.from_numpy(obs_geno_lookup),\n    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n    G =           vals,\n    G_type = 'raw',\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = batch_size,\n    shuffle = True \n)\n\nvalidation_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n    lookup_geno = torch.from_numpy(obs_geno_lookup),\n    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n    G =           vals,\n    G_type = 'raw',\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = batch_size,\n    shuffle = False \n)",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_yhat_01.html#structured-layer",
    "href": "zma_g2f_yhat_01.html#structured-layer",
    "title": "Visible Neural Network",
    "section": "Structured Layer",
    "text": "Structured Layer\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, layer_list):\n        super(NeuralNetwork, self).__init__()\n        self.layer_list = nn.ModuleList(layer_list)\n \n    def forward(self, x):\n        for l in self.layer_list:\n            x = l(x)\n        return x\n\n\ndef vnn_factory_3(M_list):\n    layer_list = []\n    for i in range(len(M_list)):\n        \n        apply_relu = None\n        if i+1 != len(M_list): # apply relu to all but the last layer\n            apply_relu = F.relu\n        \n\n        l = SparseLinearCustom(\n            M_list[i].weight.shape[1], # have to transpose this?\n            M_list[i].weight.shape[0],\n            connectivity   = torch.LongTensor(M_list[i].weight.coalesce().indices()),\n            custom_weights = M_list[i].weight.coalesce().values(), \n            custom_bias    = M_list[i].bias.clone().detach(), \n            weight_grad_bool = M_list[i].weight_grad_bool, \n            bias_grad_bool   = M_list[i].bias_grad_bool, #.to_sparse()#.indices()\n            dropout_p        = M_list[i].dropout_p,\n            nonlinear_transform= apply_relu\n            )\n\n        layer_list += [l]\n        \n    return layer_list",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_g2f_yhat_01.html#tiny-test-study",
    "href": "zma_g2f_yhat_01.html#tiny-test-study",
    "title": "Visible Neural Network",
    "section": "Tiny Test Study",
    "text": "Tiny Test Study\n\n\n\ntype\nvalue key\nvalue type\n\n\n\n\nrange\nbounds\nlist\n\n\nchoice\nvalues\nlist\n\n\nfixed\nvalue\natomic\n\n\n\n\n\n# this is a very funny trick. I'm going to call lighning from within Ax. \n# That way I can save out traces while also relying on Ax to choose new hyps. \n\n\ndef evaluate(parameterization):\n    # draw from global\n    # max_epoch = 20\n    # lightning_log_dir = \"test_tb\"\n    myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = parameterization)\n    M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)\n    layer_list =  vnn_factory_3(M_list = M_list)\n    model = NeuralNetwork(layer_list = layer_list)\n    \n    VNN = plDNN_general(model)  \n    optimizer = VNN.configure_optimizers()\n    logger = CSVLogger(lightning_log_dir, name=exp_name)\n    logger.log_hyperparams(params={\n        'params': parameterization\n    })\n\n    trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n    trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n\n\n    # if we were optimizing number of training epochs this would be an effective loss to use.\n    # trainer.callback_metrics['train_loss']\n    # float(trainer.callback_metrics['train_loss'])\n\n    # To potentially _overtrain_ models and still let the selction be based on their best possible performance,\n    # I'll use the lowest average error in an epoch\n    log_path = lightning_log_dir+'/'+exp_name\n    fls = os.listdir(log_path)\n    nums = [int(e.split('_')[-1]) for e in fls] \n\n    M = pd.read_csv(log_path+f\"/version_{max(nums)}/metrics.csv\")\n    M = M.loc[:, ['epoch', 'train_loss']].dropna()\n\n    M = M.groupby('epoch').agg(\n        train_loss = ('train_loss', 'mean'),\n        train_loss_sd = ('train_loss', 'std'),\n        ).reset_index()\n\n    train_metric = M.train_loss.min()\n    print(train_metric)\n    return {\"train_loss\": (train_metric, 0.0)}\n\n\n## Generated variables ====\n# using sql database\nsql_url = \"sqlite:///\"+f\"./{lightning_log_dir}/{exp_name}.sqlite\"\n\n# If the database exists, load it and begin from there\nloaded_db = False\nif os.path.exists(sql_url.split('///')[-1]): # must cleave off the sql dialect prefix\n    # alternate way to load an experiment (after `init_engine_and_session_factory` has been run)\n    # experiment = load_experiment(exp_name) # if this doesn't work, check if the database is named something else and try that.\n    db_settings = DBSettings(url=sql_url)\n    # Instead of URL, can provide a `creator function`; can specify custom encoders/decoders if necessary.\n    ax_client = AxClient(db_settings=db_settings)\n    ax_client.load_experiment_from_database(exp_name)\n    loaded_db = True\n\nelse:\n    ax_client = AxClient()\n    ax_client.create_experiment(\n        name=exp_name,\n        parameters=params_list,\n        objectives={\"train_loss\": ObjectiveProperties(minimize=True)}\n    )\n\nrun_trials_bool = True\nif run_hyps_force == False:\n    if loaded_db: \n        # check if we've reached the max number of hyperparamters combinations to test\n        if max_hyps &lt;= (ax_client.generation_strategy.trials_as_df.index.max()+1):\n            run_trials_bool = False\n\nif run_trials_bool:\n    # run the trials\n    for i in range(run_hyps):\n        parameterization, trial_index = ax_client.get_next_trial()\n        # Local evaluation here can be replaced with deployment to external system.\n        ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameterization))\n\n    if loaded_db == False:\n        init_engine_and_session_factory(url=sql_url)\n        engine = get_engine()\n        create_all_tables(engine)\n\n    # save the trials\n    experiment = ax_client.experiment\n    save_experiment(experiment)\n\n\nax_client.generation_strategy.trials_as_df#.tail()\n\n\n# render(ax_client.get_contour_plot())\n\n\nrender(ax_client.get_optimization_trace(objective_optimum=0.0))\n\n\n# If I need to check what tables are in the sqlite\n# import sqlite3\n# con = sqlite3.connect(\"./foo.db\")\n# cur = con.cursor()\n# # cur.execute(\".tables;\") # should work, doesn't\n# cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n# con.close()",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "gmx_yhat_01.html",
    "href": "gmx_yhat_01.html",
    "title": "Visible Neural Network",
    "section": "",
    "text": "# Data ----\nfrom dataGMX.core import get_data # &lt;- Soybean Data\nfrom dataG2F.qol  import ensure_dir_path_exists\n\n# Data Utilities ----\nimport numpy  as np\nimport pandas as pd\n\nfrom EnvDL.dlfn import BigDataset, plDNN_general\nfrom EnvDL.sets import mask_columns\n\n# Model Building  ----\n## General ====\nimport torch\nfrom   torch import nn\nimport torch.nn.functional as F\nfrom   torch.utils.data import Dataset\nfrom   torch.utils.data import DataLoader\n\n## VNN ====\nimport sparsevnn\nfrom   sparsevnn.core import\\\n    VNNHelper, \\\n    structured_layer_info, \\\n    SparseLinearCustom\nfrom   sparsevnn.kegg import \\\n    kegg_connections_build, \\\n    kegg_connections_clean, \\\n    kegg_connections_append_y_hat, \\\n    kegg_connections_sanitize_names\n\n# Hyperparameter Tuning ----\nimport os # needed for checking history (saved by lightning) \n\n## Logging with Pytorch Lightning ====\nimport lightning.pytorch as pl\nfrom   lightning.pytorch.loggers import CSVLogger # used to save the history of each trial (used by ax)\n\n## Adaptive Experimentation Platform ====\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\nfrom ax.utils.notebook.plotting import init_notebook_plotting, render\n\n# For logging experiment results in sql database\nfrom ax.storage.sqa_store.db import init_engine_and_session_factory\nfrom ax.storage.sqa_store.db import get_engine, create_all_tables\nfrom ax.storage.sqa_store.save import save_experiment # saving\nfrom ax.storage.sqa_store.structs import DBSettings # loading\n# from ax.storage.sqa_store.load import load_experiment # loading alternate\ntorch.set_float32_matmul_precision('medium')\ninit_notebook_plotting()\n\n\n\n\n[INFO 05-16 14:46:48] ax.utils.notebook.plotting: Injecting Plotly library into cell. Do not overwrite or delete cell.\n[INFO 05-16 14:46:48] ax.utils.notebook.plotting: Please see\n    (https://ax.dev/tutorials/visualizations.html#Fix-for-plots-that-are-not-rendering)\n    if visualizations are not rendering.",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "gmx_yhat_01.html#setup",
    "href": "gmx_yhat_01.html#setup",
    "title": "Visible Neural Network",
    "section": "Setup",
    "text": "Setup\n\ncache_path = '../nbs_artifacts/gmx_yhat_01/'\n\n\n# Run settings: \nparams_run = {\n    'batch_size': 256, \n    'max_epoch' : 512, # 256, \n}\n\n# data settings\nparams_data = {\n    'y_var': [\n        'sdwt100', 'ProteinDry', 'OilDry', 'AshDry', 'FiberDry', 'LysineDry', \n        'CysteineDry', 'MethionineDry', 'ThreonineDry', 'TryptophanDry', 'IsoleucineDry', \n        'LeucineDry', 'HistidineDry', 'PhenylalanineDry', 'ValineDry', 'AlanineDry', \n        'ArginineDry', 'AsparticacidDry', 'GlutamicacidDry', 'GlycineDry', 'ProlineDry', \n        'SerineDry', 'TyrosineDry', 'SucroseDry', 'Linolenic', 'Linoleic', 'Oleic', \n        'Palmitic', 'Moisture', 'RaffinoseDry', 'StachyoseDry', 'Stearic', 'SeedAS', \n        'SeedPL', 'SeedW', 'SeedLWR', 'SeedCS', 'SeedL', 'SampleWeight', 'B11', 'Na23', \n        'Mg26', 'Al27', 'P31', 'S34', 'K39', 'Ca44', 'Mn55', 'Ni60', 'Cu63', 'Zn66', \n        'Fe54', 'Co59', 'Se78', 'Rb85', 'Sr88', 'Mo98', 'Cd111', 'As75'],\n    'y_resid': 'None', # None, Env, Geno\n    'y_resid_strat': 'None', # None, naive_mean, filter_mean, ...\n    'holdout_parents': { # For this dataset a percent of genotypes are randomly held out. The seed value makes this reproducible. May switch to similarity based approach.\n        'rng_seed': 9874325,\n        'pr': 0.2} \n}\n\n\n## Settings ====\nrun_hyps = 25 \nrun_hyps_force = False # should we run more trials even if the target number has been reached?\nmax_hyps = 50\n\nparams_list = [    \n    ## Output Size ====\n    {\n    'name': 'default_out_nodes_inp',\n    'type': 'range',\n    'bounds': [1, 8],\n    'value_type': 'int',\n    'log_scale': False\n    },\n    {\n    'name': 'default_out_nodes_edge',\n    'type': 'range',\n    'bounds': [1, 32],\n    'value_type': 'int',\n    'log_scale': False\n    },\n    {\n    'name': 'default_out_nodes_out',\n    'type': 'fixed',\n    'value': len(params_data['y_var']) if type(params_data['y_var']) else 1,\n    'value_type': 'int',\n    'log_scale': False\n    },\n    ## Dropout ====\n    {\n    'name': 'default_drop_nodes_inp',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False\n    },\n    {\n    'name': 'default_drop_nodes_edge',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False\n    },\n    {\n    'name': 'default_drop_nodes_out',\n    'type': 'range',\n    'bounds': [0.01, 0.99],\n    'value_type': 'float',\n    'log_scale': False,\n    'sort_values':True\n    },\n    ## Node Repeats ====\n    {\n    'name': 'default_reps_nodes_inp',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    {\n    'name': 'default_reps_nodes_edge',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    {\n    'name': 'default_reps_nodes_out',\n    'type': 'choice',\n    'values': [1, 2, 3],\n    'value_type': 'int',\n    'is_ordered': True,\n    'sort_values':True\n    },\n    ## Node Output Size Scaling ====\n    {\n    'name': 'default_decay_rate',\n    'type': 'choice',\n    'values': [0+(0.1*i) for i in range(10)]+[1.+(1*i) for i in range(11)],\n    'value_type': 'float',\n    'is_ordered': True,\n    'sort_values':True\n    }\n    ]\n\n\nlightning_log_dir = cache_path+\"lightning\"\nexp_name = [e for e in cache_path.split('/') if e != ''][-1]\n\n\n# parameterization is needed for setup. These values will be overwritten by Ax if tuning is occuring. \n# in this file I define params later. I've included it here to gurantee that we can merge other params dicts into it.\nparams = {\n'default_out_nodes_inp'  : 1,\n'default_out_nodes_edge' : 1,\n'default_out_nodes_out'  : len(params_data['y_var']) if type(params_data['y_var']) else 1,\n\n'default_drop_nodes_inp' : 0.0,\n'default_drop_nodes_edge': 0.0,\n'default_drop_nodes_out' : 0.0,\n\n'default_reps_nodes_inp' : 1,\n'default_reps_nodes_edge': 1,\n'default_reps_nodes_out' : 1,\n\n'default_decay_rate'     : 1\n}\n\ndefault_out_nodes_inp  = params['default_out_nodes_inp' ]\ndefault_out_nodes_edge = params['default_out_nodes_edge'] \ndefault_out_nodes_out  = params['default_out_nodes_out' ]\n\ndefault_drop_nodes_inp = params['default_drop_nodes_inp' ] \ndefault_drop_nodes_edge= params['default_drop_nodes_edge'] \ndefault_drop_nodes_out = params['default_drop_nodes_out' ] \n\ndefault_reps_nodes_inp = params['default_reps_nodes_inp' ]\ndefault_reps_nodes_edge= params['default_reps_nodes_edge']\ndefault_reps_nodes_out = params['default_reps_nodes_out' ]\n\ndefault_decay_rate = params['default_decay_rate' ]\n\n\nbatch_size = params_run['batch_size']\nmax_epoch  = params_run['max_epoch']\n\ny_var = params_data['y_var']\n\n\nsave_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n\nif 'None' != params_data['y_resid_strat']:\n    save_prefix = save_prefix+'_'+params_data['y_resid_strat']\n\nensure_dir_path_exists(dir_path = cache_path)\n\n\nuse_gpu_num = 0\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif use_gpu_num in [0, 1]: \n    torch.cuda.set_device(use_gpu_num)\nprint(f\"Using {device} device\")\n\nUsing cuda device\n\n\n\ndef _dist_scale_function(out, dist, decay_rate):\n    scale = 1/(1+decay_rate*dist)\n    out = round(scale * out)\n    out = max(1, out)\n    return out\n\n\ndef _expand_node_shortcut(vnn_helper, query = 'y_hat'):\n    # define new entries\n    if True in [True if e in vnn_helper.edge_dict.keys() else False for e in \n                [f'{query}_res_-2', f'{query}_res_-1']\n                ]:\n        print('Warning! New node name already exists! Overwriting existing node!')\n\n    # Add residual connection in graph\n    vnn_helper.edge_dict[f'{query}_res_-2'] = myvnn.edge_dict[query] \n    vnn_helper.edge_dict[f'{query}_res_-1'] = [f'{query}_res_-2']\n    vnn_helper.edge_dict[query]             = [f'{query}_res_-2', f'{query}_res_-1']\n\n    # Add new nodes, copying information from query node\n    vnn_helper.node_props[f'{query}_res_-2'] = vnn_helper.node_props[query] \n    vnn_helper.node_props[f'{query}_res_-1'] = vnn_helper.node_props[query]\n\n    return vnn_helper",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "gmx_yhat_01.html#load-data",
    "href": "gmx_yhat_01.html#load-data",
    "title": "Visible Neural Network",
    "section": "Load Data",
    "text": "Load Data\n\n# Data Prep ----\nobs_geno_lookup          = get_data('obs_geno_lookup')\nphno                     = get_data('phno')\nACGT_gene_slice_list     = get_data('KEGG_slices')\nparsed_kegg_gene_entries = get_data('KEGG_entries')\n\n\n# this dataset is very close to balanced wrt to the genotypes with 9-18 obs. \n# This code will turn a seed value and percent to be held out into a list of genotypse\nrng = np.random.default_rng( params_data['holdout_parents']['rng_seed'] )\ntmp = get_data(name = 'phno')\n\ntaxa = sorted(list(set(tmp.Taxa)))\nrng.shuffle(taxa)\ntaxa = pd.DataFrame(taxa).reset_index().rename(columns={0:'Taxa', 'index':'DrawOrder'})\n\ntmp = pd.merge(taxa, tmp).loc[:,['DrawOrder', 'Taxa']].assign(n=lambda x: 1).groupby(['DrawOrder', 'Taxa']).count().reset_index()\ntmp['cdf'] = tmp['n'].cumsum(0)/tmp['n'].sum()\n# filter\nholdout_parents = list(tmp.loc[(tmp.cdf &lt;= params_data['holdout_parents']['pr'] ), 'Taxa'])\n\n\n# make holdout sets\n# holdout_parents = params_data['holdout_parents']\n\n# create a mask for parent genotype\nmask = mask_columns(df= phno, col_name= 'Taxa', holdouts= holdout_parents)\n\n\ntrain_mask = mask.sum(axis=1) == 0\ntest_mask  = mask.sum(axis=1) &gt; 0\n\ntrain_idx = train_mask.loc[train_mask].index\ntest_idx  = test_mask.loc[test_mask].index\n\n\n# convert y to residual if needed\n\nif params_data['y_resid'] == 'None':\n    pass\n# TODO update for GMX\n# else:\n#     if params_data['y_resid_strat'] == 'naive_mean':\n#         # use only data in the training set (especially since testers will be more likely to be found across envs)\n#         # get enviromental means, subtract from observed value\n#         tmp = phno.loc[train_idx, ]\n#         env_mean = tmp.groupby(['Env_Idx']\n#                      ).agg(Env_Mean = (y_var, 'mean')\n#                      ).reset_index()\n#         tmp = phno.merge(env_mean)\n#         tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n#         phno = tmp.drop(columns='Env_Mean')\n\n#     if params_data['y_resid_strat'] == 'filter_mean':\n#         # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations\n#         def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],\n#                                     year = 2014):\n#             # Within each year what hybrids are most common?\n#             tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')\n\n#             all_envs = set(tmp.Env)\n#             # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?\n#             # site counts for sets which contain all envs\n#             i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year &gt;= i), 'Env'])) == len(all_envs)])\n\n#             before = len(set(tmp.loc[:, 'Hybrid']))\n#             after  = len(set(tmp.loc[(tmp.Year &gt;= i), 'Hybrid']))\n#             print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')\n#             tmp = tmp.loc[(tmp.Year &gt;= i), ['Env', 'Hybrid']].reset_index(drop=True)\n#             return tmp\n\n\n#         tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]\n#         filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) \n#                           for i in list(set(phno.Year)) ]\n#         env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')\n\n#         env_mean = env_mean.groupby(['Env_Idx']\n#                           ).agg(Env_Mean = (y_var, 'mean')\n#                           ).reset_index()\n\n#         tmp = phno.merge(env_mean)\n#         tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n#         phno = tmp.drop(columns='Env_Mean')\n\n\n# center and y value data\nassert 0 == phno.loc[:, y_var].isna().sum().sum() # second sum is for multiple y_vars\n\ny = phno.loc[:, y_var].to_numpy() # added to make multiple ys work\n# use train index to prevent information leakage\ny_c = y[train_idx].mean(axis=0)\ny_s = y[train_idx].std(axis=0)\n\ny = (y - y_c)/y_s",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "gmx_yhat_01.html#fit-using-vnnhelper",
    "href": "gmx_yhat_01.html#fit-using-vnnhelper",
    "title": "Visible Neural Network",
    "section": "Fit Using VNNHelper",
    "text": "Fit Using VNNHelper\n\ndef vnn_factory_1(parsed_kegg_gene_entries, params):\n\n    print(''.join('#' for i in range(80)))\n    print(params)\n    print(''.join('#' for i in range(80)))\n    \n    \n    default_out_nodes_inp  = params['default_out_nodes_inp' ]\n    default_out_nodes_edge = params['default_out_nodes_edge'] \n    default_out_nodes_out  = params['default_out_nodes_out' ]\n\n    default_drop_nodes_inp = params['default_drop_nodes_inp' ] \n    default_drop_nodes_edge= params['default_drop_nodes_edge'] \n    default_drop_nodes_out = params['default_drop_nodes_out' ] \n\n    default_reps_nodes_inp = params['default_reps_nodes_inp' ]\n    default_reps_nodes_edge= params['default_reps_nodes_edge']\n    default_reps_nodes_out = params['default_reps_nodes_out' ]\n\n\n\n    default_decay_rate = params['default_decay_rate' ]\n\n\n\n    # Clean up KEGG Pathways -------------------------------------------------------\n    # Same setup as above to create kegg_gene_brite\n    # Restrict to only those with pathway\n    kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n\n    # also require to have a non-empty path\n    kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n\n    print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n        )+'/'+str(len(parsed_kegg_gene_entries)\n        )+' Entries'\n        )\n    # kegg_gene_brite[1]['BRITE']['BRITE_PATHS']\n\n\n    kegg_connections = kegg_connections_build(kegg_gene_brite = kegg_gene_brite, \n                                            n_genes = len(kegg_gene_brite)) \n    kegg_connections = kegg_connections_clean(         kegg_connections = kegg_connections)\n    #TODO think about removing \n    # \"Not Included In\n    # Pathway Or Brite\"\n    # or reinstate 'Others'\n\n    kegg_connections = kegg_connections_append_y_hat(  kegg_connections = kegg_connections)\n    kegg_connections = kegg_connections_sanitize_names(kegg_connections = kegg_connections, \n                                                    replace_chars = {'.':'_'})\n\n\n    # Initialize helper for input nodes --------------------------------------------\n    myvnn = VNNHelper(edge_dict = kegg_connections)\n\n    # Get a mapping of brite names to tensor list index\n    find_names = myvnn.nodes_inp # e.g. ['100383860', '100278565', ... ]\n    lookup_dict = {}\n\n    # the only difference lookup_dict and brite_node_to_list_idx_dict above is that this is made using the full set of genes in the list \n    # whereas that is made using kegg_gene_brite which is a subset\n    for i in range(len(parsed_kegg_gene_entries)):\n        if 'BRITE' not in parsed_kegg_gene_entries[i].keys():\n            pass\n        elif parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'] == []:\n            pass\n        else:\n            name = parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'][0][-1]\n            if name in find_names:\n                lookup_dict[name] = i\n    # lookup_dict    \n\n    brite_node_to_list_idx_dict = {}\n    for i in range(len(kegg_gene_brite)):\n        brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i        \n\n    # Get the input sizes for the graph\n    size_in_zip = zip(myvnn.nodes_inp, [np.prod(ACGT_gene_slice_list[lookup_dict[e]].shape[1:]) for e  in myvnn.nodes_inp])\n\n    # Set node defaults ------------------------------------------------------------\n    # init input node sizes\n    myvnn.set_node_props(key = 'inp', node_val_zip = size_in_zip)\n\n    # init node output sizes\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_inp, [default_out_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_edge,[default_out_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_out, [default_out_nodes_out  for e in myvnn.nodes_out]))\n\n    # # options should be controlled by node_props\n    myvnn.set_node_props(key = 'flatten', node_val_zip = zip(myvnn.nodes_inp, [True for e in myvnn.nodes_inp]))\n\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_inp, [default_reps_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_edge,[default_reps_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_out, [default_reps_nodes_out  for e in myvnn.nodes_out]))\n\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_inp, [default_drop_nodes_inp  for e in myvnn.nodes_inp]))\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_edge,[default_drop_nodes_edge for e in myvnn.nodes_edge]))\n    myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_out, [default_drop_nodes_out  for e in myvnn.nodes_out]))\n\n\n    # Scale node outputs by distance -----------------------------------------------\n    dist = sparsevnn.core.vertex_from_end(\n        edge_dict = myvnn.edge_dict,\n        end =myvnn.dependancy_order[-1]\n    )\n\n    # overwrite node outputs with a size inversely proportional to distance from prediction node\n    for query in list(dist.keys()):\n        myvnn.node_props[query]['out'] = _dist_scale_function(\n            out = myvnn.node_props[query]['out'],\n            dist = dist[query],\n            decay_rate = default_decay_rate)\n        \n\n    # Expand out node replicates ---------------------------------------------------\n    # kegg_connections_expanded = sparsevnn.core.expand_edge_dict(vnn_helper = myvnn, edge_dict = myvnn.edge_dict)\n\n    # ======================================================= #\n    # one place to add residual connections would be here.    #\n    # edit the links before instatinating the new VNNHelper   #\n    # the important thing to do is to edit the graph before   #\n    # calulating the inputs for each node.                    #\n    # ======================================================= #\n\n            # # expand then copy over the properties that have already been defined.\n            # myvnn_exp = VNNHelper(edge_dict = kegg_connections_expanded)\n\n            # import re\n            # for new_key in list(myvnn_exp.node_props.keys()):\n            #     if new_key in myvnn.node_props.keys():\n            #         # copy directly\n            #         myvnn_exp.node_props[new_key] = myvnn.node_props[new_key]\n            #     else:\n            #         # check for a key that matches the query key after removing the replicate information\n            #         query = new_key \n            #         suffix = re.findall('_rep_\\d+$', query)[0]\n\n            #         query = query.removesuffix(suffix)\n            #         if query in myvnn.node_props.keys():\n            #             myvnn_exp.node_props[new_key] = myvnn.node_props[query]\n            #         else:\n            #             print(f'WARNING: no entry {query} found for {new_key}') \n\n            # # now main vnn is the expanded version\n            # myvnn = myvnn_exp\n\n\n            # # Cleanup ----------------------------------------------------------------------\n            # # now the original VNNHelper isn't needed\n            # myvnn = myvnn_exp\n\n\n    # expand out graph.\n#     update_edge_links = {}\n#     nodes = myvnn.dependancy_order\n\n#     for query in [e for e in reversed(nodes)]:\n#         if myvnn.node_props[query]['reps'] == 1:\n#             pass\n#         else:\n#             reps = myvnn.node_props[query]['reps']\n#             # set to 1 so that we can copy all the props (except input) over\n#             myvnn.node_props[query]['reps'] = 1\n            \n#             for i in range(reps-1,0,-1):\n#                 if i == 0:\n#                     # no replicates\n#                     pass\n#                 else:\n#                     if i == 1:\n#                         # print({f'{query}_{i}':[f'{query}']})\n#                         update_edge_links[f'{query}_{i}'] = f'{query}'\n#                     else:\n#                         # print({f'{query}_{i}':[f'{query}_{i-1}']})\n#                         update_edge_links[f'{query}_{i}'] = f'{query}_{i-1}'\n\n#                     # copy over all properties except input (input will either be set for the data or calculated on the fly)\n#                     myvnn.node_props[f'{query}_{i}'] = {k:myvnn.node_props[query][k] for k in myvnn.node_props[query] if k != 'inp'}\n\n\n#     # Now there should be new nodes in the helper but the links need to be updated to point to the right names. \n\n#     if True:\n#         # update existing links\n#         # create a lookup dictionary to map the old names to new names\n#         old_to_new = {update_edge_links[k]:k for k in update_edge_links}\n#         # old_to_new\n\n#         for k in myvnn.edge_dict:\n#             myvnn.edge_dict[k] = [e if e not in old_to_new.keys() else old_to_new[e] for e in myvnn.edge_dict[k]]\n\n#         # add in new nodes\n#         for k in update_edge_links:\n#             myvnn.edge_dict[k] = [update_edge_links[k]]\n\n#         # overwrite dependancy order\n#         # myvnn.dependancy_order = VNNHelper(edge_dict= myvnn.edge_dict).dependancy_order\n# # myvnn_updated = VNNHelper(edge_dict= myvnn.edge_dict)\n# # myvnn_updated.node_props = myvnn.node_props\n#         # myvnn = myvnn_updated\n\n    # expand out graph.\n    update_edge_links = {}\n    nodes = [node for node in myvnn.dependancy_order if myvnn.node_props[node]['reps'] &gt; 1]\n\n    node_expansion_dict = {\n        node: [node if i==0 else f'{node}_{i}' for i in range(myvnn.node_props[node]['reps'])]\n        for node in nodes}\n    #   current       1st          2nd (new)      3rd (new)\n    # {'100798274': ['100798274', '100798274_1', '100798274_2'], ...\n\n    # the keys don't change here. The values will be updated and then new k:v will be inserted\n    myvnn.edge_dict = {k:[e if e not in node_expansion_dict.keys() \n        else node_expansion_dict[e][-1]\n        for e in myvnn.edge_dict[k] ] for k in myvnn.edge_dict}\n\n    # now insert connectsion to new nodes: A -&gt; A_rep_1 -&gt; A_rep_2\n    for node in node_expansion_dict:\n        for pair in zip(node_expansion_dict[node][1:], node_expansion_dict[node]):\n            myvnn.edge_dict[pair[0]] = [pair[1]]\n\n    # now add those new nodes\n    # create a new node for all the nodes\n    for node in node_expansion_dict:\n        for new_node in node_expansion_dict[node][1:]:\n            myvnn.node_props[new_node] = {k:myvnn.node_props[node][k] for k in myvnn.node_props[node] if k != 'inp'}\n\n    new_vnn = VNNHelper(edge_dict= myvnn.edge_dict)\n    new_vnn.node_props = myvnn.node_props\n    myvnn = new_vnn\n\n\n    # init edge node input size (propagate forward input/edge outpus)\n    myvnn.calc_edge_inp()\n\n    # replace lookup so that it matches the lenght of the input tensors\n    new_lookup_dict = {}\n    for i in range(len(myvnn.nodes_inp)):\n        new_lookup_dict[myvnn.nodes_inp[i]] = i\n    \n    return myvnn,  lookup_dict #new_lookup_dict\n\nmyvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = params)\n\n################################################################################\n{'default_out_nodes_inp': 1, 'default_out_nodes_edge': 1, 'default_out_nodes_out': 59, 'default_drop_nodes_inp': 0.0, 'default_drop_nodes_edge': 0.0, 'default_drop_nodes_out': 0.0, 'default_reps_nodes_inp': 1, 'default_reps_nodes_edge': 1, 'default_reps_nodes_out': 1, 'default_decay_rate': 1}\n################################################################################\nRetaining 41.160000000000004%, 3039/7383 Entries\nRemoved node \"Others\"\n\n\n\nCalculate nodes membership in each matrix and positions within each\n\ndef vnn_factory_2(vnn_helper, node_to_inp_num_dict):\n    myvnn = vnn_helper\n\n    node_props = myvnn.node_props\n    # Linear_block = Linear_block_reps,\n    edge_dict = myvnn.edge_dict\n    dependancy_order = myvnn.dependancy_order\n    node_to_inp_num_dict = new_lookup_dict\n\n    # Build dependancy dictionary --------------------------------------------------\n    # check dep order\n    tally = []\n    for d in dependancy_order:\n        if edge_dict[d] == []:\n            tally.append(d)\n        elif False not in [True if e in tally else False for e in edge_dict[d]]:\n            tally.append(d)\n        else:\n            print('error!')\n            break\n\n\n    # build output nodes \n    d_out = {0:[]}\n    for d in dependancy_order:\n        if edge_dict[d] == []:\n            d_out[min(d_out.keys())].append(d)\n        else:\n            # print((d, edge_dict[d]))\n\n            d_out_i = 1+max(sum([[key for key in d_out.keys() if e in d_out[key]]\n                    for e in edge_dict[d]], []))\n            \n            if d_out_i not in d_out.keys():\n                d_out[d_out_i] = []\n            d_out[d_out_i].append(d)\n\n\n    # build input nodes NOPE. THE PASSHTROUGHS! \n    d_eye = {}\n    tally = []\n    for i in range(max(d_out.keys()), min(d_out.keys()), -1):\n        # print(i)\n        nodes_needed = sum([edge_dict[e] for e in d_out[i]], [])+tally\n        # check against what is there and then dedupe\n        nodes_needed = [e for e in nodes_needed if e not in d_out[i-1]]\n        nodes_needed = list(set(nodes_needed))\n        tally = nodes_needed\n        d_eye[i] = nodes_needed\n\n    # d_inp[0]= d_out[0]\n    # [len(d_eye[i]) for i in d_eye.keys()]\n    # [(key, len(d_out[key])) for key in d_out.keys()]\n\n\n    dd = {}\n    for i in d_eye.keys():\n        dd[i] = {'out': d_out[i],\n                'inp': d_out[i-1],\n                'eye': d_eye[i]}\n    # plus special 0 layer that handles the snps\n        \n    dd[0] = {'out': d_out[0],\n            'inp': d_out[0],\n            'eye': []}\n\n\n    # check that the output nodes' inputs are satisfied by the same layer's inputs (inp and eye)\n    for i in dd.keys():\n        # out node in each\n        for e in dd[i]['out']:\n            # node depends in inp/eye\n            node_pass_list = [True if ee in dd[i]['inp']+dd[i]['eye'] else False \n                            for ee in edge_dict[e]]\n            if False not in node_pass_list:\n                pass\n            else:\n                print('exit') \n\n\n    # print(\"Layer\\t#In\\t#Out\")\n    # for i in range(min(dd.keys()), max(dd.keys())+1, 1):\n    #     node_in      = [node_props[e]['out'] for e in dd[i]['inp']+dd[i  ]['eye'] ]\n    #     if i == max(dd.keys()):\n    #         node_out = [node_props[e]['out'] for e in dd[i]['out'] ]\n    #     else:\n    #         node_out = [node_props[e]['out'] for e in dd[i]['out']+dd[i+1]['eye']]\n    #     print(f'{i}:\\t{sum(node_in)}\\t{sum(node_out)}')\n\n    M_list = [structured_layer_info(i = ii, node_groups = dd, node_props= node_props, edge_dict = edge_dict, as_sparse=True) for ii in range(0, max(dd.keys())+1)]\n    return M_list\n\n### Creating Structured Matrices for Layers\nM_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)\n\n\n\nSetup Dataloader using M_list\n\nlookup_dict = new_lookup_dict\n\nvals = get_data('KEGG_slices')\nvals = [torch.from_numpy(e).to(torch.float) for e in vals]\n# restrict to the tensors that will be used\nvals = torch.concat([vals[lookup_dict[i]].reshape(vals[0].shape[0], -1) \n                     for i in M_list[0].row_inp\n                    #  for i in dd[0]['inp'] # matches\n                     ], axis = 1)\n\nvals = vals.to('cuda')\n\n\ntraining_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n    lookup_geno = torch.from_numpy(obs_geno_lookup),\n    y =           torch.from_numpy(y).to(torch.float32),\n    G =           vals,\n    G_type = 'raw',\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = batch_size,\n    shuffle = True \n)\n\nvalidation_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n    lookup_geno = torch.from_numpy(obs_geno_lookup),\n    y =           torch.from_numpy(y).to(torch.float32),\n    G =           vals,\n    G_type = 'raw',\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = batch_size,\n    shuffle = False \n)",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "gmx_yhat_01.html#structured-layer",
    "href": "gmx_yhat_01.html#structured-layer",
    "title": "Visible Neural Network",
    "section": "Structured Layer",
    "text": "Structured Layer\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, layer_list):\n        super(NeuralNetwork, self).__init__()\n        self.layer_list = nn.ModuleList(layer_list)\n \n    def forward(self, x):\n        for l in self.layer_list:\n            x = l(x)\n        return x\n\n\ndef vnn_factory_3(M_list):\n    layer_list = []\n    for i in range(len(M_list)):\n        \n        apply_relu = None\n        if i+1 != len(M_list): # apply relu to all but the last layer\n            apply_relu = F.relu\n        \n\n        l = SparseLinearCustom(\n            M_list[i].weight.shape[1], # have to transpose this?\n            M_list[i].weight.shape[0],\n            connectivity   = torch.LongTensor(M_list[i].weight.coalesce().indices()),\n            custom_weights = M_list[i].weight.coalesce().values(), \n            custom_bias    = M_list[i].bias.clone().detach(), \n            weight_grad_bool = M_list[i].weight_grad_bool, \n            bias_grad_bool   = M_list[i].bias_grad_bool, #.to_sparse()#.indices()\n            dropout_p        = M_list[i].dropout_p,\n            nonlinear_transform= apply_relu\n            )\n\n        layer_list += [l]\n        \n    return layer_list",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "gmx_yhat_01.html#tiny-test-study",
    "href": "gmx_yhat_01.html#tiny-test-study",
    "title": "Visible Neural Network",
    "section": "Tiny Test Study",
    "text": "Tiny Test Study\n\n\n\ntype\nvalue key\nvalue type\n\n\n\n\nrange\nbounds\nlist\n\n\nchoice\nvalues\nlist\n\n\nfixed\nvalue\natomic\n\n\n\n\n\n# this is a very funny trick. I'm going to call lighning from within Ax. \n# That way I can save out traces while also relying on Ax to choose new hyps. \n\n\ndef evaluate(parameterization):\n    # draw from global\n    # max_epoch = 20\n    # lightning_log_dir = \"test_tb\"\n    myvnn, new_lookup_dict = vnn_factory_1(parsed_kegg_gene_entries = parsed_kegg_gene_entries, params = parameterization)\n    M_list = vnn_factory_2(vnn_helper = myvnn, node_to_inp_num_dict = new_lookup_dict)\n    layer_list =  vnn_factory_3(M_list = M_list)\n    model = NeuralNetwork(layer_list = layer_list)\n    \n    VNN = plDNN_general(model)  \n    optimizer = VNN.configure_optimizers()\n    logger = CSVLogger(lightning_log_dir, name=exp_name)\n    logger.log_hyperparams(params={\n        'params': parameterization\n    })\n\n    trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n    trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n\n\n    # if we were optimizing number of training epochs this would be an effective loss to use.\n    # trainer.callback_metrics['train_loss']\n    # float(trainer.callback_metrics['train_loss'])\n\n    # To potentially _overtrain_ models and still let the selction be based on their best possible performance,\n    # I'll use the lowest average error in an epoch\n    log_path = lightning_log_dir+'/'+exp_name\n    fls = os.listdir(log_path)\n    nums = [int(e.split('_')[-1]) for e in fls] \n\n    M = pd.read_csv(log_path+f\"/version_{max(nums)}/metrics.csv\")\n    M = M.loc[:, ['epoch', 'train_loss']].dropna()\n\n    M = M.groupby('epoch').agg(\n        train_loss = ('train_loss', 'mean'),\n        train_loss_sd = ('train_loss', 'std'),\n        ).reset_index()\n\n    train_metric = M.train_loss.min()\n    print(train_metric)\n    return {\"train_loss\": (train_metric, 0.0)}\n\n\n## Generated variables ====\n# using sql database\nsql_url = \"sqlite:///\"+f\"./{lightning_log_dir}/{exp_name}.sqlite\"\n\n# If the database exists, load it and begin from there\nloaded_db = False\nif os.path.exists(sql_url.split('///')[-1]): # must cleave off the sql dialect prefix\n    # alternate way to load an experiment (after `init_engine_and_session_factory` has been run)\n    # experiment = load_experiment(exp_name) # if this doesn't work, check if the database is named something else and try that.\n    db_settings = DBSettings(url=sql_url)\n    # Instead of URL, can provide a `creator function`; can specify custom encoders/decoders if necessary.\n    ax_client = AxClient(db_settings=db_settings)\n    ax_client.load_experiment_from_database(exp_name)\n    loaded_db = True\n\nelse:\n    ax_client = AxClient()\n    ax_client.create_experiment(\n        name=exp_name,\n        parameters=params_list,\n        objectives={\"train_loss\": ObjectiveProperties(minimize=True)}\n    )\n\nrun_trials_bool = True\nif run_hyps_force == False:\n    if loaded_db: \n        # check if we've reached the max number of hyperparamters combinations to test\n        if max_hyps &lt;= (ax_client.generation_strategy.trials_as_df.index.max()+1):\n            run_trials_bool = False\n\nif run_trials_bool:\n    # run the trials\n    for i in range(run_hyps):\n        parameterization, trial_index = ax_client.get_next_trial()\n        # Local evaluation here can be replaced with deployment to external system.\n        ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameterization))\n\n    if loaded_db == False:\n        init_engine_and_session_factory(url=sql_url)\n        engine = get_engine()\n        create_all_tables(engine)\n\n    # save the trials\n    experiment = ax_client.experiment\n    save_experiment(experiment)\n\n[INFO 05-16 14:46:55] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the `verbose_logging` argument to `False`. Note that float values in the logs are rounded to 6 decimal points.\n[INFO 05-16 14:46:55] ax.service.utils.with_db_settings_base: Loading experiment and generation strategy (with reduced state: False)...\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/ax/storage/sqa_store/decoder.py:323: AxParameterWarning:\n\n`sort_values` is not specified for `ChoiceParameter` \"default_reps_nodes_inp\". Defaulting to `True` for parameters of `ParameterType` INT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/ax/storage/sqa_store/decoder.py:323: AxParameterWarning:\n\n`sort_values` is not specified for `ChoiceParameter` \"default_reps_nodes_edge\". Defaulting to `True` for parameters of `ParameterType` INT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/ax/storage/sqa_store/decoder.py:323: AxParameterWarning:\n\n`sort_values` is not specified for `ChoiceParameter` \"default_reps_nodes_out\". Defaulting to `True` for parameters of `ParameterType` INT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/ax/storage/sqa_store/decoder.py:323: AxParameterWarning:\n\n`sort_values` is not specified for `ChoiceParameter` \"default_decay_rate\". Defaulting to `True` for parameters of `ParameterType` FLOAT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n\n[INFO 05-16 14:46:55] ax.service.utils.with_db_settings_base: Loaded experiment gmx_yhat_01 & 4 trials in 0.19 seconds.\n[INFO 05-16 14:46:55] ax.service.utils.with_db_settings_base: Loaded generation strategy for experiment gmx_yhat_01 in 0.0 seconds.\n[INFO 05-16 14:46:55] ax.service.ax_client: Loaded Experiment(gmx_yhat_01).\n[INFO 05-16 14:46:55] ax.service.ax_client: Using generation strategy associated with the loaded experiment: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 18 trials, BoTorch for subsequent trials]).\n[INFO 05-16 14:46:55] ax.service.ax_client: Generated new trial 4 with parameters {'default_out_nodes_inp': 8, 'default_out_nodes_edge': 20, 'default_drop_nodes_inp': 0.136353, 'default_drop_nodes_edge': 0.082529, 'default_drop_nodes_out': 0.231387, 'default_reps_nodes_inp': 3, 'default_reps_nodes_edge': 1, 'default_reps_nodes_out': 1, 'default_decay_rate': 2.0, 'default_out_nodes_out': 59} using model Sobol.\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:191: PossibleUserWarning:\n\nThe `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/kickd/miniconda3/envs/fastai/lib/python3.11/si ...\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type          | Params\n---------------------------------------\n0 | mod  | NeuralNetwork | 90.4 M\n---------------------------------------\n110 K     Trainable params\n90.3 M    Non-trainable params\n90.4 M    Total params\n361.755   Total estimated model params size (MB)\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: PossibleUserWarning:\n\nThe number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n\n`Trainer.fit` stopped: `max_epochs=64` reached.\n[INFO 05-16 14:47:20] ax.service.ax_client: Completed trial 4 with data: {'train_loss': (0.951867, 0.0)}.\n[INFO 05-16 14:47:20] ax.service.ax_client: Generated new trial 5 with parameters {'default_out_nodes_inp': 6, 'default_out_nodes_edge': 20, 'default_drop_nodes_inp': 0.196063, 'default_drop_nodes_edge': 0.686589, 'default_drop_nodes_out': 0.620125, 'default_reps_nodes_inp': 2, 'default_reps_nodes_edge': 3, 'default_reps_nodes_out': 3, 'default_decay_rate': 0.5, 'default_out_nodes_out': 59} using model Sobol.\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:191: PossibleUserWarning:\n\nThe `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/kickd/miniconda3/envs/fastai/lib/python3.11/si ...\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type          | Params\n---------------------------------------\n0 | mod  | NeuralNetwork | 526 M \n---------------------------------------\n690 K     Trainable params\n525 M     Non-trainable params\n526 M     Total params\n2,104.163 Total estimated model params size (MB)\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: PossibleUserWarning:\n\nThe 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: PossibleUserWarning:\n\nThe number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n\n`Trainer.fit` stopped: `max_epochs=64` reached.\n[INFO 05-16 14:48:03] ax.service.ax_client: Completed trial 5 with data: {'train_loss': (0.908138, 0.0)}.\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/ax/storage/sqa_store/decoder.py:323: AxParameterWarning:\n\n`sort_values` is not specified for `ChoiceParameter` \"default_reps_nodes_inp\". Defaulting to `True` for parameters of `ParameterType` INT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/ax/storage/sqa_store/decoder.py:323: AxParameterWarning:\n\n`sort_values` is not specified for `ChoiceParameter` \"default_reps_nodes_edge\". Defaulting to `True` for parameters of `ParameterType` INT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/ax/storage/sqa_store/decoder.py:323: AxParameterWarning:\n\n`sort_values` is not specified for `ChoiceParameter` \"default_reps_nodes_out\". Defaulting to `True` for parameters of `ParameterType` INT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n\n/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/ax/storage/sqa_store/decoder.py:323: AxParameterWarning:\n\n`sort_values` is not specified for `ChoiceParameter` \"default_decay_rate\". Defaulting to `True` for parameters of `ParameterType` FLOAT. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n\n\n\n################################################################################\n{'default_out_nodes_inp': 8, 'default_out_nodes_edge': 20, 'default_drop_nodes_inp': 0.13635292295366527, 'default_drop_nodes_edge': 0.08252909595146775, 'default_drop_nodes_out': 0.2313869714178145, 'default_reps_nodes_inp': 3, 'default_reps_nodes_edge': 1, 'default_reps_nodes_out': 1, 'default_decay_rate': 2.0, 'default_out_nodes_out': 59}\n################################################################################\nRetaining 41.160000000000004%, 3039/7383 Entries\nRemoved node \"Others\"\n0.951866626739502\n################################################################################\n{'default_out_nodes_inp': 6, 'default_out_nodes_edge': 20, 'default_drop_nodes_inp': 0.19606347749009728, 'default_drop_nodes_edge': 0.6865892287902534, 'default_drop_nodes_out': 0.620124708339572, 'default_reps_nodes_inp': 2, 'default_reps_nodes_edge': 3, 'default_reps_nodes_out': 3, 'default_decay_rate': 0.5, 'default_out_nodes_out': 59}\n################################################################################\nRetaining 41.160000000000004%, 3039/7383 Entries\nRemoved node \"Others\"\n0.9081377387046814\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nax_client.generation_strategy.trials_as_df#.tail()\n\n[INFO 05-16 14:48:03] ax.modelbridge.generation_strategy: Note that parameter values in dataframe are rounded to 2 decimal points; the values in the dataframe are thus not the exact ones suggested by Ax in trials.\n\n\n\n\n\n\n\n\n\n\nGeneration Step\nGeneration Model\nTrial Index\nTrial Status\nArm Parameterizations\n\n\n\n\n0\nGenerationStep_0\nSobol\n0\nCOMPLETED\n{'0_0': {'default_out_nodes_inp': 4, 'default_...\n\n\n1\nGenerationStep_0\nSobol\n1\nCOMPLETED\n{'1_0': {'default_out_nodes_inp': 4, 'default_...\n\n\n2\nGenerationStep_0\nSobol\n2\nCOMPLETED\n{'2_0': {'default_out_nodes_inp': 4, 'default_...\n\n\n3\nGenerationStep_0\nSobol\n3\nCOMPLETED\n{'3_0': {'default_out_nodes_inp': 3, 'default_...\n\n\n4\nGenerationStep_0\nSobol\n4\nCOMPLETED\n{'4_0': {'default_out_nodes_inp': 8, 'default_...\n\n\n5\nGenerationStep_0\nSobol\n5\nCOMPLETED\n{'5_0': {'default_out_nodes_inp': 6, 'default_...\n\n\n\n\n\n\n\n\n\n# render(ax_client.get_contour_plot())\n\n\nrender(ax_client.get_optimization_trace(objective_optimum=0.0))\n\n                                                \n\n\n: \n\n\n\n# If I need to check what tables are in the sqlite\n# import sqlite3\n# con = sqlite3.connect(\"./foo.db\")\n# cur = con.cursor()\n# # cur.execute(\".tables;\") # should work, doesn't\n# cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n# con.close()",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_tour.html",
    "href": "zma_tour.html",
    "title": "Visible Neural Network",
    "section": "",
    "text": "# Data ----\nfrom dataG2F.core import get_data\nfrom dataG2F.qol  import ensure_dir_path_exists\n\n# Data Utilities ----\nimport numpy  as np\nimport pandas as pd\n\nfrom EnvDL.dlfn import BigDataset, plDNN_general\nfrom EnvDL.sets import mask_parents\n\n# Model Building  ----\n## General ====\nimport torch\nfrom   torch import nn\nimport torch.nn.functional as F\nfrom   torch.utils.data import Dataset\nfrom   torch.utils.data import DataLoader\n\n## VNN ====\nimport sparsevnn\nfrom   sparsevnn.core import\\\n    VNNHelper, \\\n    structured_layer_info, \\\n    SparseLinearCustom\nfrom   sparsevnn.kegg import \\\n    kegg_connections_build, \\\n    kegg_connections_clean, \\\n    kegg_connections_append_y_hat, \\\n    kegg_connections_sanitize_names\n\n# Hyperparameter Tuning ----\nimport os # needed for checking history (saved by lightning) \n\n## Logging with Pytorch Lightning ====\nimport lightning.pytorch as pl\nfrom   lightning.pytorch.loggers import CSVLogger # used to save the history of each trial (used by ax)\n\n## Adaptive Experimentation Platform ====\nfrom ax.service.ax_client import AxClient, ObjectiveProperties\nfrom ax.utils.notebook.plotting import init_notebook_plotting, render\n\n# For logging experiment results in sql database\nfrom ax.storage.sqa_store.db import init_engine_and_session_factory\nfrom ax.storage.sqa_store.db import get_engine, create_all_tables\nfrom ax.storage.sqa_store.save import save_experiment # saving\nfrom ax.storage.sqa_store.structs import DBSettings # loading\n# from ax.storage.sqa_store.load import load_experiment # loading alternate\ntorch.set_float32_matmul_precision('medium')\ninit_notebook_plotting()",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_tour.html#setup",
    "href": "zma_tour.html#setup",
    "title": "Visible Neural Network",
    "section": "Setup",
    "text": "Setup\n\ncache_path = '../nbs_artifacts/aim_2a_G_Gene_VNN/'\n\n\n# Run settings: \nparams_run = {\n    'batch_size': 256,\n    'max_epoch' : 64,   \n}\n\n# data settings\nparams_data = {\n    'y_var': 'Yield_Mg_ha',\n    'y_resid': 'None', # None, Env, Geno\n    'y_resid_strat': 'None', # None, naive_mean, filter_mean, ...\n    'holdout_parents': [\n        ## 2022 ====\n        'LH244',\n        ## 2021 ====\n        'PHZ51',\n        # 'PHP02',\n        # 'PHK76',\n        ## 2019 ====\n        # 'PHT69',\n        'LH195',\n        ## 2017 ====\n        # 'PHW52',\n        # 'PHN82',\n        ## 2016 ====\n        # 'DK3IIH6',\n        ## 2015 ====\n        # 'PHB47',\n        # 'LH82',\n        ## 2014 ====\n        # 'LH198',\n        # 'LH185',\n        # 'PB80',\n        # 'CG102',\n ],    \n}\n\n# in this file I define params later. I've included it here to gurantee that we can merge other params dicts into it.\nparams = {\n'default_out_nodes_inp'  : 4,\n'default_out_nodes_edge' : 16,\n'default_out_nodes_out'  : 1,\n\n'default_drop_nodes_inp' : 0.0,\n'default_drop_nodes_edge': 0.0,\n'default_drop_nodes_out' : 0.0,\n\n'default_reps_nodes_inp' : 1,\n'default_reps_nodes_edge': 1,\n'default_reps_nodes_out' : 1,\n\n'default_decay_rate'     : 1\n}\n\n\ndefault_out_nodes_inp  = params['default_out_nodes_inp' ]\ndefault_out_nodes_edge = params['default_out_nodes_edge'] \ndefault_out_nodes_out  = params['default_out_nodes_out' ]\n\ndefault_drop_nodes_inp = params['default_drop_nodes_inp' ] \ndefault_drop_nodes_edge= params['default_drop_nodes_edge'] \ndefault_drop_nodes_out = params['default_drop_nodes_out' ] \n\ndefault_reps_nodes_inp = params['default_reps_nodes_inp' ]\ndefault_reps_nodes_edge= params['default_reps_nodes_edge']\ndefault_reps_nodes_out = params['default_reps_nodes_out' ]\n\n\n\ndefault_decay_rate = params['default_decay_rate' ]\n\n\nbatch_size = params_run['batch_size']\nmax_epoch  = params_run['max_epoch']\n\ny_var = params_data['y_var']\n\n\nsave_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n\nif 'None' != params_data['y_resid_strat']:\n    save_prefix = save_prefix+'_'+params_data['y_resid_strat']\n\nensure_dir_path_exists(dir_path = cache_path)\n\n\nuse_gpu_num = 0\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif use_gpu_num in [0, 1]: \n    torch.cuda.set_device(use_gpu_num)\nprint(f\"Using {device} device\")\n\n\nimport matplotlib.pyplot as plt\ndef f(dist = 0, rate = 100): return (1/((rate*dist)+1))\njmin, jmax = [0, 2]\n\nxs = np.linspace(0, 6, 70)\nfig, ax = plt.subplots()\nfor j in np.linspace(jmin, jmax, 10):\n    ys = [f(dist= i, rate = j ) for i in xs]\n    ax.plot(xs, ys, color = (j/jmax, 0, 1-(j/jmax)))\n\n[round(100*e) for e in ys[-2:-1] ]\n\n\ndef f(dist = 0, rate = 100): return (1/(rate*np.sqrt(dist)+1))\njmin, jmax = [1, 20]\n\nxs = np.linspace(0, 6, 70)\nfig, ax = plt.subplots()\nfor j in np.linspace(jmin, jmax, 10):\n    ys = [f(dist= i, rate = np.log10(j) # &lt;- note log10 scaling\n            ) for i in xs]\n    ax.plot(xs, ys, color = (j/jmax, 0, 1-(j/jmax)))\n    \n[round(100*e) for e in ys[-2:-1] ]\n\n\ndef _dist_scale_function(out, dist, decay_rate):\n    scale = 1/(1+decay_rate*dist)\n    out = round(scale * out)\n    out = max(1, out)\n    return out\n\n# _dist_scale_function(\n#     out = 100,\n#     dist = 2,\n#     decay_rate = .1)\n\n\ndef _expand_node_shortcut(vnn_helper = myvnn, query = 'y_hat'):\n    # define new entries\n    if True in [True if e in vnn_helper.edge_dict.keys() else False for e in \n                [f'{query}_res_-2', f'{query}_res_-1']\n                ]:\n        print('Warning! New node name already exists! Overwriting existing node!')\n\n    # Add residual connection in graph\n    vnn_helper.edge_dict[f'{query}_res_-2'] = myvnn.edge_dict[query] \n    vnn_helper.edge_dict[f'{query}_res_-1'] = [f'{query}_res_-2']\n    vnn_helper.edge_dict[query]             = [f'{query}_res_-2', f'{query}_res_-1']\n\n    # Add new nodes, copying information from query node\n    vnn_helper.node_props[f'{query}_res_-2'] = vnn_helper.node_props[query] \n    vnn_helper.node_props[f'{query}_res_-1'] = vnn_helper.node_props[query]\n\n    return vnn_helper",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_tour.html#load-data",
    "href": "zma_tour.html#load-data",
    "title": "Visible Neural Network",
    "section": "Load Data",
    "text": "Load Data\n\n# Data Prep ----\nobs_geno_lookup          = get_data('obs_geno_lookup')\nphno                     = get_data('phno')\nACGT_gene_slice_list     = get_data('KEGG_slices')\nparsed_kegg_gene_entries = get_data('KEGG_entries')\n\n\n# make holdout sets\nholdout_parents = params_data['holdout_parents']\n\n# create a mask for parent genotype\nmask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)\n\ntrain_mask = mask.sum(axis=1) == 0\ntest_mask  = mask.sum(axis=1) &gt; 0\n\ntrain_idx = train_mask.loc[train_mask].index\ntest_idx  = test_mask.loc[test_mask].index\n\n\n# convert y to residual if needed\n\nif params_data['y_resid'] == 'None':\n    pass\nelse:\n    if params_data['y_resid_strat'] == 'naive_mean':\n        # use only data in the training set (especially since testers will be more likely to be found across envs)\n        # get enviromental means, subtract from observed value\n        tmp = phno.loc[train_idx, ]\n        env_mean = tmp.groupby(['Env_Idx']\n                     ).agg(Env_Mean = (y_var, 'mean')\n                     ).reset_index()\n        tmp = phno.merge(env_mean)\n        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n        phno = tmp.drop(columns='Env_Mean')\n\n    if params_data['y_resid_strat'] == 'filter_mean':\n        # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations\n        def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],\n                                    year = 2014):\n            # Within each year what hybrids are most common?\n            tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')\n\n            all_envs = set(tmp.Env)\n            # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?\n            # site counts for sets which contain all envs\n            i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year &gt;= i), 'Env'])) == len(all_envs)])\n\n            before = len(set(tmp.loc[:, 'Hybrid']))\n            after  = len(set(tmp.loc[(tmp.Year &gt;= i), 'Hybrid']))\n            print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')\n            tmp = tmp.loc[(tmp.Year &gt;= i), ['Env', 'Hybrid']].reset_index(drop=True)\n            return tmp\n\n\n        tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]\n        filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) \n                          for i in list(set(phno.Year)) ]\n        env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')\n\n        env_mean = env_mean.groupby(['Env_Idx']\n                          ).agg(Env_Mean = (y_var, 'mean')\n                          ).reset_index()\n\n        tmp = phno.merge(env_mean)\n        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n        phno = tmp.drop(columns='Env_Mean')\n\n\n# center and y value data\nassert 0 == phno.loc[:, y_var].isna().sum()\n\ny = phno.loc[:, y_var]\n# use train index to prevent information leakage\ny_c = y[train_idx].mean()\ny_s = y[train_idx].std()\n\ny = (y - y_c)/y_s",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_tour.html#fit-using-vnnhelper",
    "href": "zma_tour.html#fit-using-vnnhelper",
    "title": "Visible Neural Network",
    "section": "Fit Using VNNHelper",
    "text": "Fit Using VNNHelper\n\n# Same setup as above to create kegg_gene_brite\n# Restrict to only those with pathway\nkegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n\n# also require to have a non-empty path\nkegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n\nprint('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n    )+'/'+str(len(parsed_kegg_gene_entries)\n    )+' Entries'\n    )\n# kegg_gene_brite[1]['BRITE']['BRITE_PATHS']\n\n\nkegg_connections = kegg_connections_build(kegg_gene_brite = kegg_gene_brite, \n                                          n_genes = len(kegg_gene_brite)) \nkegg_connections = kegg_connections_clean(         kegg_connections = kegg_connections)\n#TODO think about removing \n# \"Not Included In\n# Pathway Or Brite\"\n# or reinstate 'Others'\n\nkegg_connections = kegg_connections_append_y_hat(  kegg_connections = kegg_connections)\nkegg_connections = kegg_connections_sanitize_names(kegg_connections = kegg_connections, \n                                                   replace_chars = {'.':'_'})\n\n\n# initialize helper for input nodes\nmyvnn = VNNHelper(edge_dict = kegg_connections)\n\n# Get a mapping of brite names to tensor list index\nfind_names = myvnn.nodes_inp # e.g. ['100383860', '100278565', ... ]\nlookup_dict = {}\n\n# the only difference lookup_dict and brite_node_to_list_idx_dict above is that this is made using the full set of genes in the list \n# whereas that is made using kegg_gene_brite which is a subset\nfor i in range(len(parsed_kegg_gene_entries)):\n    if 'BRITE' not in parsed_kegg_gene_entries[i].keys():\n        pass\n    elif parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'] == []:\n        pass\n    else:\n        name = parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'][0][-1]\n        if name in find_names:\n            lookup_dict[name] = i\n# lookup_dict\n\n\nbrite_node_to_list_idx_dict = {}\nfor i in range(len(kegg_gene_brite)):\n    brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i        \n\n# Get the input sizes for the graph\nsize_in_zip = zip(myvnn.nodes_inp, [np.prod(ACGT_gene_slice_list[lookup_dict[e]].shape[1:]) for e  in myvnn.nodes_inp])\n\n\n# init input node sizes\nmyvnn.set_node_props(key = 'inp', node_val_zip = size_in_zip)\n\n# init node output sizes\nmyvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_inp, [default_out_nodes_inp  for e in myvnn.nodes_inp]))\nmyvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_edge,[default_out_nodes_edge for e in myvnn.nodes_edge]))\nmyvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_out, [default_out_nodes_out  for e in myvnn.nodes_out]))\n\n# # options should be controlled by node_props\nmyvnn.set_node_props(key = 'flatten', node_val_zip = zip(myvnn.nodes_inp, [True for e in myvnn.nodes_inp]))\n\nmyvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_inp, [default_reps_nodes_inp  for e in myvnn.nodes_inp]))\nmyvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_edge,[default_reps_nodes_edge for e in myvnn.nodes_edge]))\nmyvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_out, [default_reps_nodes_out  for e in myvnn.nodes_out]))\n\nmyvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_inp, [default_drop_nodes_inp  for e in myvnn.nodes_inp]))\nmyvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_edge,[default_drop_nodes_edge for e in myvnn.nodes_edge]))\nmyvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_out, [default_drop_nodes_out  for e in myvnn.nodes_out]))\n\n\ndist = sparsevnn.core.vertex_from_end(\n    edge_dict = myvnn.edge_dict,\n    end =myvnn.dependancy_order[-1]\n)\n\n\n# overwrite node outputs with a size inversely proportional to distance from prediction node\nfor query in list(dist.keys()):\n    myvnn.node_props[query]['out'] = _dist_scale_function(\n        out = myvnn.node_props[query]['out'],\n        dist = dist[query],\n        decay_rate = default_decay_rate)\n\nTo constrain the number of parameters to optimize over this model will follow these steps:\n\nInitialize the graph’s structure\nInitialize default output values for each node type (input, edge, output)\nInitialize default dropout values for each node type (input, edge, output)\nApply scaling with respect to a node’s distance from the output node\n\nThis factor should have a value which does not apply any scaling and on that applys severe scaling\nDepending on the function used, it may make sense to transform this value e.g. use log10(S) instead of S\nAn option for this would be to use an inverse of a root: \\(scale = \\frac{1}{1+ rate*\\sqrt{dist}}\\)\nA simpler option would be: \\(scale = \\frac{1}{1+ rate*dist}\\)\nUsing 0 or 1 based indexing will be relevant here.\n\nExpand the graph to replicate any nodes that should be replicated.\n\nNOTE: The order here is key. Applying scaling before expanding the graph will result in a smaller scaling penalty than if it is expanded before.\n\nInsert shortcuts, likely for nodes at certain levels or for edge nodes.\n\n\ndist_to_node = {i:[ee for ee in dist if dist[ee] == i] for i in set([dist[e] for e in dist])}\n\n\nkegg_connections_expanded = sparsevnn.core.expand_edge_dict(vnn_helper = myvnn, edge_dict = myvnn.edge_dict)\n\n# ======================================================= #\n# one place to add residual connections would be here.    #\n# edit the links before instatinating the new VNNHelper   #\n# the important thing to do is to edit the graph before   #\n# calulating the inputs for each node.                    #\n# ======================================================= #\n\n# expand then copy over the properties that have already been defined.\nmyvnn_exp = VNNHelper(edge_dict = kegg_connections_expanded)\n\nimport re\nfor new_key in list(myvnn_exp.node_props.keys()):\n    if new_key in myvnn.node_props.keys():\n        # copy directly\n        myvnn_exp.node_props[new_key] = myvnn.node_props[new_key]\n    else:\n        # check for a key that matches the query key after removing the replicate information\n        query = new_key \n        suffix = re.findall('_rep_\\d+$', query)[0]\n\n        query = query.removesuffix(suffix)\n        if query in myvnn.node_props.keys():\n            myvnn_exp.node_props[new_key] = myvnn.node_props[query]\n        else:\n            print(f'WARNING: no entry {query} found for {new_key}') \n\n# now main vnn is the expanded version\nmyvnn = myvnn_exp\n\n\ndemo_usage = False\nif demo_usage:\n    tmpvnn = _expand_node_shortcut(vnn_helper = myvnn, query = 'y_hat')\n\n    tmp = sparsevnn.core.vertex_from_end(\n        edge_dict = tmpvnn.edge_dict,\n        end = 'y_hat'\n    )\n\n    tmp = [e for e in tmp if tmp[e]&lt;=1]\n\n    # subset of edge dict that is at most # upstream of target\n    sparsevnn.core.dict_to_digraph(\n        {e:tmpvnn.edge_dict[e] for e in tmp}\n    )\n\n\n# now the original VNNHelper isn't needed\nmyvnn = myvnn_exp\n\n\n# init edge node input size (propagate forward input/edge outpus)\nmyvnn.calc_edge_inp()\n\n\n# replace lookup so that it matches the lenght of the input tensors\nnew_lookup_dict = {}\nfor i in range(len(myvnn.nodes_inp)):\n    new_lookup_dict[myvnn.nodes_inp[i]] = i\n\n\nCalculate nodes membership in each matrix and positions within each\n\nnode_props = myvnn.node_props\n# Linear_block = Linear_block_reps,\nedge_dict = myvnn.edge_dict\ndependancy_order = myvnn.dependancy_order\nnode_to_inp_num_dict = new_lookup_dict\n\n\n# check dep order\ntally = []\nfor d in dependancy_order:\n    if edge_dict[d] == []:\n        tally.append(d)\n    elif False not in [True if e in tally else False for e in edge_dict[d]]:\n        tally.append(d)\n    else:\n        print('error!')\n        break\n\n\n# build output nodes \nd_out = {0:[]}\nfor d in dependancy_order:\n    if edge_dict[d] == []:\n        d_out[min(d_out.keys())].append(d)\n    else:\n        # print((d, edge_dict[d]))\n\n        d_out_i = 1+max(sum([[key for key in d_out.keys() if e in d_out[key]]\n                   for e in edge_dict[d]], []))\n        \n        if d_out_i not in d_out.keys():\n            d_out[d_out_i] = []\n        d_out[d_out_i].append(d)\n\n\n# build input nodes NOPE. THE PASSHTROUGHS! \nd_eye = {}\ntally = []\nfor i in range(max(d_out.keys()), min(d_out.keys()), -1):\n    # print(i)\n    nodes_needed = sum([edge_dict[e] for e in d_out[i]], [])+tally\n    # check against what is there and then dedupe\n    nodes_needed = [e for e in nodes_needed if e not in d_out[i-1]]\n    nodes_needed = list(set(nodes_needed))\n    tally = nodes_needed\n    d_eye[i] = nodes_needed\n\n# d_inp[0]= d_out[0]\n    \n[len(d_eye[i]) for i in d_eye.keys()]\n\n\n[(key, len(d_out[key])) for key in d_out.keys()]\n\n\ndd = {}\nfor i in d_eye.keys():\n    dd[i] = {'out': d_out[i],\n             'inp': d_out[i-1],\n             'eye': d_eye[i]}\n# plus special 0 layer that handles the snps\n    \ndd[0] = {'out': d_out[0],\n         'inp': d_out[0],\n         'eye': []}\n\n\n# check that the output nodes' inputs are satisfied by the same layer's inputs (inp and eye)\n\nfor i in dd.keys():\n    # out node in each\n    for e in dd[i]['out']:\n        # node depends in inp/eye\n        node_pass_list = [True if ee in dd[i]['inp']+dd[i]['eye'] else False \n                          for ee in edge_dict[e]]\n        if False not in node_pass_list:\n            pass\n        else:\n            print('exit')\n\n\nprint(\"Layer\\t#In\\t#Out\")\nfor i in range(min(dd.keys()), max(dd.keys())+1, 1):\n    node_in      = [node_props[e]['out'] for e in dd[i]['inp']+dd[i  ]['eye'] ]\n    if i == max(dd.keys()):\n        node_out = [node_props[e]['out'] for e in dd[i]['out'] ]\n    else:\n        node_out = [node_props[e]['out'] for e in dd[i]['out']+dd[i+1]['eye']]\n    print(f'{i}:\\t{sum(node_in)}\\t{sum(node_out)}')\n\n\n\nCreating Structured Matrices for Layers\n\ndd.keys()\n\n\nM_list = [structured_layer_info(i = ii, node_groups = dd, node_props= node_props, edge_dict = edge_dict, as_sparse=True) for ii in range(0, max(dd.keys())+1)]\n\n\n[list(e.weight.shape) for e in M_list]\n\n\n\nSetup Dataloader using M_list\n\n# vals = X.get('KEGG_slices', ops_string='asarray from_numpy float')\nvals = get_data('KEGG_slices')\n# parsed_kegg_gene_entries = get_data('KEGG_entries')\nvals = [torch.from_numpy(e).to(torch.float) for e in vals]\n# restrict to the tensors that will be used\nvals = torch.concat([vals[lookup_dict[i]].reshape(4926, -1) \n                     for i in M_list[0].row_inp\n                    #  for i in dd[0]['inp'] # matches\n                     ], axis = 1)\nvals.shape\nvals = vals.to('cuda')\n\n\ntraining_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(np.array(train_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n    lookup_geno = torch.from_numpy(obs_geno_lookup),\n    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n    G =           vals,\n    G_type = 'raw',\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = batch_size,\n    shuffle = True\n)\n\nvalidation_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(np.array(test_idx)), #X.get('val:train',       ops_string='   asarray from_numpy      '),\n    lookup_geno = torch.from_numpy(obs_geno_lookup),\n    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n    G =           vals,\n    G_type = 'raw',\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = batch_size,\n    shuffle = True\n)",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_tour.html#structured-layer",
    "href": "zma_tour.html#structured-layer",
    "title": "Visible Neural Network",
    "section": "Structured Layer",
    "text": "Structured Layer\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, layer_list):\n        super(NeuralNetwork, self).__init__()\n        self.layer_list = nn.ModuleList(layer_list)\n \n    def forward(self, x):\n        for l in self.layer_list:\n            x = l(x)\n        return x\n\nlayer_list = []\nfor i in range(len(M_list)):\n    \n    apply_relu = None\n    if i+1 != len(M_list): # apply relu to all but the last layer\n        apply_relu = F.relu\n    \n\n    l = SparseLinearCustom(\n        M_list[i].weight.shape[1], # have to transpose this?\n        M_list[i].weight.shape[0],\n        connectivity   = torch.LongTensor(M_list[i].weight.coalesce().indices()),\n        custom_weights = M_list[i].weight.coalesce().values(), \n        custom_bias    = M_list[i].bias.clone().detach(), \n        weight_grad_bool = M_list[i].weight_grad_bool, \n        bias_grad_bool   = M_list[i].bias_grad_bool, #.to_sparse()#.indices()\n        dropout_p        = M_list[i].dropout_p,\n        nonlinear_transform= apply_relu\n        )\n\n    layer_list += [l]\n\n\nmodel = NeuralNetwork(layer_list)\n\n\nsave_prefix += '_inv' \nsave_prefix\n\n\n# # ~12 mins to run \n\n# torch.set_float32_matmul_precision('medium')\n\n# from lightning.pytorch.loggers import CSVLogger\n\n# VNN = plDNN_general(model)  \n\n# optimizer = VNN.configure_optimizers()\n\n# logger = CSVLogger(\"nifa_tb\", name=save_prefix)\n# logger.log_hyperparams(params={\n#     'params': params,\n#     'params_data': params_data,\n#     'params_run' : params_run,\n#     'misc': {\n#         'n_train': len(train_idx),\n#         'n_test':  len(test_idx)\n#     }\n# })\n\n# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n\n# trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "zma_tour.html#tiny-test-study",
    "href": "zma_tour.html#tiny-test-study",
    "title": "Visible Neural Network",
    "section": "Tiny Test Study",
    "text": "Tiny Test Study\n\n## Data ====\n\ntraining_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(get_data('obs_env_lookup')[:, 0]),\n    lookup_env  = torch.from_numpy(get_data('obs_env_lookup')),\n    y =           torch.from_numpy(get_data('phno')['Yield_Mg_ha'].to_numpy()).to(torch.float32)[:, None],\n    S           = torch.from_numpy(get_data('SMat')).to(torch.float32),\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = 256,\n    shuffle = True\n)\n\nvalidation_dataloader = DataLoader(BigDataset(\n    lookups_are_filtered = False,\n    lookup_obs  = torch.from_numpy(get_data('obs_env_lookup')[:, 0]),\n    lookup_env  = torch.from_numpy(get_data('obs_env_lookup')),\n    y =           torch.from_numpy(get_data('phno')['Yield_Mg_ha'].to_numpy()).to(torch.float32)[:, None],\n    S           = torch.from_numpy(get_data('SMat')).to(torch.float32),\n    send_batch_to_gpu = 'cuda:0'\n    ),\n    batch_size = 256,\n    shuffle = False\n)\n\n\n[e.shape for e in next(iter(training_dataloader))]\n\n\n## Settings ====\nhyps_trials = 3\n\nrun_hyps = 3\nrun_hyps_force = False # should we run more trials even if the target number has been reached?\nmax_hyps = 5\n\nmax_epoch = 2\nlightning_log_dir = \"test_tb\"\nexp_name = \"fcn_simple\"\nparams_list = [\n        {\n            \"name\": \"size\",\n            \"type\": \"range\",\n            \"bounds\": [1, 256],\n            \"value_type\": \"int\",  # Optional, defaults to inference from type of \"bounds\".\n            \"log_scale\": False,  # Optional, defaults to False.\n        },\n        {\n            \"name\": \"drop\",\n            \"type\": \"range\",\n            \"value_type\": \"float\",  # Optional, defaults to inference from type of \"bounds\".\n            \"bounds\": [0.0, 1.0],\n        }\n    ]\n\n\n# class to convert parametrization to model\nclass fcn_simple(nn.Module):\n    def __init__(self, params):\n        super().__init__()\n        self.Lin1 = nn.Linear(23, params['size'])\n        self.Drp1 = nn.Dropout(p = params['drop'])\n        self.Lin2 = nn.Linear(params['size'], 1)\n    \n    def forward(self, x):\n        x = self.Lin1(x)\n        x = self.Drp1(x)\n        x = F.relu(x)\n        x = self.Lin2(x)\n        return x\n    \n# model = fcn_simple(parameterization)\n# model.to('cuda')(next(iter(training_dataloader))[1]).shape\n\n\n# this is a very funny trick. I'm going to call lighning from within Ax. \n# That way I can save out traces while also relying on Ax to choose new hyps. \n\n# logger = CSVLogger(\"test_tb\", name=exp_name)\n# # logger.log_hyperparams(params={\n# #     'params': params,\n# #     'params_data': params_data,\n# #     'params_run' : params_run,\n# #     'misc': {\n# #         'n_train': len(train_idx),\n# #         'n_test':  len(test_idx)\n# #     }\n# # })\n\ndef evaluate(parameterization):\n    # draw from global\n\n    # max_epoch = 20\n    # lightning_log_dir = \"test_tb\"\n    \n    model = fcn_simple(parameterization)\n\n    VNN = plDNN_general(model)  \n    optimizer = VNN.configure_optimizers()\n    logger = CSVLogger(lightning_log_dir, name=exp_name)\n\n    trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n    trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n\n\n    # if we were optimizing number of training epochs this would be an effective loss to use.\n    # trainer.callback_metrics['train_loss']\n    # float(trainer.callback_metrics['train_loss'])\n\n    # To potentially _overtrain_ models and still let the selction be based on their best possible performance,\n    # I'll use the lowest average error in an epoch\n    log_path = lightning_log_dir+'/'+exp_name\n    fls = os.listdir(log_path)\n    nums = [int(e.split('_')[-1]) for e in fls] \n\n    M = pd.read_csv(log_path+f\"/version_{max(nums)}/metrics.csv\")\n    M = M.loc[:, ['epoch', 'train_loss']].dropna()\n\n    M = M.groupby('epoch').agg(\n        train_loss = ('train_loss', 'mean'),\n        train_loss_sd = ('train_loss', 'std'),\n        ).reset_index()\n\n    train_metric = M.train_loss.min()\n    print(train_metric)\n    return {\"train_loss\": (train_metric, 0.0)}\n\n\n## Generated variables ====\n# using sql database\nsql_url = \"sqlite:///\"+f\"./{lightning_log_dir}/{exp_name}.db\"\n\n# If the database exists, load it and begin from there\nloaded_db = False\nif os.path.exists(sql_url.split('///')[-1]): # must cleave off the sql dialect prefix\n    # alternate way to load an experiment (after `init_engine_and_session_factory` has been run)\n    # experiment = load_experiment(exp_name) # if this doesn't work, check if the database is named something else and try that.\n    db_settings = DBSettings(url=sql_url)\n    # Instead of URL, can provide a `creator function`; can specify custom encoders/decoders if necessary.\n    ax_client = AxClient(db_settings=db_settings)\n    ax_client.load_experiment_from_database(exp_name)\n    loaded_db = True\n\nelse:\n    ax_client = AxClient()\n    ax_client.create_experiment(\n        name=exp_name,\n        parameters=params_list,\n        objectives={\"train_loss\": ObjectiveProperties(minimize=True)}\n    )\n\nrun_trials_bool = True\nif run_hyps_force == False:\n    if loaded_db: \n        # check if we've reached the max number of hyperparamters combinations to test\n        if max_hyps &lt;= (ax_client.generation_strategy.trials_as_df.index.max()+1):\n            run_trials_bool = False\n\nif run_trials_bool:\n    # run the trials\n    for i in range(run_hyps):\n        parameterization, trial_index = ax_client.get_next_trial()\n        # Local evaluation here can be replaced with deployment to external system.\n        ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameterization))\n\n    if loaded_db == False:\n        init_engine_and_session_factory(url=sql_url)\n        engine = get_engine()\n        create_all_tables(engine)\n\n    # save the trials\n    experiment = ax_client.experiment\n    save_experiment(experiment)\n\n\nax_client.generation_strategy.trials_as_df#.tail()\n\n\n# render(ax_client.get_contour_plot())\n\n\nrender(ax_client.get_optimization_trace(objective_optimum=0.0))\n\n\n# If I need to check what tables are in the sqlite\n# import sqlite3\n# con = sqlite3.connect(\"./foo.db\")\n# cur = con.cursor()\n# # cur.execute(\".tables;\") # should work, doesn't\n# cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n# con.close()",
    "crumbs": [
      "Visible Neural Network"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "vnnpaper",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "vnnpaper"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "vnnpaper",
    "section": "Install",
    "text": "Install\npip install vnnpaper",
    "crumbs": [
      "vnnpaper"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "vnnpaper",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "vnnpaper"
    ]
  }
]